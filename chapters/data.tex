\chapter{Data structure, features and states}
\todo{this cahpter could be absorbed into next chapter if it is too short...}
So far we know that sandbox output can be quite rich. There re even such experiments that used machine learning algorithms on all the produced data but we would like to use some subset. In this chapter we describe our reasoning on choosing feature vector and hidden states.

The input for us to this chapter are collected malware analysis results from previous part. The expected output is reasoning on what we can use as features for \emph{hmill} model. This decision should be based on the data we have and even on experience from prior work (reference Mandlik, Pevny, Stiborek and maybe even others and really compare my decisions with them)

\todo{Final result of data collection - no internet samples (maybe mention at the end of previous chapter), I did collect some, but did not use them in further parts because priority was not to have everything}
The outputs of the analysis are quite comprehensive, but we have only some setup. We did collect much more than we can use. Regarding this fact we were not able to use data with internet connection because its collection is quite complicated and at the time we wanted examine it we did not have it. Their analysis is in future work same as data from static analysis (Virus total). These data are still part of our data set and was big part of our job so far. So the subject of our research is sandox output.

Summary of the data set - How many samples, size...
Also we can try to specify the requirements for our model (quite big amount of data - all the reports), we are not able to process whole outputs, reference hmill capacities on previous experiments (for example in Julia language)

Among goals of this thesis is to investigate different parts of sandbox output responsible for some kinds of behavior. This is why we aim at the parts of log where we can see behavior of target malware sample. Those are:
\begin{enumerate}
  \item \emph{pcap reports} - network traffic report
  \item \emph{memory dump} - dump of RAM memory
  \item \emph{bingraph} - \todo{based on the paper https://ieeexplore.ieee.org/document/6461015 - bingraph}
  \item \emph{behavioral logs} - in raw form (usually bson)
  \item \emph{files} - dropped files
  \item \emph{CAPE} - \todo{investigate configurations and https://www.youtube.com/watch?v=qEwBGGgWgOM} extracted payload
  \item \emph{procdump}
  \item \emph{reports} - reports generated by sandbox reporting moduls (especially \emph{report.json} contains comprehensive amount of behavioral information)
  \item \emph{shots} - screenshots of machine screen during sample run
\end{enumerate}
\todo{checkout some web interface example, if there is something else, and even configuration files}


paragraph about each and resoning on if we can use it or not, and what kind of analysis can be based on particular part (I think we enumerated almost all parts of original log)


Reasoning on choice of apropriate input based on our preference (json report contains all significant things, we do not want to investigate network particular and we do not aim at something specific)
- here they used json reports - file:///C:/Users/domia/Downloads/088851962.pdf, file:///C:/Users/domia/Downloads/TSP_CMC_40145.pdf, https://core.ac.uk/download/pdf/237431148.pdf, https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8262998


\todo{Summarize what we know from previous sections \todo{especially from analysis part, where we should summarize what usual program is doing in the computer and what we can observe (and what we can get from cuckoo monitor)} and go to the most concetrated information about run of program - we should end at the things which are in summary part of json log, but we can have more variants}

Describe parts of json report
JSON report parts \todo{describe that json report is quite comprehensive and contain even earlier mentioned outputs} \todo{to appendix we should add some example of report.json}
- size vary a lot, could be from 10MB to GBs
lack of docs - https://github.com/cuckoosandbox/cuckoo/issues/2458

"statistics - time statistics for particular part of malware analysis
"CAPE - extraxted payload
"info - sandbox details (machine, category, used modul...)
    "version"
    "started"
    "ended"
    "duration"
    "id"
    "category"
    "custom"
    "machine
    "package"
    "timeout"
"behavior
    "processes
    "processtree
    "summary
    "enhanced
"debug - debug log of sandbox
"deduplicated_shots - screenshots
"network
    "domains
    "tcp
    "udp
    "dns
    "pcap_sha256"
    "sorted_pcap_sha256"
"static - static analysis of files
"strings - extracted strings (static analysis technique)
"suricata - network detection Suricata tool log
"target - sample details
"malfamily_tag"
"signatures - list of signatures and their data
"malscore"
"ttps

\todo{add sections of report from virus total and its features}
https://developers.virustotal.com/v3.0/reference#files


\section{Features and states for classification}
In chapter \todo{ref chapter classification} we defined \emph{features} and \emph{states}. Our goal \todo{do not forget to add goals to each chapter and go through the trash below each chapter!!} is to try train classifier from so we need to find appropriate $$\mathcal{X}, \mathcal{Y}$$. Following our statements we will find them in \emph{report.json} and in \emph{virustotal.json}. More then one possibilites are stated. Based on further conditions we are going to train models on some of combination/s of \emph{features} and \emph{states}.

List candidates and reasoning

\subsection{States}
We know that our samples are malware (or should be) \todo{clarify it in thesis, reference section about abuse.ch!} so it does not make sense to try \emph{malware X cleanware} classification \todo{reference section where I am trying to describe different types of samples classification}. The classification classes has to be related to the malwaware itself, its attributes or features.

From the behavioral log we extracted three candidates \emph{malware family}, \emph{signatures}, \emph{malscore}. All those attributes we can see as dependent variables where independent are some behavioral features or combination of behavioral and static features. 

Assigning \emph{malware family} is very complex task \todo{https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6107902}, often we involve behavior of the sample, similarity to other samples. Potential model could attempt to generalize \todo{https://www.google.com/search?q=how+malware+family+is+assigned&oq=how+malware+family+is+assigned&aqs=chrome..69i57.6391j0j7&sourceid=chrome&ie=UTF-8} assignment process and point out features which could be used in stochastic way (not as usual static assignment \todo{some citation}). \todo{reference malware families in malware chapter (first)}

Signatures most often tells us what sandbox or other tools observed - special kind of behavior, static features like encryption \dots \todo{more about them we say in chapter on modelling, or maybe here give example of code and signature detection and data part (and all parts) and to appendix we can add more, with reference to the repository} We can see them as local detectors checking particular feature in malware analysis result. Potential model could again generalize their assignment or even investigate other malicious sign that are often connected with some signatures - find new signatures.


Malware score is defined in a lot of papers \todo{https://ieeexplore.ieee.org/document/8666454, http://dodccrp.org/events/6th_ICCRTS/Tracks/Papers/Track7/105_tr7.pdf, malware score to google} and it is crucial problem of these metrics. On the other hand potential classifier could assign score based on more complex decision and we could observe where it is against original assignment or intended by human. On the other hand this is quite risky because even the original label assignment (malscore assignement) is not so clear.


\subsection{Features}
We want to follow to the assumption that our state candidates are somehow related to the behavior of malware sample. That is why in this case we will choose some subset of original \emph{report.json}.
From the log we can extract infromation about samples, state candidates and everything not related to behavior. Two things remain - network and behavior part. In our dataset we do not have relevant network traffic due to the data collection details mentioned above so we take into consideration only behavior part of original report.

Brief description and example (if was not given earlier)
"behavior
    "processes, "processtree  - list of processes related to malware run with details (api name, arguments...)
    "summary
        "files
        "read_files
        "write_files
        "delete_files
        "keys
        "read_keys
        "resolved_apis    
    "enhanced - detail sequence of events (library loading, api calls with parameters...)

As features could be used the whole section or just segments. We have to be aware of potential bias like timestamps which are in enhanced and processes parts.

\section{Data processing pipeline}
\todo{add some image}
After we decided which data parts are useful we used linux tools and Julia language to \todo{reference Julia in appendix technology description and maybe even lazyjson library} extract what we need from original analysis results \todo{reference what we used in bash - if necessary (maybe I can add to attachment bash script with trasformation like that if needed), and reference pruner code in attachements, we should have appendix description of attachments}

After data investigation and processing we are moving to modelling. This statement is truth even physically such big data amounts we have to solve on bigger cluster like cesnet.metacenter. \todo{mention data amount before and after processing pipeline}


Technology - Julia (I think I should add this in some appendix, summarize it and reference all sources used)
  - describe why and basic features and advantages and cons
  - Also all different codes, documented, refactored


%%----------------------------------------------------------------------------------------------

Previous connection
- Follow up statements about data in cybersecurity, follow previous chapter with choosing relevant parts of logs
Way through this chapter, we do not use internet access data, because we did not have it at right time, no internet data are sufficient for our further experimentation
Next connection
- Chosen data will be used for further experiments


reference some source that is talking about data in cyber security, resp. on theory part


Data processing
extracting desired parts
Is there something else in processing?
Pruning jsons in Julia...

reference that we did use similar parts as predecessors (Pevny, Mandlik, Stiborek)

Detail descriptions of candidates, for example describe signatures (just describe what they do and sort them to categories according to what they are doing) and give example and at once slightly say why the signatures could be interesting (based on behavior like this we can validate model, such that it is using the proper part of report, but we can see whatever else which looks like malicious)


Next chapter - Due to the data quality we are not able to use some of them

Get several reasonings for some parts of analysis, what we want and we do not, also we can compare it to Stiborek and other works (for example Mandlik had quite straight-forward dataset and we know that it worked), we can really investigate the whole output and present ideas. 


At the end we can have more variants, but at the end we did examine two and that should not be a problem