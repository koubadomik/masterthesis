\chapter{Data structure, features and states} \label{chap:data}

\todo{this chapter could be absorbed into next chapter if it is too short...}
\todo{do not forget to add goals to each chapter and go through the trash below each chapter!!}

The output of previous chapter are collected malware analysis samples. There are even such experiments that used machine learning algorithms on all the produced data but this is very performance demanding because the output may have 15~\emph{MB} but even gigabytes. That is why we want to choose only subset of original analysis which we find contains the most information about the run and behavior. The goal of this thesis is to use \emph{hmill} model (copmrehensively desribed in \ref{chap:hmill}). Our approach is to create simple binary classifier and then scale up to multiclass or even multilabel if it is possible regarding performance and time we have. In this chapter we describe our reasoning on choosing features and hidden states for our \emph{hmill} classifier.

The input for us to this chapter are collected malware analysis results from previous part. The expected output is reasoning on what we can use as features for \emph{hmill} model. This decision should be based on the data we have and even on experience from prior work we reference.

The outputs of the analysis are quite comprehensive, but we have only some of those which were mentioned in \ref{chap:infrastructure}. At the time we started experiments and even later we were able to collect the data without the internet connection because the setup is more straight-forward and easier. Other setup with the internet connection will be investigated in future work as its collection continues. 

The data in such domain are very biased. Sometimes we can see that malware is not able to be run under the conditions in virtual machine. The fact that we do not have internet connection can make this bias stronger so it has to be counted on in making the decision on what to use to train the model. We definitely do not want to investigate network activities.

The dataset we have consist of $80000$ different samples. Complete data have \texttildelow$2,5$~\emph{TB} where the analysis result is compressed.

The documentation of the output is quite weak but there is some (\url{https://cuckoo.readthedocs.io/en/latest/usage/results/}) and we also investigated outputs we have. The output of general \emph{CAPEv2} sandbox analysis is described in table \ref{tab:sandbox-out}.

\begin{table}[h]
    \centering
    \caption{CAPEv2 Sandbox output (all possibilities)}
    \begin{tabular}{p{2cm}p{6cm}p{6cm}} 
        \toprule
        \textbf{Output} &
        \textbf{Meaning} &
        \textbf{In our output} \\
        \midrule
        pcap report & network traffic record (packet sequences) & Presented in output but not significant regarding the fact that during run was not internet access \\
        \midrule
        memory dump & dump of RAM (its analysis results could be also presented)& Our version of sandbox do not support it \\
        \midrule
        bingraph & mechanism that discovers metamorphic malware \cite{Kwon2012}& Presented\\
        \midrule
        behavioral log & raw logs of api calls and other (usually in bson) & Presented \\
        \midrule
        dropped files & all dropped files are unchanged in separate directory & Presented \\
        \midrule
        CAPE, procdump & Other extraxted payload in separate directory, extracted by various techniques \cite{Cape} & Presented \\
        \midrule
        reports & Sandbox allows us using several reporting and processing modules, their results are in separate directory  & Several of them presented but the most comprehensive and important is \emph{report.json} \\
        \midrule
        screenshots & All taken during analysis  & Presented \\
        \bottomrule
    \end{tabular}
    \label{tab:sandbox-out}
\end{table}

Of course not all the outputs are presented in each sample. All the information about result is presented in sandbox web interface for example here \url{https://capesandbox.com/} (after authentication).

Our decision we would like to build on the fact that in \ref{chap:analysis} we stated most important things during program run - \emph{processes, files, mutexes, registries, commands, api calls}. Another important reason is that our model is able to process \emph{tree-structured data} which are also \emph{JSON} documents and that is what we want to use.

There are various experiments involving machine learning approaches with analysis result input. Based on the fact that a lot of them use \emph{report.json} as main input \cite{Darshan2016, Dinh2019a, Kim2020, Sethi2019}, we would like to investigate this first. Later we can add something more if we miss some data. We also knot that this report include all mentioned information. This report also covers the majority of analysis storage on the disk so included information is very comprehensive (in \ref{tab:report}).

Of course those experiments are chosen by us, very often research is aiming at \emph{pcap} files of memory analysis and even more. But even those projects into the \emph{report.json}.

% \todo{Summarize what we know from previous sections \todo{especially from analysis part, where we should summarize what usual program is doing in the computer and what we can observe (and what we can get from cuckoo monitor)} and go to the most concetrated information about run of program - we should end at the things which are in summary part of json log, but we can have more variants}

\section{Report}
\emph{JSON} notation is described in \ref{sec:json_notation}. The report is has usually tens of megabytes but sometimes even gigabytes. It is quite easy to compress, we can have even 10 times less storage because it contains a lot of redundancies (sequencies of api calls\dots). 

The documentation of each part of the report is quite poor but a lot of attributes are self-descriptive. Complete schema is in \ref{tab:report}.

\begin{table}[h]
    \centering
    \caption{Parts of \emph{report.json}}
    \begin{tabular}{p{2cm}p{12cm}} 
        \toprule
        \textbf{Entry} &
        \textbf{Meaning} \\
        \midrule
        statistics & time statistics for particular part of malware analysis \\
        \midrule
        info & sandbox details (machine, category, used modul, timeout...) \\
        \midrule
        debug & sandbox debug log \\
        \midrule
        target &  info about examined sample\\
        \midrule
        CAPE & extracted payload info \\
        \midrule
        behavior & processes, mutexes, commands and other attributes as enhanced log but even summary view \\
        \midrule
        deduplicated shots & screenshot summary \\
        \midrule
        network & network traffic report (domains, tcp, udp\dots) \\
        \midrule
        static analysis & results per file \\
        \midrule
        strings & extracted strings \\
        \midrule
        suricata &  output of suricata network detection tool (\url{https://suricata.readthedocs.io/en/latest/quickstart.html})\\
        \midrule
        malfamily tag &  malware family detection result\\
        \midrule
        malscore &  malware sample metric\\
        \midrule
        signatures &  list of signatures which were detected by sandbox with their attributes \\
        \bottomrule
    \end{tabular}
    \label{tab:report}
\end{table}

In \ref{app:report_example} we can see examples for each part of log.


\section{Features and states for classification}
In chapter \todo{ref chapter classification} we defined \emph{features} and \emph{states}. Our goal is to try train classifier so we need to find appropriate $$\mathcal{X}, \mathcal{Y}$$. Following our statements we will find them in \emph{report.json}. More than one possibilites are stated. Based on further conditions we are going to train models on some of combination/s of \emph{features} and \emph{states}.

% List candidates and reasoning

% \subsection{States}
% We know that our samples are malware (or should be) \todo{clarify it in thesis, reference section about abuse.ch!} so it does not make sense to try \emph{malware X cleanware} classification \todo{reference section where I am trying to describe different types of samples classification}. The classification classes has to be related to the malwaware itself, its attributes or features.

% From the behavioral log we extracted three candidates \emph{malware family}, \emph{signatures}, \emph{malscore}. All those attributes we can see as dependent variables where independent are some behavioral features or combination of behavioral and static features. 

% Assigning \emph{malware family} is very complex task \todo{https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6107902}, often we involve behavior of the sample, similarity to other samples. Potential model could attempt to generalize \todo{https://www.google.com/search?q=how+malware+family+is+assigned&oq=how+malware+family+is+assigned&aqs=chrome..69i57.6391j0j7&sourceid=chrome&ie=UTF-8} assignment process and point out features which could be used in stochastic way (not as usual static assignment \todo{some citation}). \todo{reference malware families in malware chapter (first)}

% Signatures most often tells us what sandbox or other tools observed - special kind of behavior, static features like encryption \dots \todo{more about them we say in chapter on modelling, or maybe here give example of code and signature detection and data part (and all parts) and to appendix we can add more, with reference to the repository} We can see them as local detectors checking particular feature in malware analysis result. Potential model could again generalize their assignment or even investigate other malicious sign that are often connected with some signatures - find new signatures.


% Malware score is defined in a lot of papers \todo{https://ieeexplore.ieee.org/document/8666454, http://dodccrp.org/events/6th_ICCRTS/Tracks/Papers/Track7/105_tr7.pdf, malware score to google} and it is crucial problem of these metrics. On the other hand potential classifier could assign score based on more complex decision and we could observe where it is against original assignment or intended by human. On the other hand this is quite risky because even the original label assignment (malscore assignement) is not so clear.


% \subsection{Features}
% We want to follow to the assumption that our state candidates are somehow related to the behavior of malware sample. That is why in this case we will choose some subset of original \emph{report.json}.
% From the log we can extract infromation about samples, state candidates and everything not related to behavior. Two things remain - network and behavior part. In our dataset we do not have relevant network traffic due to the data collection details mentioned above so we take into consideration only behavior part of original report.

% Brief description and example (if was not given earlier)
% "behavior
%     "processes, "processtree  - list of processes related to malware run with details (api name, arguments...)
%     "summary
%         "files
%         "read_files
%         "write_files
%         "delete_files
%         "keys
%         "read_keys
%         "resolved_apis    
%     "enhanced - detail sequence of events (library loading, api calls with parameters...)

% As features could be used the whole section or just segments. We have to be aware of potential bias like timestamps which are in enhanced and processes parts.

% \section{Data processing pipeline}
% \todo{add some image}
% After we decided which data parts are useful we used linux tools and Julia language to \todo{reference Julia in appendix technology description and maybe even lazyjson library} extract what we need from original analysis results \todo{reference what we used in bash - if necessary (maybe I can add to attachment bash script with trasformation like that if needed), and reference pruner code in attachements, we should have appendix description of attachments}

% After data investigation and processing we are moving to modelling. This statement is truth even physically such big data amounts we have to solve on bigger cluster like cesnet.metacenter. \todo{mention data amount before and after processing pipeline}


% Technology - Julia (I think I should add this in some appendix, summarize it and reference all sources used)
%   - describe why and basic features and advantages and cons
%   - Also all different codes, documented, refactored



% NICE to have
% We can try to create some histogram using the data from virus total (families or something like that...)
% \todo{checkout some web interface example, if there is something else, and even configuration files}
% \todo{add sections of report from virus total and its features}
% https://developers.virustotal.com/v3.0/reference#files
%%----------------------------------------------------------------------------------------------
% Describe parts of json report
% JSON report parts \todo{describe that json report is quite comprehensive and contain even earlier mentioned outputs} \todo{to appendix we should add some example of report.json}
% - size vary a lot, could be from 10MB to GBs
% lack of docs - https://github.com/cuckoosandbox/cuckoo/issues/2458

% "statistics - time statistics for particular part of malware analysis
% "info - sandbox details (machine, category, used modul...)
%     "version"
%     "started"
%     "ended"
%     "duration"
%     "id"
%     "category"
%     "custom"
%     "machine
%     "package"
%     "timeout"
% "CAPE - extraxted payload
% "behavior
%     "processes
%     "processtree
%     "summary
%     "enhanced
% "debug - debug log of sandbox
% "deduplicated_shots - screenshots
% "network
%     "domains
%     "tcp
%     "udp
%     "dns
%     "pcap_sha256"
%     "sorted_pcap_sha256"
% "static - static analysis of files
% "strings - extracted strings (static analysis technique)
% "suricata - network detection Suricata tool log
% "target - sample details
% "malfamily_tag"
% "signatures - list of signatures and their data
% "malscore"
% "ttps

% paragraph about each and resoning on if we can use it or not, and what kind of analysis can be based on particular part (I think we enumerated almost all parts of original log)


% Summary of the data set - How many samples, size...
% Also we can try to specify the requirements for our model (quite big amount of data - all the reports), we are not able to process whole outputs, reference hmill capacities on previous experiments (for example in Julia language)


% \todo{Final result of data collection - no internet samples (maybe mention at the end of previous chapter), I did collect some, but did not use them in further parts because priority was not to have everything}

% Their analysis is in future work same as data from static analysis (Virus total). These data are still part of our data set and was big part of our job so far. So the subject of our research is sandox output.

% Previous connection
% - Follow up statements about data in cybersecurity, follow previous chapter with choosing relevant parts of logs
% Way through this chapter, we do not use internet access data, because we did not have it at right time, no internet data are sufficient for our further experimentation
% Next connection
% - Chosen data will be used for further experiments


% reference some source that is talking about data in cyber security, resp. on theory part


% Data processing
% extracting desired parts
% Is there something else in processing?
% Pruning jsons in Julia...

% reference that we did use similar parts as predecessors (Pevny, Mandlik, Stiborek)

% Detail descriptions of candidates, for example describe signatures (just describe what they do and sort them to categories according to what they are doing) and give example and at once slightly say why the signatures could be interesting (based on behavior like this we can validate model, such that it is using the proper part of report, but we can see whatever else which looks like malicious)


% Next chapter - Due to the data quality we are not able to use some of them

% Get several reasonings for some parts of analysis, what we want and we do not, also we can compare it to Stiborek and other works (for example Mandlik had quite straight-forward dataset and we know that it worked), we can really investigate the whole output and present ideas. 


% At the end we can have more variants, but at the end we did examine two and that should not be a problem