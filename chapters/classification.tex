\chapter{Malware classification} \label{chap:classification}
In this chapter we describe machine learning background which is important for further model description. We cover basic terms, cyber security context and models for hierarchical structured data (\emph{JSON}). All of these is background for our target experiments in modelling.


We can expose a lot of information from malware analysis result (\ref{chap:analysis}). It could be used to explain directly what is going on during the run, every single step. Sandbox itself is often labelling malware samples. For instance we can observe \emph{malware family} estimation or some kind of \emph{signature}. According to that we are able to categorize malware samples and determine the danger to the target computer. These times we can see many applications of machine learning in this field, e. g. \ \cite{Yen2019}. Its automation may lead to new knowledge and might improve zero-day detection and other situations where deterministic behavioral feature detection might be insufficient. One of our goals is to perform such techniques on the data we collect. At first in this chapter we will build theoretical knowledge for its later application.

In this chapter we focus on classification problem in general and in second part we describe malware classification as particular case. In previous chapter we described different data outputs which are often produces by different kinds of analysis. The one we concentrate on is structured output, e.g.\ \emph{JSON}because our model (\emph{hmill}) is able to perform well on such data. That is why we mention models which are designed for such data later in this chapter.

\section{Classification in general}
Firstly, let us quote author which summarized our motivation very well.

\say{Learning is a search through the space of possible hypotheses for one that will perform well, even on new examples beyond the training set. To measure the accuracy of a hypothesis we give it a test set of examples that are distinct from the training set.} \cite{Russell2009}

As we can see learning itself might be described quite easily and it is very important for its understanding. On the other hand the crucial is its mathematical base and the largest challenge in machine learning theory is very often used formal framework and point of view which we consider as there are many. So firstly we would like to summarize where in this tangle we are and where we are going.

After short look into several sources we can find varigated kinds of machine learning or just learning as \cite{Russell2009} says. Its easy to mix perpectives and make one big taxonomy but we do not aspire to describe everything, just choose our subspace.

At first let us state that the main field of our interest in machine learning theory begins in statistical machine learning. Where we are aiming at optimization of predictive function to fit our training and perform sufficiently on testing data, using statistical tools such as \emph{maximum likelihood estimation} and many others. As oposite we can see for example symbolic learning where we are more interested in symbolic knowledge representation, often human-readable. This approach is older and sometimes called \emph{GOFAI} - Good Old-Fashioned Artificial Intelligence \cite{Haugeland1985}. This kind of learning is nowadays not going through such a huge upswing as statistical branch. Let us define basic terms based on \cite{Franc2020}.

% \todo{we should define machine learning algorithm and model and how it relates to the hypothesis itself}

\paragraph{Sample - Independent variable set}
By sample we mean collection of features which tends to be represented as $x\in\mathbb{R}^{n}$, where $x_i$ is often called feature and $x$ is called sample or feature vector \cite{GoodBengCour16}. We can also generalize this definition for tensors.

This is often just the finish of our way, in real word we can see object features as $x \in \mathcal{X}$, where $x$ could be categorical variables, scalar, real valued vectors, sequences, images, graphs, structured formats (\emph{JSON}) and much more. We might involve feature extraction process to get to real-valued vectors mentioned above.

\paragraph{States - dependent variable set}
By state we mean the subject of our prediction, represented often as $y \in \mathcal{Y}$, where $\mathcal{Y}$ is often called \emph{state space}. That could be whatever what we enumerated by $\mathcal{X}$ (images, documents, real values\dots). States are sometimes also called labels or targets. 

\paragraph{Prediction strategy, hypothesis}
Hypothesis we define as $h:\mathcal{Y} \rightarrow \mathcal{X}$. The output of prediction strategy (state estimation) we denote as $h(x)=\hat{y}$ on the contrary real state we denote as above just $y$.

\paragraph{Example}
Assume very usual situation that before the learning we receive set of examples to learn from. Based on what we receive we can distinguish between several types of learning. This thesis works with \emph{supervised} case.\
\begin{enumerate}
    \item \emph{Supervised learning} - example denotes pair $(x,y)$, where $x\in \mathcal{X}$ and $y\in \mathcal{Y}$. 
    \item \emph{Unsupervised learning} - example denotes $x\in \mathcal{X}$
    \item \emph{Semi-supervised learning} - each example could be one of the possibilities above
\end{enumerate}
We are usualy working with the set of examples which we later divide into different subsets e.g. \ \emph{training, testing, validation set}. \

One of the biggest challenges of machine learning nowadays are labelled data, because labellig itself is often not part of sample and has to be added post-hoc like in case of images (marking where on image is some object). Sometimes it is not possible e.g. \ in cyber security often we do not know what label should be presented because situation is just too complex. It is because in principle attacker or any kind of distractor often wants to hide acts such that the protector is not able to identify it. So the hidden state itself could be assigned rather based on further data collection or never. Examples of unsupervised learniing in cyber security are \cite{Alom2017}, where authors present deep unsupervised learning techniques during intrusion detection.

The crucial assumption is that $X, Y$ are random variables related by unknown joint p.d.f $p(x,y)$. This assumption make the whole learning process reasonable because we assume relation between variables. We also assume that we are able to draw examples from this p.d.f.

\paragraph{Loss function}
Loss funtion denotes objective of our optimization task during learning, $\ell: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}^{+}$. Usually we compute output for each example such that it shows us penalization on particular example $\ell(y, h(\hat{y}))$.

\paragraph{Learning}
Main consequence of our assumption is that $h(x)$ and $\ell(Y,h(X))$ are also random variables. These assumptions also let us define \emph{expected risk} (\ref{eq:exploss}).

\begin{equation} \label{eq:exploss}
    R(h)=\mathbb{E}_{(x,y) \sim p}\ell(Y,h(X))=\sum_{x \in \mathcal{X}}\sum_{y \in \mathcal{Y}}p(x,x)\ell(y,h(x))
\end{equation}

If $p(x,y)$ would be known our learning process would find optimal prediction strategy by $h^*(x)=\operatorname*{argmin}_{{y}'\in \mathcal{Y}}\sum_{y\in\mathcal{Y}}p(y|x)\ell(y,{y}')$. But usualy it is unknown and we have to involve some kind of approximation (learning algorithm) to find best attaineable strategy using drawn data.

Assume $\mathcal{T}^m$ being set of examples to learn from. Then we can distinguish two basic learning approaches.
\begin{enumerate}
    \item \emph{Discriminative learning}
    \item \emph{Generative learning}
\end{enumerate}

In \emph{Discriminative learning} we assume that $h^* \in \mathcal{H}$. We replace \emph{expected risk} with \emph{empirical risk} (sometimes called \emph{generalization error}) (\ref{eq:emploss}).

\begin{equation} \label{eq:emploss}
    R_{\mathcal{T}}(h)= \frac{1}{|\mathcal{T}|} \sum_{(x,y) \in \mathcal
    T}\ell(y,h(x))
\end{equation}

Optimal strategy is denoted by $h_{\mathcal{T}}^*(x)=\operatorname*{argmin}_{{h}'\in \mathcal{H}}R_{\mathcal{T}(h)}$. Algorithms using this approach are for instance linear reggression, support vector machines and neural networks (with back-propagation).

\emph{Generative learning} assumes that true p.d.f. $p(x,y)$ is part of some parametrized family of distributions. Task for our algorithm is to localize point estimate of parameters $\theta$ based on $\mathcal{T}$. It could be done by \emph{MLE} or \emph{Bayes inference rule}.

In both cases the biggest challenge is size of deviation we can gain by choosing wrong family distribution or just having small number of training examples.

In our work we are using \emph{discriminative} models namely neural networks.

\paragraph{Discriminative learning algorithm}
The learning algorithm is realization of the learning process. Input of this process are training examples and additional parameters (often called hyperparameters) and its output is usualy statistical model of the data. Statistical model is often defined by its type, e.g. \ \emph{neural net} or \emph{linear regression model} and by its parameters e.g. \ weights, biases, which are tuned during the learning process. Given observed sample $x$ this model can provide prediction  $\hat{y}$ (the model is representing prediction strategy). Quality of this strategy (its predictions) is conditioned by number of training examples and choice of $\mathcal{H}$ which is often connected to the algorithm we choose. 

We distinguish two types of error. \emph{Approximation error} $R(h_{\mathcal{H}}-R^*)$  is caused by choice of $\mathcal{H}$ (choice of algorithm), $R(h_{\mathcal{H}}$ denotes best attainable risk using only hypotheses from $\mathcal{H}$. \emph{Estimation error} $R(h_{\mathcal{m}})-R(h_{\mathcal{H}})$ where $R(h_{\mathcal{m}})$ denotes risk learned from training data by learning algorithm.

% \todo{challenges train, test, validation and hyperparameters, overfitting, underfitting 5.2 in \cite{GoodBengCour16}, maybe we can mention examples Here we can find something else https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/ - \cite{JasonBrownlee2020}}
% % SURELY MENTION RETRAIN PROBLEM AND reason that this is true but machine learning algorithm could be much more efficient (this is kind of motivation of our work)

\section{Machine learning tasks}
There are many types of machine learning tasks \citet{GoodBengCour16}, we will address only some examples. Machine learning tasks are divided not only according to the examples (as mentioned above) but also according to the states.

\paragraph{Regression}
In this case $y \in \mathcal{Y}$ is continous-valued tensor, most often $\mathcal{y} \in \mathbb{R}^{n}$. In this example features are outputs of static and dynamic vulnerablities detectors and network traffic record and predicted value is score represented as real-value \cite{Jaganathan2015}.

\paragraph{Classification}
In this case $y \in \mathcal{Y}$ is categorical tensor and often called \emph{classes}, in most cases $y \in \{1,\dots,\mathcal{C}\}$, where $\{1,\dots,\mathcal{C}\}$ is encoding for real world values like \emph{man} and \emph{woman} and similar. If $\mathcal{C}$ is $2$, than we call it \emph{binary classification} and if $\mathcal{C}>2$ we call it \emph{multiclass classification}. We can classify to more classes at once (to not mutually exclusive classes), we call it \emph{multi-label classification}, which is sometimes called \emph{multiple output model} \cite{murphy2013machine}. Classification could be even more complicated e.g. \ we can also classify into hierarchical related classes \cite{zhang2020dive}.

Given $x$ classifier outputs $\hat{y}$ which is ecoded class or probability distribution over classses \cite{GoodBengCour16}. Example of such distribution might by popular \emph{softmax} layer in case of neural networks. This distribution might be later interpreted and used during evaluation and further result analysis (explaning, interpreting). In case of scoring classifier where the output is representing the probability distribution we have to determine its final predictions. This is often done by setting \emph{treshold}, if the result is above specified treshold it is in positive class and it is in negative otherwise.

Example of such classification task could be malware classification using well-known classification algorithm SVM \cite{Kruczkowski2014}.

\emph{Classification} and \emph{regression} are not the only variants. There are others like transcription \cite{GoodBengCour16} or anomaly detection \cite{Chandola2009}. And many other defined problems mentioned in \cite{GoodBengCour16} or \cite{zhang2020dive}. 

Anomaly detection is quite frequent problem in cyber security field, where we are interested in detecting what is not maching usual pattern. This is usualy connected with unsupervised learning task where we can use algorithms like Expectation Maximization \cite{Dempster1977}. For example in this paper \cite{IglesiasVazquez2014} we can see example of anomaly detection on network traffic data.

Our intention is in \emph{classification} and that is why further we talk particularly about this task and algorithms used in this field.

Examples of classification learning algorithms are discriminative - Support vector machine, Decision trees, Logistic reggression, neural nets; generative - Naive bayes, Fisher's linear discriminant.


\section{Loss functions for classification}
The learning part of the whole process consists mainly of function optimization. Our criterion is chosen Loss function. The way of optimizing it may vary for example in case of neural networks we use the output of loss function to be able to update the parameters of the model \cite{JasonBrownlee2020}.

We mention two common functions which are often involved in classification tasks.

\subsection{Multinomial logistic loss (multiclass cross entropy)} 
In this situation we assume that the output of our model are conditional probabilites  $x \in \mathcal{X}$  $p(c|x;\theta)$ for each class $c \in \mathcal{C}$ and for each of $n$ examples.
Multiclass cross entropy is defined in \ref{eq:crossent}. 

We can see that the idea behind is that we are minimizing value representing average logarithm of probability of truth class (denoted by $c_i$) assigned by the model, across examples. We want the probability to be the maximum attaineable and $\log$ of value between $0$ and $1$ is negative larger as we are getting closer to $0$. 

\begin{equation} \label{eq:crossent}
    \ell(\theta)=-\frac{1}{n}\sum_{i=1}^{n}\sum_{c \in \mathcal{C}}\mathbbm{1}\{c==c_i\}\log p(c|x;\theta)
\end{equation}

In multilabel case we use variant of this function sometimes called binary cross entropy. %may mention formula, nice-to-have

\subsection{Hinge loss}
This loss was introduced in \cite{Gentile1998} and its main usage we can find in Support vector machine algorithm. The formula could be seen in \ref{eq:hinge}, where $y=\pm1$ denotes truth label and $\hat{y}$ classifier score.

\begin{equation} \label{eq:hinge}
    \ell(y)=\max(0,1-\hat{y}\cdot y)
\end{equation}

\section{Model Evaluation}
Based on what we are solving and what we want to compare to we have to choose proper metrics to evaluate our classifier. Majority of those techniques does not depend on type of model we have. The classifier itself is just blackbox which we are evaluating based on classificcation results.
When we defined learning we also mentioned that learning is often divided into two or three phases - \emph{training}, \emph{testing} (sometimes \emph{validation}). Metrics which we described further have to be connected strictly with the context. By context we mean on which data it is evaluated e.g. \ accuracy on training set is absolutely different from accuracy on testing set. 
% \todo{reference bias X variance tradeoff in the beginning of this chapter}
Evaluation metrics are most often used to measure the quality of the model given hyperparameters, compare to results of different algorithms, tuning early stopping to avoid overfitting or just to demonstrate the performance of the model.

\paragraph{Loss function}
As we defined earlier loss function is the objective with its own meaning and possible interpretation. Loss function value is often monitored directly during the learning process.

\paragraph{Confusion matrix}
Most significant metrics are derived from very straight-forward concept of \emph{Confusion matrix}. For our convenience we define confusion matrix for binary classifier. Its generalization is obvious. Common problems in binary classification are formulated in the way that $y \in \{positive, negative\}$. As an example we can introduce classification that a patient has cancer or not. All $(x,y)$ where $y=positive$ we call positive examples and oposite examples are called negative.

\begin{table}[h]
    \centering
    \caption{Confusion matrix}
        \begin{tabular}{l|l|c|c|}
        \multicolumn{2}{c}{}&\multicolumn{2}{c}{Ground truth}\\
        \cline{3-4}
        \multicolumn{2}{c|}{}&Positive&Negative\\
        \cline{2-4}
        \multirow{2}{*}{Classified}& Positive & True positive ($TP$) & False negative($FP$)\\
        \cline{2-4}
        & Negative & False negative ($FN$) & True negative ($TN$)\\
        \cline{2-4}
        \end{tabular}
    \label{tab:confmatrix}
  \end{table}

Before deriving basic metrics we have to emphasize that all of them have to be trated in particular context. The most important condition is the overall balance of the dataset - the ratio of positive and negative examples. List of metrics that relevant for our use is in \ref{tab:metrics} \todo{might be in appendix}.

\begin{table}[h]
    \centering
    \caption{Classifier evaluation metrics}
    \begin{minipage}{\linewidth}
    \begin{tabular}{lcp{5cm}}
      \toprule
      \textbf{Metric} &
      \textbf{Formula} &
      \textbf{Interpretation}
      \\
      \midrule
      accuracy & $\frac{TP+TN}{TP+TN+FN+FP}$ & shows the ratio of correctly classified examples to all examples (imbalanced dataset may bias its interpretation)\\
      \midrule
      false positive rate ($FPR$) & $\frac{FP}{FP+TN}$ & shows the ratio of misclassified positive examples to all examples classified positive \\
      \midrule
      false negative rate ($FNR$) & $\frac{FN}{FN+TP}$ &  shows the ratio of misclassified negative examples to all examples classified negative \\
      \midrule
      true positive rate or recall ($TPR$) & $\frac{TP}{TP+FN}$ & shows the ratio of truly classified positive examples to all positive examples  \\
      \midrule
      true negative rate ($TNR$) & $\frac{TN}{TN+FP}$ & shows the ratio of truly classified negative examples to all negative examples \\
      \midrule
      precision & $\frac{TP}{TP+FP}$ & shows the ratio of truly classified positive examples to all examples classified as positive \\
      \midrule
      balanced accuracy &$\frac{TNR+TPR}{2}$ & shows average performance balanced for both classes (imbalanced dataset is less risky here) \\
      \midrule
      f1 score & $2 \cdot \frac{precision \dot recall}{precision+recall}$ & good measure if we seek for trade-of between precision and recall (we are not able to tweak both simultaneously), we are able to compare classifiers using that\\
      \bottomrule
    \end{tabular}
    \end{minipage}
    \label{tab:metrics}
  \end{table}

Very often we can see also curves which are plotted along reported metrics. These plots fits in situation when we are trying to compare multiple classifiers. Most seen are \emph{ROC} (described in \cite{Fawcett2006}) and \emph{PRC} (described in \cite{Flach2015}). Data point for these two curves are collected by iterating over possible \emph{treshold} values and for each we calculate specific metric. In case of \emph{ROC} metrics are \emph{FPR} on x-axis and \emph{TPR} on y-axis. In case of \emph{PRC} metrics are \emph{TPR} (sometimes \emph{recall}) on x-axis and \emph{precision} on \emph{y-axis}.

There are many differences and similarities of these two curves but the crucial one is that the \emph{PRC} is better in case of having imbalanced dataset where we have more negative examples and we especially care about positive examples and their predictions. Sometimes we can compensate this even in case of \emph{ROC} curve by setting \emph{logarithm scale} on x-axis.

If we need to have single number as performance metric including multiple tresholds we can use \emph{area under the ROC or PR curve}. However it usage have to be careful and strictly in given context.

\paragraph{In Cyber Security}
The main point which we are walking around is the cybersecurity. Some metrics are more important than other in this field. It is crutial to think about domain speciific facts choosing appropriate metric to measure our models's performance.

For example \emph{accuracy} used in this article \cite{Ghanaei2016} may make sense because authors are interested just in the positive examples. But in general we do not have balanced datasets (same number of positive and negative examples). So the bias we are facing is that if in the dataset consists of 80~\% of positive example then sily classifier classifying only positive class has $accuracy=0.8$. This risk shares all domains but in cyber security we really often face unbalanced datasets. For instance in this article \cite{Hernandez-Callejo2019} we can see usage of \emph{geometrical mean} or in general we can use even \emph{balanced accuracy} to cover even dataset balance or just join negative example into the metric calculation.

During Intrusion Detection in cyber security \emph{False negative} examples can be potential security risk for target subject (person, company, state...) so it is often priority number one. On the other hand we \emph{False positive} means false alarm and time of people and of course their trust \cite{owaspintrusion}. So in intrusion detection we have to find optimal point where the detection is useful regarding false negatives but not detecting to many false positives because our solution will not be financialy beneficial - false alarms.

Frequency of malware creation and distribution is really high so even quite small false positive rate can cause that the security team is solving something harmless instead of real risk \cite{Apruzzese2018}. In malware detection this could be also reinforced by the fact that the protection is just too aggresive. Such results can lead us into systematic expenses. Answer is again finding tradeoff and combining solutions.\cite{Kubovic2017}

As result, further in this thesis we will work with imbalanced datasets where we have more negative examples. We use several metrics mentioned above especially \emph{balanced accuracy}, \emph{ROC curve with logarithm scale on x-axis}, \emph{PR curve}.

\section{Neural Networks}
In the section about models we refered to different approaches. One specific we would like to discuss more copmrehensively - neural nets. The reason is that our method is building on top of this approach so we want to state the formalism for the rest of the thesis.
\begin{figure}
    \centering
    \begin{neuralnetwork}[height=4]
        \newcommand{\x}[2]{$x_#2$}
        \newcommand{\y}[2]{$\hat{y}_#2$}
        \newcommand{\hfirst}[2]{\small $h^{(1)}_#2$}
        \newcommand{\hsecond}[2]{\small $h^{(2)}_#2$}
        \inputlayer[count=3, bias=true, title=Input\\layer, text=\x]
        \hiddenlayer[count=4, bias=false, title=Hidden\\layer 1, text=\hfirst] \linklayers
        \hiddenlayer[count=3, bias=false, title=Hidden\\layer 2, text=\hsecond] \linklayers
        \outputlayer[count=2, title=Output\\layer, text=\y] \linklayers
    \end{neuralnetwork}
    \caption{Neural net example}
    \label{fig:neuralnet}
\end{figure}

Neural net is discriminative model which is based on \emph{Empirical Risk Minimization} (mention earlier). It is a composition of simple linear or non-linear functions (\emph{neurons}) which are parametrized. Specific neural network architecture is defined by used functions and their count (functions are organized into layers). The architecture implies $\mathcal{H}$ a hypothesis space. Example of general neural net could be seen in \ref{fig:neuralnet}. \todo{describe a little more the picture and maybe even change it}

The overall goal is to optimize $\ell$ with respect to the parameters of the net. For this task is most often used \emph{gradient descent} optimization technique.

Usage of the \emph{gradient descent} puts minimal demands on the data we are using which makes this model so popular. We also do not insist on strict convexity of the function we are optimizing (the function does not have to have one global \emph{minimum}). The assumption about all functions we are using is that they have to be differentiable. The price we are paying is that the optimization guarantees only stationary point ending which could be a \emph{local minimum} or worse a \emph{saddle point}. But applications lead us to the conclusion that in right hands this model may perform really well.

For the gradient computation is used \emph{backpropagation algorithm} \cite{Rumelhart1988}. The main idea of this algorithm builds on computing derivatives of every function's outputs with respect to its outputs (functions have to differentiable). Then by applying the \emph{chain rule} we are able to compute how the parametrized layers (not all of them are) influence the loss function. Finally, we might converge to parameters which minimizes loss over training dataset. The parameter's update is most often done by \emph{stochastic gradient descent} \cite{Kiefer1952} as this variant of the original optimization algorithm is feasible even in situation of huge datasets or online learning.

% Main advantage of this kind of model is that we do not have assume much about our data. This is main reason why neural nets are used very often nowadays.

% Neural nets are machine learning phenomenon mainly because its robustness and not so demanding data assumptions. They do not guarantee global maxima as the function we are optimizing does not have to be convex
% \todo{image}
% - approach, notation
% optimization theory, gradient descent, stochastic gradient descent, backprop
% cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf
% Minibatches (random subsampling...), hyper parameters
% regularization, standardization techniques and other related techniques
%     Standardization meaning mean 0 variance 1 and normalization meaning min max scaling
% types - https://www.ccdcoe.org/uploads/2018/10/Art-19-On-the-Effectiveness-of-Machine-and-Deep-Learning-for-Cyber-Security.pdf

% Loss functions and nets used for classification - We can use various, in our case we used logit cross entropy described above. We use softmax as penultimate layer

% https://www.deeplearningbook.org/contents/ml.html - \cite{GoodBengCour16}
% http://neuralnetworksanddeeplearning.com/chap1.html - \cite{Nielsen2018}



\section{Hierarchical structured data}
Real world use cases provide often more complex datasets than just fixed dimension matrices or images. As we mentioned malware analysis data are often stored as JSON files. Those files could be formally seen as tree-structured inputs. We are able to perform the feature extraction to gain fixed dimension objects but in the structure itself is a lot information.\cite{PevnyDedic2020}.  The documents are often hierarchical which means that nodes in same subtree are semantically related. 

There are two main approaches to classify such hierarchical structured data - \emph{Hierarchical Multiple Instance Learning} and \emph{Graph neural networks}. The first one is described in next chapter \ref{chap:hmill} and second is decribed below.

\todo{define graph}
Both methods are able to generally operate on node-level, edge-level, and graph-level prediction tasks. We aim at graph-level classification (class is predicted for each graph).

\emph{Graph neural networks}
This kind of neural network was introduced in \cite{Scarselli2009}. 


% \subsection{Problem}
% Define it as problem of processing structured data meaning JSON which are very often and they could be interpreted as graphs (specifically trees).
% Inspiration can be Simon's chapter Towards automated processing...
% Define JSON document

% \subsection{Approaches and prior}
% Very shortly mention
% Formalism, why we are solving that (connect to graph structured data)

% Graph neural networks
%     Also seen here file:///C:/Users/domia/Downloads/Explainability_ICML2021.pdf \cite{Pevny2020}
% Hmill
% See in Mandlik, reference next chapter
% prior only examples
%     Deep Convolutional Networks on Graph-Structured Data \cite{Henaff2015}
%     \cite{Borgwardt2005} - Graph data in bioinformatics

%     Hmill - \cite{Janisch2020} \cite{PevnyDedic2020}




% \section{Classification in cyber security, classifying malware} %% HIGH PRIORITY


% \todo{Add some examples of use of ML and AI in cyber security field and challenges - https://arxiv.org/abs/1812.07858}
%     Part Machine Learning Challenges in Cyber-Security \cite{Amit2019}

% challenges we mentioned above, now let us describe typical tasks in cyber security and especially malware detection and classification

% In this section we want to relate our work to other articles and show prior work. The field of cyber security experience a lot of machine learning applications. The biggest trend is in fraud detection \cite{Babu2020}, cloud security \cite{Coppolino2017} and also IoT \cite{Zhou2019}. But some of them are also in the field of malware research and malware classification (references below).

% Thanks to works like the following we are able to create some overview above the problems related to malware and machine learning, describe taxonomy mentioned here (add something my, maybe even copy the picture) -  file:///C:/Users/domia/Downloads/CoRR2018_submission_v3.pdf \cite{Ucci2017} to every case add references

% Point of view from the side of features which could be used and algorithms wich examples - https://www.sciencedirect.com/science/article/pii/S1084804519303868 \cite{Gibert2020}

% Should be mentioned
% % Intrusion detection, network traffic attacks and anomalies and malware X cleanware classification,...
% % We can classify different things like malware X cleanware, malware family (not so useful in detection but in further learning)
% % Based on Dynamic, static, other kind of input data...

% Examples
% % If I finally go through this summary report I will be save with this part hopefully - https://www.jstor.org/stable/resrep22692?seq=53#metadata_info_tab_contents, this is great summary for modern approaches and papers in cybersec, I can use it even somewhere else for example in intro of this chapter and whole thesis
% % For example one part was focused on types of classifiers - in machine learning is quite big difference between zero-days and known malware...

% % Using N-grams in detection https://www.researchgate.net/publication/262366662_A_Close_Look_on_n_-Grams_in_Intrusion_Detection_Anomaly_Detection_vs_Classification

% % not so interesting can be added between other examples - https://www.researchgate.net/publication/312964059_Malware_Classification_Based_on_Dynamic_Behavior, https://link.springer.com/content/pdf/10.1631/FITEE.1601325.pdf, https://dspace.cvut.cz/bitstream/handle/10467/87850/F3-DP-2020-Dvorak-Stepan-dvorast6.pdf?sequence=-1&isAllowed=y (mention graph neural networks)

% % Using static analysis https://dl.acm.org/doi/pdf/10.1145/2402599.2402604?casa_token=Kv3xJb3iPssAAAAA:rm6bhWnusg7eEvalXNveaJXILphAxhpZHGS6OxpDk36na4q5u9RpZ3gM83IMQWq1QQ0aEIVoeyZN

% % family classification - https://arxiv.org/pdf/1912.11249v1.pdf

% % Recurrent networks - https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7178304

% Finally - hmill in malware classification tool
% % HMILL in cyber security - https://arxiv.org/pdf/2002.04059.pdf, mandlik, Pevny


% In this chapter we described general overview and notation on task we are solving - classification in the cybersecurity. We addressed even structured data techniques which are the crux for us, especially \emph{hmill} model. We are going to use this model during data modelling part. In next chapter we describe it comprehensively.



%%--------------------------------NICE TO HAVE----------------------------------------------
% Graphical models
%     Simon

% Cross-Entrophy (and logit version, binary version)
% Just define mathematically and then say its intepretation, nice to have would be to mention KL divergence
% https://stats.stackexchange.com/questions/272754/how-do-you-interpret-the-cross-entropy-value - intepretation, add even my own intuition
% https://stats.stackexchange.com/questions/31985/definition-and-origin-of-cross-entropy - KL div
% mention logit transformation for numerical stability
% Hinge Loss
% math, intuition (ours)

% Metrics
% \cite{Hossin2015}
% https://ntnuopen.ntnu.no/ntnu-xmlui/bitstream/handle/11250/2730527/Hameed%252C%2BIbr.pdf?sequence=2&isAllowed=y


% \todo{ In this section we are aiming at standard machine learning approach and tasks formulation, later we will define task which is actual for our work. add scheme of machine learning algorithm in classical manner, so features to hidden states (sometimes label) mapping, define some formalism for math signing}
    % \todo{from this we can go to missing values in data and maybe even to various dimesion data learning}
    % \subsection{Challenges}
% \todo{more like nice-to-have}
% The oposite point of view on non-ideal datasets are those where features are missing. \todo{http://www.iiis.org/CDs2008/CD2008SCI/SCI2008/PapersPdf/S507DT.pdf is source of information here}
% If I finally go through this summary report I will be save with this part hopefully - https://www.jstor.org/stable/resrep22692?seq=53#metadata_info_tab_contents, this is great summary for modern approaches and papers in cybersec, I can use it even somewhere else for example in intro of this chapter and whole thesis



% \todo{in all cases we can have reference and short definition, describe some of basic models and their usual output and its intepretation and also loss function, Mention on what kind of optimization it is doing}
% \paragraph{Discriminative}
% SVM
% Decision Trees
% Logistic regression
% Neural Nets - will be defined rigorosly later

% \paragraph{Generative}
% Naive Bayes
% Fisher's linear discriminant


% \todo{add formula for geometrical mean}

% \todo{nice-to-have applications of NN (but there are tons...)}

%%--------------------------------REMAINING----------------------------------------------
%%--------------------------------REMAINING----------------------------------------------
%%--------------------------------REMAINING----------------------------------------------
%%--------------------------------REMAINING----------------------------------------------
%%--------------------------------REMAINING----------------------------------------------
%%--------------------------------REMAINING----------------------------------------------
% Remaninders:

% \section{Data characteristics}
% Model which we choose for given task is based on several facts like domain which we are dealing with and mainly on the data we have.
% Input to usual model is binary vector or rather tensor, but those tensor could be encoding of different things
% Basic point of view is to distinguish between two basic types of variables - continous and discrete (https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/)
% Highlevel view will refer to images, time series, graphs (json...), just binary...
% Maybe we can talk about fixed dimension, various dimension

% Write more about json files and generally describe graph inputs

% Previous connection
% - One of conclusions of previous chapter is that that character of data is often structural as graph or tree (json, xml,...)
% Way through this chapter
% - Generally about machine learning challenges and types of tasks
% - Classification models and their evaluation
% - Neural networks
% - Classifying structured data as jsons - models
%     - hmill and other techniques - reference next chapter
% - classification task in cyber security
% - Prior on this topic - cyber security!
% Next connection
% - Hmill is one what we want to use to create model of the data for classification


% \cite{GoodBengCour16}
% Potentially:
% - general points at the beggining
%     - Increasing model sizes
%     - Increasing Accuracy, Complexity and Real-World Impact
%     - Gradient-Based Optimization
%     - Stochastic Gradient Descent
%     - Challenges Motivating Deep Learning
% - More Sure
%     - Supervised X Unsupervised
%     - The Task, T - classification, regression, others exists
%     - The Performance Measure, P
%     - Capacity, Overfitting and Underfitting
%     - Hyperparameters and Validation Sets
%     - Bias and variance - Trading off Bias and Variance to Minimize Mean Squared
%     Error

% \cite{Bishop2006}
%  - generally the introduction to classification models


% - Classification problems - binary, multiclass, multilabel...
% - Theory
% - Single label, multilabel

% - evaluation of classifiers - more or less independent on model choose
%   - Confusion matrix
%   - F-score, train accuracy, test accuracy, loss function, plots
%         - AUC, ROC?
%   - What is important metric during malware classification and why


% - learning classifier from graph data - json files,...
% - possible models - based mainly on the data structure we have and use case we are modelling (find how to choose proper model)
%     - among them even neural networks and refer to mill/hmil in next chapter - connect to goal of this thesis, use this framework to classify malware - go on with next part
% - (one of them should be using neural nets - describe deeply usage of loss function in both cases - single label, multi label...)
% - hyper parameters...
% - minibatch gradient descent, gradient descent itself

% - Machine learning in cyber security in general
% - General approaches to malware classfication - theory
%     - classifying type of malware or some kind of behavior X classifying malware Vs. cleanware
%     - based on dynamic/static anysis results...
%     - get to neural network and finally to mill - stiborek and other applications in cyber security (Mandlik, Pevny, Dedic...) and reference next chapter.


% - Prior work
% Our goal is not to compare, mentions are just for reference to similar work, mention practical papers used above

% nice-to have:
% - (Overfitting, early stopping)

% Remainders:
% Malware analysis results could be used
% Following data collection and processing next task is  
% Based on type of analysis and characteristics of collected data}
