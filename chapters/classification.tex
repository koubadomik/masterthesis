\chapter{Malware classification} \label{chap:classification}
In this chapter, we describe the machine learning background, which is important for further model description. We cover basic terms in machine learning, cybersecurity context, and models for hierarchically structured data (\emph{JSON} documents).

\section{Machine learning}
\say{Learning is a search through the space of possible hypotheses for one that will perform well, even on new examples beyond the training set. To measure the accuracy of a hypothesis, we give it a test set of distinct examples from the training set.} \cite{Russell2009}

As we can see in the quote, the essentials of machine learning can be described concisely and elegantly. On the other hand, the crux is its mathematical base. The most considerable challenge in machine learning theory is the formal framework and point of view, which the authors consider. Firstly, we would like to put our method into context.

After a brief look into several sources, we can find varied kinds of machine learning or just learning \cite{Russell2009}. It is easy to mix perspectives and make one extensive taxonomy. However, we start with statistical machine learning as the most general notion in this chapter. 

In statistical machine learning, we aim to optimize the predictive function to fit our training data and perform sufficiently on testing data. It is done using statistical tools such as \emph{maximum likelihood estimation} and many others. On the opposite, we can see symbolic learning where we are more interested in symbolic knowledge representation, often human-readable. This approach is older and sometimes called \emph{GOFAI} --- Good Old-Fashioned Artificial Intelligence \cite{Haugeland1985}. This kind of learning is not going through such a massive upswing as the statistical branch nowadays. Let us define the basic terms based on \cite{Franc2020}.

\subsection{Definitions}
\subsubsection*{Sample --- independent variable set}
By sample, we mean a collection of features which tends to be represented as $x\in\mathbb{R}^{n}$, where $x_i$ is often called feature and $x$ is called sample or feature vector \cite{GoodBengCour16}. We can also generalize this definition for tensors.

Generally, we can see object features as $x \in \mathcal{X}$, where $\mathcal{X}$ could be a set of categorical variables, scalars, real-valued vectors, sequences, images, graphs, structured formats (\emph{JSON}) and much more. We might involve a feature extraction process to get to the real-valued vectors mentioned above.

\subsubsection*{States, classes --- dependent variable set}
By state, we mean the subject of our prediction, often represented as $y \in \mathcal{Y}$, where $\mathcal{Y}$ is often called \emph{state space}. That could be whatever we enumerated by $\mathcal{X}$ (images, documents, vectors\dots). They also tend to be represented as real-valued or discrete vectors. States are sometimes also called labels or targets. We focus on classification tasks, so we call them classes.

\subsubsection*{Prediction strategy, hypothesis}
A prediction strategy is defined as $h:\mathcal{Y} \rightarrow \mathcal{X}$. The output of prediction strategy we denote as $h(x)=\hat{y}$, where $h\in\mathcal{H}$ (often called hypothesis class). On the contrary, the real state we denote by $y$. 

\subsubsection*{Example}
Assume the usual situation that before learning, we receive a set of examples to learn from. Based on what we receive, we can distinguish between several types of learning. This thesis works with \emph{supervised} case.\
\begin{enumerate}
    \itemsep0em 
    \item \emph{Supervised learning} --- example denotes pair $(x,y)$, where $x\in \mathcal{X}$ and $y\in \mathcal{Y}$
    \item \emph{Unsupervised learning} --- example denotes $x\in \mathcal{X}$
    \item \emph{Semi-supervised learning} --- each example could be one of the possibilities above
\end{enumerate}
We are usually working with the set of examples that we later divide into different subsets, e.g., \emph{training, testing}, and \emph{validation sets}.

The crucial assumption is that $X, Y$ are random variables related by unknown joint p.d.f\footnote{Probability Density Function} $p(x,y)$. This assumption makes the whole learning process reasonable because we assume the relationship between the variables. We also assume that we can draw i.i.d.\footnote{identically independently distributed} examples from this p.d.f.

\subsubsection*{Loss function}
Loss function denotes the objective of our optimization task during learning, $\ell: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}^{+}$. Usually, we compute its value for each particular example $\ell(y, h(x))$ and use some aggregation, e.g., mean, to get one value for the whole set of examples.

\subsubsection*{Learning}
Main consequence of the assumption about randomness of $\mathcal{X}$ and $\mathcal{Y}$ is that $h(x)$ and $\ell(Y,h(X))$ are also random variables. This fact allows us definition of \emph{expected risk}(\eqref{eq:exploss}).

\begin{equation} \label{eq:exploss}
    R(h)=\mathbb{E}_{(x,y) \sim p}\ell(Y,h(X))=\sum_{x \in \mathcal{X}}\sum_{y \in \mathcal{Y}}p(x,y)\ell(y,h(x))
\end{equation}

If $p(x,y)$ is known, optimal prediction strategy would be denoted by $h^*(x)=\operatorname*{argmin}_{{y}'\in \mathcal{Y}}\sum_{y\in\mathcal{Y}}p(y|x)\ell(y,{y}')$. In practise, it is unknown and we have to involve an approximation (learning algorithm) to find the best attainable strategy using drawn data.

Assume $\mathcal{T}^m$ is a set of examples for supervised learning. We can distinguish two basic learning approaches --- \emph{Discriminative learning}, \emph{Generative learning}.

In \emph{Discriminative learning} we assume $h^* \in \mathcal{H}$ and we approximate \emph{expected risk} with \emph{empirical risk} (\eqref{eq:emploss}).

\begin{equation} \label{eq:emploss}
    R_{\mathcal{T}}(h)= \frac{1}{|\mathcal{T}|} \sum_{(x,y) \in \mathcal
    T}\ell(y,h(x))
\end{equation}

Optimal strategy is denoted by $h_{\mathcal{T}}^*(x)=\operatorname*{argmin}_{{h}'\in \mathcal{H}}R_{\mathcal{T}(h)}$. Models trained using this approach are, for instance, linear regression, support vector machine, and neural networks.

\emph{Generative learning} assumes that true p.d.f. $p(x,y)$ is part of some parametrized family of distributions. The task for our algorithm is to localize the point estimate of parameters $\theta$ based on $\mathcal{T}$.

In our work, we are using \emph{discriminative} models, namely neural networks.


We distinguish two types of error. \emph{Approximation error} $R(h_{\mathcal{H}})-R^*$  is caused by the choice of $\mathcal{H}$ (choice of model), $R(h_{\mathcal{H}})$ denotes best attainable risk using only hypotheses from $\mathcal{H}$. \emph{Estimation error} $R(h_{m})-R(h_{\mathcal{H}})$ where $R(h_{m})$ denotes the risk learned from the training data.


\subsection{Machine learning tasks}
\subsubsection*{Regression}
States $y \in \mathcal{Y}$ are continuous-valued tensor in this case, most often $y \in \mathbb{R}^{n}$. For example, features might be the outputs of static and dynamic detectors and network traffic records, and the state is a risk score represented as a real value (\cite{Jaganathan2015}).

\subsubsection*{Classification}
In this case, $y \in \mathcal{Y}$ is categorical vector, in most cases $y \in \{1,\dots,\mathcal{C}\}$, where $\{1,\dots,\mathcal{C}\}$ is encoding for real world values like \emph{man} and \emph{woman}. If $\mathcal{C}$ is $2$, than we call the task \emph{binary classification} and if $\mathcal{C}>2$ we call it \emph{multiclass classification}. We can classify to not mutually exclusive classes at once, that is called \emph{multi-label classification} or \emph{multiple output model} \cite{murphy2013machine}. Classification could be even more complicated, e.g., \ we can classify into a class hierarchy \cite{zhang2020dive}.

Given $x$ classifier outputs $\hat{y}$ which is an encoded class or probability distribution over classes \cite{GoodBengCour16}. An example of such a distribution might be the output of the \emph{softmax} activation function in a neural network. This distribution might be later interpreted and used during evaluation and further analysis. We have to determine predictions of such classifier because we have real values instead of discrete classes. In binary classification, the discretization is often done by setting a \emph{treshold}. If the result is above the specified threshold, it is in a positive class and vice versa.

An example of a classification task could be malware classification using the well-known SVM algorithm \cite{Kruczkowski2014}.

\emph{Classification} and \emph{regression} are not the only variants. There are others like transcription \cite{GoodBengCour16} or anomaly detection \cite{Chandola2009} and many other problems mentioned in \cite{GoodBengCour16} or \cite{zhang2020dive}. 

Anomaly detection is a frequent problem in cybersecurity, where we are interested in detecting what is not matching the usual pattern. That might be related to an unsupervised learning task where we can use algorithms like Expectation Maximization \cite{Dempster1977}. For example, in this paper \cite{IglesiasVazquez2014}, we can see the anomaly detection of network traffic data.

Examples of discriminative classification learning algorithms are --- Support vector machine, Decision trees, Logistic regression, and neural nets.

Our intention is in \emph{classification}, we focus on it further.


\section{Loss functions for classification}
The learning process builds on function optimization. Our criterion is the chosen loss function. Every loss function has its interpretation, and we choose it based on our goals (domain-specific). We mention two standard functions which are often involved in classification tasks.

\subsection*{Multinomial logistic loss} 
We assume that the outputs of our model are conditional probabilites $p(c|x;\theta)$ for each class $c \in \mathcal{C}$ and each of $n$ examples. The model's parameters are denoted by $\theta$.
Multiclass cross entropy is defined in \eqref{eq:crossent}. This loss function is sometimes called \emph{multiclass cross entropy}.

\begin{equation} \label{eq:crossent}
    \ell(\theta)=-\frac{1}{n}\sum_{i=1}^{n}\sum_{c \in \mathcal{C}}\mathbbm{1}\{c==c_i\}\log p(c|x;\theta)
\end{equation}

Its idea is to minimize values representing the average logarithm of the probability of the actual class (denoted by $c_i$) across examples, correctly predicted by the model.

\subsection*{Hinge loss}
This loss was introduced in \cite{Gentile1998} and its well-known usage we can find in the Support vector machine algorithm. The formula could be seen in the figure \eqref{eq:hinge}, where $y\in\mathcal{Y}=\{\pm1\}$ denotes truth label and $\hat{y}$ denotes the classifier's score.

\begin{equation} \label{eq:hinge}
    \ell(y)=\max(0,1-\hat{y}\cdot y)
\end{equation}

\section{Model Evaluation}
Based on our domain and goal, we have to choose proper classifier evaluation metrics. The majority of such techniques do not depend on the type of model we have. The classifier itself is just a black box, and we evaluate its predictions.

Learning is often divided into two or three phases --- \emph{training}, \emph{testing} (sometimes \emph{validation}). Metrics are connected to the data, e.g., \ accuracy on the training set is different from accuracy on the testing set. 

Evaluation metrics are most often used to measure the quality of the model. It also might be used to compare the results of different approaches. If we tune the model's hyperparameters, we often monitor accuracy on the validation set.

\subsection*{Loss function}
The loss function is the objective with its meaning and possible interpretation (described above). Its value is often monitored directly during the learning process. We might observe the loss function value difference between the training set and the validation set as a possible overfitting indicator.

\subsection*{Confusion matrix}
The most significant metrics are derived from the concept of \emph{Confusion matrix} (table \ref{tab:confmatrix}. For our convenience, we define a confusion matrix for a binary classifier. Its generalization for the multiclass case is just larger, but the idea is the same. Common problems in binary classification are formulated in a way that $y \in \{positive, negative\}$. As an example, we can introduce the classification that a patient has cancer or not. All $(x,y)$ where $y=positive$ we call positive examples and the opposite examples are negative.

\begin{table}[h]
    \centering
    \caption{Confusion matrix for binary classifier}
        \begin{tabular}{l|l|c|c|}
        \multicolumn{2}{c}{}&\multicolumn{2}{c}{Ground truth}\\
        \cline{3-4}
        \multicolumn{2}{c|}{}&Positive&Negative\\
        \cline{2-4}
        \multirow{2}{*}{Classified}& Positive & True positive ($TP$) & False positive($FP$)\\
        \cline{2-4}
        & Negative & False negative ($FN$) & True negative ($TN$)\\
        \cline{2-4}
        \end{tabular}
    \label{tab:confmatrix}
  \end{table}

Before deriving basic metrics, we have to emphasize that they have to be treated in a particular context. The most important condition is the overall balance of the dataset --- the ratio of positive and negative examples. List of important metrics is in \ref{tab:metrics}.

Very often, we can see also curves which are plotted along reported metrics. These plots fit in situation when we are trying to compare multiple classifiers. Most seen are \emph{ROC} (described in \cite{Fawcett2006}) and \emph{PRC} (described in \cite{Flach2015}). Data points for these two curves are collected by iterating over possible \emph{treshold} values and for each we calculate specific metric. In case of \emph{ROC} metrics are \emph{FPR} on x-axis and \emph{TPR} on y-axis. In case of \emph{PRC} metrics are \emph{TPR} (sometimes \emph{recall}) on x-axis and \emph{precision} on y-axis.

The \emph{PRC} is better in the case of an imbalanced dataset where we have a larger number of negative examples, and we especially care about positive examples and their predictions. 
% We can compensate the imbalance even in the case of \emph{ROC} curve, where class balance is usually ignored, by setting \emph{logarithm scale} on the x-axis.

If we need to have a single number as a performance metric, including multiple thresholds, we can use the area under the \emph{ROC} or \emph{PRC}.

\subsection*{Cyber Security context}
Some metrics are more critical than others in cybersecurity. It is crucial to think about domain-specific facts choosing appropriate metric to measure our model's performance.

The class imbalance is one of the challenges in cybersecurity. If the dataset consists of 80~\% of positive examples, then a classifier predicting only positive class has $accuracy=0.8$. In \cite{Hernandez-Callejo2019}, we can see the usage of \emph{geometrical mean} to deal with this issue. We can also use \emph{balanced accuracy} to cover the dataset's imbalance in the metric calculation.

During Intrusion Detection in cybersecurity, false negative examples can be a potential security risk for the target subject (person, company, state etc.), so it is often the priority number one. On the other hand, \emph{False positive} classification means false alarm, which costs employee time and trust \cite{owaspintrusion}. The frequency of software creation is significant, so even a relatively low false positive rate can cause the security team to solve something harmless instead of a real risk \cite{Apruzzese2018}. In malware detection, that could also be reinforced by the fact that the protection is too aggressive. Such results can lead us to recurring expenses \cite{Kubovic2017}. In such a case, we can use \emph{ROC} curve with \emph{logarithm scale} on the x-axis to observe even a low false positive rate. Note that our dataset has to be sufficiently large to observe a statistically significant estimate of a low positive rate.

To sum up, we have to find an optimal point where the model predicts attacks successfully, but still with a low number of false alarms.

In the thesis, we will work with imbalanced datasets (more negative examples). We use several metrics mentioned above especially \emph{balanced accuracy}, \emph{ROC} curve with logarithm scale on x-axis, and \emph{PR curve}.

\section{Neural Networks}
\begin{figure}
    \centering
    \begin{neuralnetwork}[height=4]
        \newcommand{\x}[2]{$x_#2$}
        \newcommand{\y}[2]{$\hat{y}_#2$}
        \newcommand{\hfirst}[2]{\small $h^{(1)}_#2$}
        \newcommand{\hsecond}[2]{\small $h^{(2)}_#2$}
        \inputlayer[count=3, bias=true, title=Input\\layer, text=\x]
        \hiddenlayer[count=4, bias=false, title=Hidden\\layer 1, text=\hfirst] \linklayers
        \hiddenlayer[count=3, bias=false, title=Hidden\\layer 2, text=\hsecond] \linklayers
        \outputlayer[count=2, title=Output\\layer, text=\y] \linklayers
    \end{neuralnetwork}
    \caption{Neural net example}
    \label{fig:neuralnet}
\end{figure}

A neural net is a discriminative model which is based on \emph{Empirical Risk} minimization \eqref{eq:emploss}. The neural network is a parametric function created by composing simple functions. Specifically, $nn(x) = g_1(g_2(g_3(\dots(g_n(x,\theta_n),\dots;\theta_1)$, where each $g_n$ has the form $\sigma(W * x + b)$ and $\theta$ denotes parameters $(W,b)$. Functions, their input and output are often demonstrated as layers of a neural net. An example of a general neural net could be seen in \ref{fig:neuralnet}. The input layer represents items of the feature vector. A \emph{multilayer perceptron} described in \cite{Russell2009} with a nonlinear activation function might realize hidden layers. The output layer represents a predicted state. Many functions are used in neural nets, e.g., before the output layer is often a softmax or sigmoid activation function to normalize the input into a probability distribution (score). The overall goal is to optimize $\ell$ with respect to the parameters of the net. Usually, we use \emph{gradient descent} optimization technique.

Usage of the \emph{gradient descent} puts no extra demands on the data we are using. We also do not insist on the strict convexity of the function we are optimizing (the function does not have to have one global \emph{minimum}). The assumption about all functions is that they have to be (at least piecewise) differentiable with respect to their inputs and parameters. 

\emph{Backpropagation algorithm} is used for the effective gradient computation \cite{Rumelhart1988}. This algorithm allows us computation of the derivative of $\ell$ with respect to all network parameters. The main idea builds on computing derivatives of every function's output with respect to its input. Then by applying the \emph{chain rule}, we can propagate the information from the following layers to the previous ones.

The optimization is most often done by \emph{stochastic gradient descent} \cite{Kiefer1952}. This variant of the \emph{gradient descent} algorithm is usable even in huge datasets or online learning. Usually, we divide the original training dataset into subsets (we call them \emph{batches} or \emph{minibatches}), and we use them to compute the gradient rather than the whole dataset. That is done for a specified number of iterations. \emph{Batch} is randomly subsampled from the training set, which is the reason for '\emph{stochastic}' in the method name. The \emph{batch} size and the number of iterations are hyperparameters of the neural net.

\section{Tree-structured data classification}
Real-world use cases often provide more complex datasets than just fixed dimension matrices or images. As we mentioned, malware analysis data are stored as \emph{JSON} files, and modelling such data is our goal. Those files could be formally seen as tree-structured inputs (more generally graphs). In the following section, we describe two straightforward approaches of data classification and two more sophisticated.

\subsection{Rules}
An example of a rule might be that if we observe a specific value in a specific key part in the document, e.g., \ \lstinline|{"api_calls": "DeleteFileA"}|, we classify the current document to a specific class, e.g., \ \emph{dropper}. The logic might be more complex. We can count a score aggregating more information about the current document and classify the document according to the score. As an example of such a rule-based approach, we can see malware signatures mentioned in the previous chapter. Their implementation deterministically checks some part of the original \emph{JSON} report and performs binary classification (\emph{positive} --- the signature is added to the report, \emph{negative} --- the signature is not added to the report). The rules are usually defined manually.

\subsection{Flattening}
This approach deals with the data structure itself, which has no fixed dimensions. It is more a feature engineering technique than a learning approach. Each document might contain a different number of keys in a different order, and the size of arrays may vary. Using flattening, we find a mapping/procedure which allows vectorization of each document. The target learning algorithm is used on the flattened dataset. An advantage of this approach is that we can use a plethora of off-the-shelf algorithms, but on the other hand, we reduce the hypothesis class.
\hfill \break
The motivation of more complex techniques is that the data structure also keeps some information, and it is worth trying to model even the structure, not only the data.

\subsection{Graph neural networks}
Let us firstly define a graph $\mathcal{G}=(\mathcal{V},\mathcal{E})$, where $\mathcal{V}$ denotes a finite set of vertices and $\mathcal{E}\subseteq\binom{\mathcal{V}}{2}$ for indirected graphs or $\mathcal{E}\subseteq\mathcal{V}\times\mathcal{V}$ for directed graph denotes a finite set of edges. 

Graph neural network (GNN) was introduced in \cite{Scarselli2009}, and it is a suitable model for problems represented by a graph (directed or undirected). Initial setup is that the problem representation is $G(\mathcal{V},\mathcal{E})$ where each vertex is associated with its embedded value $v_i$, where $i\in\{1,\dots,|\mathcal{V}|\}$ denotes index of vertex in graph. By an embedded value, we usually mean representation of vertex information in $\mathbb{R}^m$. The output of a GNN is typically the same structured graph (same edges and vertices) with optimized $v_i$. The output graph might be used in various ways --- calculate the loss for the next iteration, calculate an aggregation of all values, interpretation of particular $v_i$. The usual task for GNN is node selection, node classification, or graph classification. The most prevalent method for GNN optimization is based on message passing. Each iteration of such GNN consists of three steps:
\begin{itemize}
    \itemsep0em
    \item Compute a message for each of the chosen pairs $(v_i,v_j)$ of vertices (might be all pairs, just neighbouring or other) using values from the previous iteration
    \item Aggregate messages for each vertex $i$ by an aggregation function
    \item Update $v_i$ using aggregated message for $i\in\{1,\dots,|\mathcal{V}|\}$
\end{itemize}

Assuming that all functions in the net are at least piecewise differentiable with respect to their parameters, we can use the stochastic gradient descent optimization approach.

The natural structure of \emph{JSON} document is a tree which is a less general example of a graph. Thus, in theory, GNNs can be used to classify \emph{JSON} documents, yet the approach might be unnecessary computationally expensive \cite{Pevny2020}.

\subsection{HMill framework}
In our experiments, we aim specifically at the hierarchical multiple instance learning (\emph{HMill}) framework defined in \cite{Mandlik2020} because it was designed specifically for tree-structured data and it uses the structure of the input for the model optimization. The authors proposed a use case of \emph{JSON} modelling with good results, and we would like to demonstrate the framework's performance on more complex data. In comparison to the GNNs approach, \emph{HMill} model has better scalability, and it is computationally efficient since a single pass over the data is sufficient (unlike in GNN where you need multiple passes) \cite{Mandlik2020}. Authors adopted and proved the universal approximation theorem \cite{Hornik1991} in the \emph{HMill} situation which shows that \emph{HMill} can approximate any continuous (measurable) function from the space of \emph{JSON} documents to $R^n$. Concerning mentioned facts, we believe \emph{HMill} is more suited for our problem. We will describe this framework in the next chapter.

\hfill \break
Another example of structured data modelling motivated by recurrent and recursive neural networks is in \cite{Woof2020}. More generally, graph-structured data modelling is part of \cite{Henaff2015} or \cite{Borgwardt2005}.