\chapter{Explaining model} \label{chap:expex}
The final part of this thesis is to follow the results of the previous chapter about models and their performance. We want to examine their explanations using \emph{hmill} explainer \cite{Pevny2020} which we described in \ref{chap:explain}. We address one of the goals: to identify artefacts corresponding to different malware behaviour. As artefacts, we can see whatever is among the features of our model. 

Our models classify the presence of signatures. We defined \emph{signature subject} as the fact which the original signature implementation is using for its detection (see \ref{chap:models}). The list of signatures with their subjects is in \ref{app:signatures}. The main question in this chapter is if the original subject correlates or coincides with the model explanation. Another thing that should not be overlooked is the signature part in \emph{report.json} where we can also compare its data part to the explanation. An example of detected signature is in \ref{app:signatures}. Those two and the performance of models from \ref{chap:models} are building blocks of our further discussion.

\section{Explainer}
We performed two explainig experiments using \emph{ExplainMill.jl} (described in \ref{app:technologies}). 

Explainer code is in attachments (\ref{app:attach}). There is nothing extraordinary we used a similar setup as authors of the tool \cite{Pevny2020}. We extracted a couple of examples from the testing set in each run. We attempted to explain only positive examples, which were truly classified into positive class with confidence above the specified threshold. The confidence threshold we used is $0.99$ for the first run and $0.9$ for the second run. We decreed it by $0.1$ if no results were found in the data subset in both cases. We run the explaining process on each of chosen examples separately.

The authors of the original paper concluded that the \emph{BanzHof values} method shows the best performance, and we followed this setup. We involve no clustering method and \emph{level by level heuristic addition random removal} pruning method (due to problems with the library version compatibility, we did not experiment with other clustering and pruning methods). \todo{Maybe I will add one more experiment with a clustered set of examples.}

We received an explanation for each example as a subset of the original \emph{JSON} report. Usually, we have up to one hundred explanations, which we cannot assess in one piece. That is why we involved some additional aggregations. In our case, we merged all explanations for one signature into one \emph{JSON} file and for each entry computed its frequencies. We assume that the most general formulation of an explanation should be seen repeatedly. 

Post-hoc, we decided to involve two more ideas. First, we counted the frequency of a particular key (name of the field in JSON file, e.g. \ \emph{read files, resolved apis}\dots) in explanations such that for each signature, we see how often a particular key is detected by explainer. We can compare the original signature subject with the most seen key if they coincide.

The second idea is that we compute frequencies of entries seen across the different signature. We assume that in such a way, we could identify some bias that is caused by entries that we see in multiple explanations. Such case we should consider being a too general concept. 

All the original outputs and additional aggregations are in attachments (\ref{app:attach}) - merged explanations are in \emph{merged} directory, frequencies of keys are in \emph{freq.json} and merged keys across signatures are in \emph{overall.json}. Parts of results we discuss below.

% \todo{connect to theory in the chapter about explainer theory}

\section{Results and discussion}
The size of the original JSON file with only the behavioural part can be hundreds but even thousands of items (averaged \texttildelow~$3000$ but included even the signature part). Average size of explanation is $3-5$ entries (detail could be see in \ref{app:expl}). In case of low performance signatures like \emph{invalidauthenticodesignature} and \emph{packerentropy} we can see even more than $10$ in both runs.

The number of explanations may vary because of the difference between confidence levels of models. In the second run, we attempted to normalize the number of explanations to be $100$ per signature, but we were not successful in some cases because, in a subsample was just not enough such samples.  In \ref{app:expl} we can see the number of explanations for both runs. The frequency of entry in explanations is every time related to the overall number of explanations.

Our expectation regarding explanations is quite specific. Hopefully, we can find the original subject of signature in some cases and even something else which we can see as more general than the original subject. We know that low-performance models (below $70$~\% of balanced accuracy) explanations are not relevant due to low overall confidence, so we skip them in further discussion.

We assume that the explanations will have very high entropy because of the model input. In principle \emph{C://Programs/app.exe} is something else than \emph{C://Programs2/app.exe}, even if target files might be identical. There are experiments involving some data compression, but we did not use these tool. This problem we reference in the future work section. \todo{If we have time, we can add here the clustering experiment}

We are discussing results after presenting them to expert and having the discussion. The results are often assumptions and hypotheses because we have to anticipate the risks mentioned in \ref{chap:explain}. Especially the \emph{causality X correlation} problem and the \emph{confounding variable existence}. We are aiming at observation description more than concluding.

\paragraph{antidebug setunhandledexceptionfilter}
The most seen keys in explanations are \emph{read keys, resolved APIs, executed commands}, including even API calls, which are the signature subject. Among entries, the most seen are \emph{kernel32.dll.IsProcessorFeaturePresent} (153/377) API and \emph{DisableUserModeCallbackFilter} (34/377) registry key. Those are presented in other explanations once and twice, so it does not look like something too general but also not unique. The registry key is related to exceptions, and the original API call is also related to them.

\paragraph{copies self}
The most seen keys in explanations are \emph{write files, executed commands, delete files} and the first is seen in all explanations, and it also coincides with the original subject. Among entries the most seen are \emph{ikkzowxr.exe} (13/100) file, \emph{WerFault.exe} (13/100) file and \emph{StikyNot yakuza} mutex. The first file is prevalent across different signatures. The mutex is also seen more than one time in explanations.

\paragraph{deletes self}
The most seen keys in explanations are \emph{deleted files, write files, executed commands}. The first is seen in all explanations, but the original signature subject is API calls. Here we can see some generalization because the original signature does not check deleted files directly. However, the model is using it with high performance. We also checked if this trend is not seen in more cases, but this is unique that all explanations include deleted files.

\paragraph{enumerates running processes}
The most seen keys in explanations are \emph{executed commands, mutexes, read keys}. These do not include the original subject, which was API calls.  Among entries, the most seen is \emph{"IESQMMUTEX0208"} (17/84) mutex, but this mutex is quite common. The performance of this classifier is significant, but we cannot generalize to a more specific subset using our explanation.

\paragraph{stealth timeout}
The most seen keys in explanations are \emph{executed commands, files} which does not include the original API calls section. Among entries the most seen is \emph{DisableUserModeCallbackFilter} (11/78) registry. Nevertheless, the situation is the same as in the previous case. We are not able to generalize more.

\paragraph{uses windows utilities}
The most seen key in explanations is \emph{executed commands} which is included in each explanation, and it coincides with the original subject, which is commands. The most frequent commands are \emph{netsh, schtasks.exe}.

\paragraph{removes zoneid ads}
The most seen keys in explanations are \emph{delete files, keys}. The first is seen in each explanation, but the original signature is using API calls. We are not able to identify specific redundancies, but we identified one great conformity. The original signature implementation includes following \emph{\dots .endswith(":Zone.Identifier")} so it is detecting end of API call argument and even \emph{.startswith("DeleteFile")} is detecting name of API starting with specific string. These two facts perfectly correlate with our explanations.

\paragraph{antisandbox sleep}
The most seen keys in explanations are \emph{write keys, keys, read keys} which does not correspond to the original subject. The most seen entries is \emph{HKEY CURRENT USER/\dots} (63/100) registry key. We see this registry key in the case of two signatures. We do not see a direct relation between this key and the original subject. It might be something more general.

\paragraph{dropper}
The most seen keys in explanations are \emph{write files, executed commands, mutexes}. The first is presented in all explanations. The second is not only in negligible fraction. The original subject is not trivial but dropped files are there, which corresponds to the first key. Among entries, the most seen is \emph{IESQMMUTEX0208} (28/71) mutex, but this mutex was mentioned earlier as too general.

\paragraph{stealth network}
The most seen keys in explanations are \emph{keys, files}. The original subject (network) is not presented in the input at all. It looks like registry keys play a significant role. However, neither in the case of registry keys, we can not find any redundancies.

\hfill \break

In particular cases, we can see several situations. Sometimes the model explanations correspond to the original subject. That is a clue that the model uses what we expected, and its generalization might go the right way (e.g. \ \emph{copies self}). There is even a particular case where the original subject does not fit, but the explanations logically correlate with it. In the case of \emph{deletes self} we see key \emph{deleted files}, but the original signature is detecting the same thing but according to \emph{api calls}. This example is unique because the causality is straightforward. The API call causes that the file is deleted and appear even among deleted files. However, the model generalized to that which should not be overlooked. Another example is \emph{stealth network} where we do not see a direct cause of the fact that registry keys are often used in the explanation.

Choosing the most used key is one way, but the second is investigating particular entries (specific calls, files, mutexes\dots). It is challenging to interpret them and connect them to specific subjects because their variance is enormous, as we expected. The most significant is in \emph{removes zoneid ads} where we definitely can see that the model is mainly using the same entries as the original signature (with the same suffix). That should also be considered as a clue that the signature generalize the right way.

Using our method, we were able to identify several too general parts of explanations. We see mutexes that were presented very often across the signatures, and they should be considered confounding variables. Then also, some files are repeatedly seen in reports across the signatures. Both play a significant role in detecting a particular family or classify malware/cleanware, but they should not be used to identify particular behaviour.

In several cases, we cannot identify any direct cause of the model's high performance (e.g. \ \emph{antisandbox sleep}).


\subsection{Final thoughts}
After organizing research in explaining theory chapter \ref{chap:explain} we are cautious. Explaining is a complicated field. We summarized its challenges. We can not be sure about the output, especially using \emph{post hoc} explanation per sample. The explaining algorithm (\emph{Banzhaf values} and  input randomness causes that explanation is a random variable too. 

Nevertheless, our observation evidence that some models strongly involve original subjects. That leads us to future work. The main interest should be improving aggregation of particular explanations, too general explanation detection (across classes), confounding variable detection. It is noteworthy that our \emph{post hoc} explanation should perform better with more extensive datasets as we do not have one regarding the input entropy. However, also the \emph{transparency} approaches should be added to the game. 

Suppose we can make the explanation more accessible to the client (e.g. \ security engineer). In that case, there is a significant chance for \emph{hmill} models to be used during malware analysis in real-world applications. The main reason is their high performance seen in our thesis and \cite{Mandlik2020} \todo{ref even others}. Of course, more complex examples, broader datasets and further testing have to be involved.

% \todo{try to connect it to the theory chapter reference explain theory properly (mainly the desiderata and interpretable model challenges, assumptions, interpretability, explainability, credibility)}







% \todo{are we able to learn more than the sandbox is doing deterministically so far?}

%NICE TO HAVE
%COMPATE SIGNATURES DATA PART WITH THE EXPLANATION
% Would it be better with more samples? (some mathematical magic?)
  %%---------------------
  
% discussion, why we are doing that - I think we can somehow see, what the original signature should care about - what else is significant from the behavior log, but we can see in general other malicious patterns - regardless original signature goal, obviously we see something else which is common
% reason why we want to look at it, about the main question - are we able to learn more than the sandbox is doing deterministically so far?
  % Connect to motivation part of this chapter
  % Do not be black and white, try to formulate hypothesis which should be derived, do not state truth
  % Add some philozophiing... (about the risky things, assumptions, reference explanation theory chapter)
  % Based on \todo{chapter about explanation theory}, mention specific setup (confidence level, pruning method...), configuration and pseudocode of the explainer (all steps, extract only positive...) (we derived it from Pevny's paper, we did not experiment with another setups), reference code in attachement
  % What we did with output more (aggregation of results), how many explanations we have, (two rounds)

  % Motivation about mention Thorsten's advice to look at implementation of python signatures and 
%   hopefully discussion on if the parts contains something significant according to type of signature and so on, or we can find something else than only the deterministic base which the signatue implementation is checking


%   Compare the results to Pevny's Explaner results

%   take signature groups and compare their code to explanation, from each signature type (rest get to the Appendix)
%   report explanation and even some information from logs!!
%   - Time to explain, Explanation size for different types of signatures, input size (Pevny is addressing those)


% Results for each signature (present both attempts)
% performance context and what is the strenght of the explanation (is it relevant)
% What it does (implementation) and if we can see it in explanation
% top of histogram.
% hypothesis we can formulate

% Final thoughts, what categories we can see based on results and what are main problems, compare the explanation of initial two groups of signatures and even try to say something about different subject groups
% we are able to explain and this part should be further researched - describe particular problems including the type of explaning for this model complexity.
% We are experiencing two challenges which were mentioned earlier and those are - huge entrophy (take an example path to file for example) leads to big entrophy of explanation and we do not have some special method to determine similarities (some norm or...) (above our scope), confounding variables which we are not able to identify.
% It would be interesting to thing about the second approach - trasparency, to extract explanation for model not for sample, it is really hard to generalize such conslusions
% I think we can even state that the principle is working and we are able to identify why the model is predicting some things, cons of it is the example based explanation, better could be some kind of dataset based explanation

% - Admit failures on bias variables
%   ○ 

  

% it is not problem to report that result is not easily interpretable and in practise we would have to create way of extracting general knowledge from sample explanations)

% Explanation as another random variable




%   Describe steps - how many samples we have and how we choose them, how we just draw histogram results and try some reasoning
%   - Goal 
% - identify the artifacts corresponding to different malware behavior. Report results.
% - Investigate which parts of the CapeV2 log are important to different malware behavior. 

% - Expert view
%   - Ask Thorsten to try to explain some patterns and domain specific subjects 


%   What we used - specific method (follow explanation theory) - we are choosing only samples where the model is above some confidence and explanation is per sample, we have several explanations to see some trend; we did follow results in Pevny's work, we did not experiment with different kinds of explainers
  

% (This chapter should have simmilar structure as models before)

%   Previous connection
% - We are trying to explain previously trained models
% Way through this chapter
% - describe steps
% - discuss results
% Next connection
% - Next will be conclusions and final statements, we can summarize results of explanation with respect to different models
