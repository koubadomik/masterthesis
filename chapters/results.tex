\chapter{Experimental results} \label{chap:results}
In this chapter we cover results of \emph{hmill} and \emph{hmill Explainer} experiments, their setup, results and discussion. It is a final part of the thesis.



% \chapter{Modelling classifier} \label{chap:models}

\todo{this reasoning have to be on another place}
In this chapter, we present trained \emph{hmill} models and their results. Our goal is to demonstrate that it is possible to train such a model and later explain it. We do not aim to strictly perform a long sequence of experiments and compare to prior. Model is just part of our work.

We do not model all variants of features and states mentioned in the previous chapter. We concentrate on at least one working model, and then we move on to model explaining experiments. From \emph{states} and \emph{features} in question we start with our top candidate - \emph{signatures} as states and \emph{behavior log} as features. 

\section{Behaviour as feature vector}
\todo{this should be on another place, here it should be clean, absordb it in previous chapter}
After further look at the \emph{behavior} part of \emph{report.json} we start with only \emph{summary} and \emph{enhanced} part. The reason is that other parts are quite comprehensive, and we would not be able to train the model with the hardware resources we have. Those two should be sufficient for a comprehensive look at malware behaviour. If the model were still too complex to train under our conditions, we would also omit the \emph{enhanced part}. We would mainly lose the information about the order of events because the summary part lists only artefacts (API calls, commands\dots).

\section{Signatures as states}
Our reasoning about states is based on the fact that the signatures are usually assigned based on some atomic fact, for example, the presence of some API call in the log. Let us call this fact \emph{subject of signature}. It can be one or more patterns seen in the behavioural log. The implementation of signature is deterministically detecting this part and add the signature to the log. We can consider \emph{copies self} signature example. If a specific dropped file that corresponds to the currently analysed one is dropped, the signature is active, and we would find it in \emph{report.json}.

If we choose features where the subjects are presented, we should be able to evaluate the model's explanation. By evaluation, we mean that we can examine the coincidence between the original subject and model explanation. Also, we want to see if the model can generalise the knowledge and maybe come up with some new explanation of particular signature - new better subject. It can also find parts that go with the particular signature's original subject - maybe another malicious behaviour.

More complex labels like malware family or malware score we might explore later. We assume that signatures themselves often support the assignment of those, and it is better to explore the original cause of the whole chain.

After examining the histogram of seen signature in our dataset, we choose a subset based on the frequency. We prefered signatures that are implemented in Python for convenient investigation of the original subject. In the \ref{app:signatures} we can see our candidates and their frequencies in the dataset.

The next task is to investigate the implementation of a particular signature and try to create groups for further conclusion-making. \emph{Python} implementation of each signature processes the output of sandbox and generates boolean values signing that the subject of a particular signature is presented in sample behavioural log. The signature subject can be API calls, file, registry key, process, command, and even more complex. If the signature implementation returns \emph{True}, we observe instance of this signature in the \emph{signatures} part of \emph{report.json}. An example of such a signature item could be seen in \ref{app:signatures}. The most important aside from \emph{name} and \emph{description} is field \emph{data} which sometimes contains signature subject.

We explored two types of grouping aspects. The first is the subject of a particular signature. The second is if we can see this subject in the behavioural log directly. For instance, if the signature implementation uses API calls, the API call list is directly in the log. If it uses the entropy of dropped files, it is not directly in the output. In \ref{app:signatures} we can see a simple example of implementation of \emph{antidebugsetunhandledexceptionfilter} signature, which is just checking the presence of a specific signature.
More about particular signatures and identified categories could be found in \ref{app:signatures}. 

We distinguish two groups based on the direct presence of the original subject in the feature vector. The reason why we categorise signatures in such a manner is that we would like to connect those categories with the model performance in further discussion. We expect that different categories might have different performance.

\section{Model}
The objective is to create a binary classifier that predicts the presence of a particular signature. Later, we can train even a multilabel classifier, assigning a set of signatures according to dynamic analysis. However, the binary classification, its performance and its explanations are the most important for us. 

% Multilabel classifier could be seen as multioutput binary model, so if we are able to scale up the hardware resources and its usage using available techniques (multithreading) multilabel model should be easily deriveable from original simle binary classfier.

Technology stack we used to train \emph{hmill} model is described in \ref{app:technologies}. Important is that we unsuccessfully experimented with multithreading in \emph{Julia} language for gradient computation. The resulting model was trained on one CPU, which a little bit shrank our possibilities.

The first experiments are without reported results because the model was too huge to converge to some significant performance. We were not able to train it on hardware and with thread limitations in a feasible time. This reality led us to skip one part of the feature vector - \emph{enhanced} part. So the resulting model is working with \emph{summary} part only. The tighter feature vector is an advantage for further training time. We still know that summary parts include compressed information aiming at (\emph{api calls, files, mutexes, registries, commands}).

The original code of the model is in thesis attachment \ref{app:attach}. It is a standard binary classifier. What is not so ordinary is that the model is a function of the input data (typical for \emph{hmill}). Another important fact is that the \emph{hmill} framework \emph{API} is general, so it is not accepting JSON documents directly. For this purpose, we use \emph{JsonGrinder} library, which can accept an array of JSON documents and produce schema. The schema is then used to create \emph{hmill} structures from each JSON document. Example of a schema, implied extractor and \emph{hmill} model for our data is in attachment (\ref{app:attach}). In our case, each signature has its own training set and even its schema implied by training set examples.

% \begin{algorithm}
%   \caption{Hmill model training}\label{algo:hmillbinary}
%   \begin{algorithmic}
%       \Procedure{HmillTrain}{$signature\_name, jsons, hyperparameters$}
%       \State $samples \gets load(jsons)$
%       \State $train,test \gets split(samples)$
%       \State $trnstates,tststates \gets extractstates(train, test)$
%       \State $schema \gets getshema(train)$
%       \State $extractor \gets suggestextractor(schema)$
%       \State $model \gets reflectinmodel(schema, extractor, class\_num, hyperparams)$
%       \State $optimize(model, train, trnstates)$
%       \State $metrics \gets evaluate(model, tststates)$
%       \EndProcedure
%   \end{algorithmic}
% \end{algorithm}

\subsection{Model hyperparameters and dataset attributes}
In our experiment, we do not involve techniques like early stopping or hyperparameter tuning. Our goal is to demonstrate the classifier performance. We do not aspire to concentrate on this part only. We build on previous experiments on \cite{Mandlik2020} where the author published even the hyperparameter set in \emph{device id} example. This example was also from the cybersecurity fieldâ€”those parameters we used as the initial setup.

We were not successful immediately, and it took several attempts of our hand-picked adjustments. After each adjustment, we tried to train the model and observed convergence of loss function and balanced accuracy on training and testing data. Since we did not tune hyperparameters or anything else, we did not use a validation set. 
% Due to the excellent ration between testing and training data, signature distribution and convincing performance we did not involve cross validation.

In the table \ref{tab:hyperparams} we can observe hyperparameters of the model and other training-independent facts which we used.


\begin{table}[h]
  \centering
  \caption{Hyperparameters of \emph{hmill} model}
  \begin{tabular}{p{6cm}p{8cm}} 
      \toprule
      \textbf{Parameter} &
      \textbf{Value} \\
      \midrule
      samples & $80000$ (1:1 testing and training) \\
      \midrule
      minibatch size & $1000$ (random subsampled from training set in each iteration)\\
      \midrule
      hidden units (neurons)& $20$\\
      \midrule
      iterations & $120$\\
      \midrule
      optimizer & \emph{ADAM} \cite{Kingma2014}\\
      \midrule
      loss function & \emph{logit cross entrophy} (\ref{chap:classification})\\
      \bottomrule
  \end{tabular}
  \label{tab:hyperparams}
\end{table}


Difference from \cite{Mandlik2020} is \emph{minibatch size} and \emph{number of iterations} which worked for us better in this changed form. Data used in their case were of a much smaller scale (\emph{device id}) which could be why.

We set up the number of \emph{iterations}  so that we observe reasonable convergence. We checked overfitting by monitoring the difference between accuracy on training and testing data. They do not differ significantly.
% We did not challenge this parameter, and we expect further convergence.

\section{Results}
% Chosen metrics are based on metrics which are reported by similar papers \todo{reference Pevny, Madlik and Stiborek}. 
We choose the metrics following the chapter \ref{chap:classification} where we mentioned classifiers in the cybersecurity field and their pitfalls. The calculation was supported by \emph{EvalMetrics.jl} library (\ref{app:technologies}). Results of experiments could be seen in \ref{tab:models_res}, the table is divided into two groups based on categories presented above. There are also some other metrics and visualisations in \ref{app:models}. Resulting models are part of attachment \ref{app:attach}.

Metrics which we are reporting in \ref{tab:models_res} are robust in the case of an imbalanced dataset which is also our case. In the table, we also include a percentage of positive examples to see this disbalance and assess the \emph{FNR} and \emph{FPR}.

\begin{table}[h]
  \centering
  \caption{Models performance (values rounded for 3 decimal spaces, P denotes positive examples ratio in our dataset)}
  % \begin{tabular}{p{5cm}p{3cm}p{3cm}p{3cm}} 
  \begin{tabular}{lllll}
      \toprule
      \textbf{Signature} &
      \textbf{Bal. acc.} &
      \textbf{FNR} &
      \textbf{FPR} &
      \textbf{P [\%]}
      \\
      \midrule
      antidebug setunhandledexceptionfilter & $0.9801$ & $0.0289$ & $0.0109$ & $45$ \\
      \midrule
      copiesself & $0.924$ & $0.125$ & $0.0279$ & $18$ \\
      \midrule
      deletesself & $0.997$ & $0.005$ & $0.002$ & $27$ \\
      \midrule
      enumeratesrunningprocesses & $0.972$ & $0.050$ & $0.007$ & $16$ \\
      \midrule
      stealthtimeout & $0.701$ & $0.064$ & $0.331$ & $21$ \\
      \midrule
      useswindowsutilities & $0.958$ & $0.078$ & $0.006$ & $18$ \\
      \midrule
      removeszoneidads & $0.999$ & $0.000$ & $0.000$ & $28$ \\
      \midrule[0.3pt]
      \midrule[0.3pt]
      antisandboxsleep & $0.969$ & $0.037$ & $0.026$ & $39$ \\
      \midrule
      dropper & $0.911$ & $0.147$ & $0.032$ & $15$ \\
      \midrule
      invalidauthenticodesignature & $0.607$ & $0.668$ & $0.113$ & $36$ \\
      \midrule
      packerentropy & $0.605$ & $0.748$ & $0.043$ & $22$ \\
      \midrule
      stealthnetwork & $0.942$ & $0.008$ & $0.109$ & $66$ \\
      \bottomrule
  \end{tabular}
  \label{tab:models_res}
\end{table}

\section{Discussion}
Original signature implementation is deterministically examining the sandbox output and marking some behaviour as we stated above. The most significant difference between the signatures is if their subject (\emph{api call, file \dots}) is directly among our features or it is not. 

Firstly, let us describe our expectations. Based on this categorisation, we assume that the performance of signatures in the first category is better than the one in the second. We could not be sure that the model would have something according to what it will generalise, but we know that it should consider the subject of the original signature in the first case.

In the case of the first category of signatures, we observe quite consistent performance above $95$~\%. Signature \emph{copiesself} has $92$~\%, which is still sufficient for explaining. This deviation could be caused by the fact that the original signature is examining dropped files and checking if the current file is among them. Nevertheless, the filename varies, so the entropy might be very high and might cause big \emph{FNR}. Signature \emph{deletesself} might have similar issues, but it is determining according to API calls and not dropped files which could cause that its performance is better. The only significant outlier in this group is \emph{stealthtimeout} signature which examines a sequence of API calls that could be quite complicated. After going through some files where this signature was presented, we could not identify the particular API calls in the original log, so that the assignment might be more tricky. The main cause of low accuracy is, in this case, \emph{FPR}.

Despite some anomalies that we inspect in the next chapter, this signature group's performance is as we expected. 

% Even the \emph{stealthtimeout} we will explain on positive examples with high confidence to see what is in the game there.

The second group behaves more unexpectedly because the performance in 3 of 5 cases is above $90$~\%. Signature \emph{antisandboxsleep} uses API calls in a more complex way, so the original classifier may involve these. \emph{Dropper} has significantly larger \emph{FNR}. However, its performance is still good for explanations. \emph{Stealthnetwork} should look at network activity which is not among the features. The excellent performance of some signatures in this group arouses our interest in the explaining part. In cases of \emph{invalidauthenticodesignature} and \emph{packerentropy} explanation, we should not take it too seriously due to the performance. Overall the first group has statistically better performance as we expected, but this comparison is not so relevant.

We also performed several experiments with a more general multilabel classifier, but we did not observe convergence under our hardware resource conditions.

This chapter concludes that we can train \emph{hmill} classifier to classify the presence of a particular signature based on the summary part of the behavioural log from \emph{CAPEv2} sandbox. This classifier has a significant performance of more than $90\%$ as balanced accuracy for most signatures, which is sufficient for further explaining experiments. Such models should have strong confidence for us to choose some samples for explainer (\ref{chap:explain}). 

% We are able to classify only some of signatures with such performace which we mentioned in discussion.

In the explanation chapter, we should examine different categories of signatures based on their performance. The models with our intuition should investigate such that they are using what the original signature did. The unexpectedly bad ones we should not try to explain because of their low confidence. The unexpectedly good ones we need to explain to see the subject used by the model.


%---------------------

% \chapter{Explaining model} \label{chap:expex}
The final part of this thesis is to follow the results of the previous chapter about models and their performance. We want to examine their explanations using \emph{hmill} explainer \cite{Pevny2020} which we described in \ref{chap:explain}. We address one of the goals: to identify artefacts corresponding to different malware behaviour. As artefacts, we can see whatever is among the features of our model. 

Our models classify the presence of signatures. We defined \emph{signature subject} as the fact which the original signature implementation is using for its detection (see \ref{chap:models}). The list of signatures with their subjects is in \ref{app:signatures}. The main question in this chapter is if the original subject correlates or coincides with the model explanation. Another thing that should not be overlooked is the signature part in \emph{report.json} where we can also compare its data part to the explanation. An example of detected signature is in \ref{app:signatures}. Those two and the performance of models from \ref{chap:models} are building blocks of our further discussion.

\section{Explainer}
We performed two explainig experiments using \emph{ExplainMill.jl} (described in \ref{app:technologies}). 

Explainer code is in attachments (\ref{app:attach}). There is nothing extraordinary we used a similar setup as authors of the tool \cite{Pevny2020}. We extracted a couple of examples from the testing set in each run. We attempted to explain only positive examples, which were truly classified into positive class with confidence above the specified threshold. The confidence threshold we used is $0.99$ for the first run and $0.9$ for the second run. We decreed it by $0.1$ if no results were found in the data subset in both cases. We run the explaining process on each of chosen examples separately.

The authors of the original paper concluded that the \emph{BanzHof values} method shows the best performance, and we followed this setup. We involve no clustering method and \emph{level by level heuristic addition random removal} pruning method (due to problems with the library version compatibility, we did not experiment with other clustering and pruning methods). \todo{Maybe I will add one more experiment with a clustered set of examples.}

We received an explanation for each example as a subset of the original \emph{JSON} report. Usually, we have up to one hundred explanations, which we cannot assess in one piece. That is why we involved some additional aggregations. In our case, we merged all explanations for one signature into one \emph{JSON} file and for each entry computed its frequencies. We assume that the most general formulation of an explanation should be seen repeatedly. 

Post-hoc, we decided to involve two more ideas. First, we counted the frequency of a particular key (name of the field in JSON file, e.g. \ \emph{read files, resolved apis}\dots) in explanations such that for each signature, we see how often a particular key is detected by explainer. We can compare the original signature subject with the most seen key if they coincide.

The second idea is that we compute frequencies of entries seen across the different signature. We assume that in such a way, we could identify some bias that is caused by entries that we see in multiple explanations. Such case we should consider being a too general concept. 

All the original outputs and additional aggregations are in attachments (\ref{app:attach}) - merged explanations are in \emph{merged} directory, frequencies of keys are in \emph{freq.json} and merged keys across signatures are in \emph{overall.json}. Parts of results we discuss below.

% \todo{connect to theory in the chapter about explainer theory}

\section{Results and discussion}
The size of the original JSON file with only the behavioural part can be hundreds but even thousands of items (averaged \texttildelow~$3000$ but included even the signature part). Average size of explanation is $3-5$ entries (detail could be see in \ref{app:expl}). In case of low performance signatures like \emph{invalidauthenticodesignature} and \emph{packerentropy} we can see even more than $10$ in both runs.

The number of explanations may vary because of the difference between confidence levels of models. In the second run, we attempted to normalize the number of explanations to be $100$ per signature, but we were not successful in some cases because, in a subsample was just not enough such samples.  In \ref{app:expl} we can see the number of explanations for both runs. The frequency of entry in explanations is every time related to the overall number of explanations.

Our expectation regarding explanations is quite specific. Hopefully, we can find the original subject of signature in some cases and even something else which we can see as more general than the original subject. We know that low-performance models (below $70$~\% of balanced accuracy) explanations are not relevant due to low overall confidence, so we skip them in further discussion.

We assume that the explanations will have very high entropy because of the model input. In principle \emph{C://Programs/app.exe} is something else than \emph{C://Programs2/app.exe}, even if target files might be identical. There are experiments involving some data compression, but we did not use these tool. This problem we reference in the future work section. \todo{If we have time, we can add here the clustering experiment}

We are discussing results after presenting them to expert and having the discussion. The results are often assumptions and hypotheses because we have to anticipate the risks mentioned in \ref{chap:explain}. Especially the \emph{causality X correlation} problem and the \emph{confounding variable existence}. We are aiming at observation description more than concluding.

\paragraph{antidebug setunhandledexceptionfilter}
The most seen keys in explanations are \emph{read keys, resolved APIs, executed commands}, including even API calls, which are the signature subject. Among entries, the most seen are \emph{kernel32.dll.IsProcessorFeaturePresent} (153/377) API and \emph{DisableUserModeCallbackFilter} (34/377) registry key. Those are presented in other explanations once and twice, so it does not look like something too general but also not unique. The registry key is related to exceptions, and the original API call is also related to them.

\paragraph{copies self}
The most seen keys in explanations are \emph{write files, executed commands, delete files} and the first is seen in all explanations, and it also coincides with the original subject. Among entries the most seen are \emph{ikkzowxr.exe} (13/100) file, \emph{WerFault.exe} (13/100) file and \emph{StikyNot yakuza} mutex. The first file is prevalent across different signatures. The mutex is also seen more than one time in explanations.

\paragraph{deletes self}
The most seen keys in explanations are \emph{deleted files, write files, executed commands}. The first is seen in all explanations, but the original signature subject is API calls. Here we can see some generalization because the original signature does not check deleted files directly. However, the model is using it with high performance. We also checked if this trend is not seen in more cases, but this is unique that all explanations include deleted files.

\paragraph{enumerates running processes}
The most seen keys in explanations are \emph{executed commands, mutexes, read keys}. These do not include the original subject, which was API calls.  Among entries, the most seen is \emph{"IESQMMUTEX0208"} (17/84) mutex, but this mutex is quite common. The performance of this classifier is significant, but we cannot generalize to a more specific subset using our explanation.

\paragraph{stealth timeout}
The most seen keys in explanations are \emph{executed commands, files} which does not include the original API calls section. Among entries the most seen is \emph{DisableUserModeCallbackFilter} (11/78) registry. Nevertheless, the situation is the same as in the previous case. We are not able to generalize more.

\paragraph{uses windows utilities}
The most seen key in explanations is \emph{executed commands} which is included in each explanation, and it coincides with the original subject, which is commands. The most frequent commands are \emph{netsh, schtasks.exe}.

\paragraph{removes zoneid ads}
The most seen keys in explanations are \emph{delete files, keys}. The first is seen in each explanation, but the original signature is using API calls. We are not able to identify specific redundancies, but we identified one great conformity. The original signature implementation includes following \emph{\dots .endswith(":Zone.Identifier")} so it is detecting end of API call argument and even \emph{.startswith("DeleteFile")} is detecting name of API starting with specific string. These two facts perfectly correlate with our explanations.

\paragraph{antisandbox sleep}
The most seen keys in explanations are \emph{write keys, keys, read keys} which does not correspond to the original subject. The most seen entries is \emph{HKEY CURRENT USER/\dots} (63/100) registry key. We see this registry key in the case of two signatures. We do not see a direct relation between this key and the original subject. It might be something more general.

\paragraph{dropper}
The most seen keys in explanations are \emph{write files, executed commands, mutexes}. The first is presented in all explanations. The second is not only in negligible fraction. The original subject is not trivial but dropped files are there, which corresponds to the first key. Among entries, the most seen is \emph{IESQMMUTEX0208} (28/71) mutex, but this mutex was mentioned earlier as too general.

\paragraph{stealth network}
The most seen keys in explanations are \emph{keys, files}. The original subject (network) is not presented in the input at all. It looks like registry keys play a significant role. However, neither in the case of registry keys, we can not find any redundancies.

\hfill \break

In particular cases, we can see several situations. Sometimes the model explanations correspond to the original subject. That is a clue that the model uses what we expected, and its generalization might go the right way (e.g. \ \emph{copies self}). There is even a particular case where the original subject does not fit, but the explanations logically correlate with it. In the case of \emph{deletes self} we see key \emph{deleted files}, but the original signature is detecting the same thing but according to \emph{api calls}. This example is unique because the causality is straightforward. The API call causes that the file is deleted and appear even among deleted files. However, the model generalized to that which should not be overlooked. Another example is \emph{stealth network} where we do not see a direct cause of the fact that registry keys are often used in the explanation.

Choosing the most used key is one way, but the second is investigating particular entries (specific calls, files, mutexes\dots). It is challenging to interpret them and connect them to specific subjects because their variance is enormous, as we expected. The most significant is in \emph{removes zoneid ads} where we definitely can see that the model is mainly using the same entries as the original signature (with the same suffix). That should also be considered as a clue that the signature generalize the right way.

Using our method, we were able to identify several too general parts of explanations. We see mutexes that were presented very often across the signatures, and they should be considered confounding variables. Then also, some files are repeatedly seen in reports across the signatures. Both play a significant role in detecting a particular family or classify malware/cleanware, but they should not be used to identify particular behaviour.

In several cases, we cannot identify any direct cause of the model's high performance (e.g. \ \emph{antisandbox sleep}).


\subsection{Final thoughts}
After organizing research in explaining theory chapter \ref{chap:explain} we are cautious. Explaining is a complicated field. We summarized its challenges. We can not be sure about the output, especially using \emph{post hoc} explanation per sample. The explaining algorithm (\emph{Banzhaf values} and  input randomness causes that explanation is a random variable too. 

Nevertheless, our observation evidence that some models strongly involve original subjects. That leads us to future work. The main interest should be improving aggregation of particular explanations, too general explanation detection (across classes), confounding variable detection. It is noteworthy that our \emph{post hoc} explanation should perform better with more extensive datasets as we do not have one regarding the input entropy. However, also the \emph{transparency} approaches should be added to the game. 

Suppose we can make the explanation more accessible to the client (e.g. \ security engineer). In that case, there is a significant chance for \emph{hmill} models to be used during malware analysis in real-world applications. The main reason is their high performance seen in our thesis and \cite{Mandlik2020} \todo{ref even others}. Of course, more complex examples, broader datasets and further testing have to be involved.

% \todo{try to connect it to the theory chapter reference explain theory properly (mainly the desiderata and interpretable model challenges, assumptions, interpretability, explainability, credibility)}

