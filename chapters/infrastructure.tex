\chapter{Infrastucture and data collection} \label{chap:infrastructure}
This chapter describes the realization of the data collection process using \emph{CAPEv2} sandbox. The sandbox is described in the chapter \ref{chap:analysis}, here we focus on the specific setup, problems we experienced and their solution. We have a data source of malware samples mentioned in the thesis introduction https://bazaar.abuse.ch/. The output of this task is a dataset of dynamic malware analyses. It includes behavioural features and signatures, both input for our \emph{HMill} model.

Although this chapter is shorter than the previous ones, we spent the most significant time on this task. All scripts and other outcomes are part of the attachment (\ref{app:attach}), and the most important tools are listed in \ref{app:technologies}.

\section{Host machine}
The host machine is where the sandbox environment and virtualization software is running. We know that an analysis of one malware sample takes up to five minutes, and we want to have as many samples as possible. That is why we want to run several distributed host machines. 

The whole process of a host machine initialization is automated to be able to set up multiple machines. The initialization consists of several steps:
\begin{itemize}
    \itemsep0em 
    \item Install host operating system - recommended Ubuntu
    \item Enable SSH to be able to access remotely
    \item Enable basic security - firewall and supporting tools
    \item Install virtualization software - recommended KVM QEMU
    \item Copy virtual machine images to the host machine
    \item Sandbox initialization and configuration
    \item Data collection script initialization
\end{itemize}

To run all steps on multiple machines at once, we used Ansible, a network orchestration tool. We also added some usual management functions - copy the new configuration from the server and clean up the sandbox data.

Hardware resources on host machines are 256GB SSD and 16GB RAM, which means that a sufficient number of virtual machines per one host is four. We experimented with more, but there was an overload which might lead to biased analysis results.

Various issues accompanied the automation of the whole process. Such a process often needs manual steps, and their automation is very challenging. Issues were caused mainly by our low experience. Sometimes poor documentation was involved. Especially the process of virtual machine images creation and copying was connected with issues. Overall, we have configured seven host machines.

\section{Guest machine}
Guest denotes the virtual machine where the malware sample runs and where the \emph{CAPEv2} monitor operates. We used Windows 7 as an operating system. The crucial goal of the guest machine is to look like an ordinary computer that is in regular use. Due to the virtualization usage, we have to care about the sandbox evasion techniques mentioned in \ref{chap:analysis}.

There are two options for anti-evasion setup in \emph{CAPEv2} sandbox. Firstly, we experimented with vmcloak\footnote{https://github.com/hatching/vmcloak}. We were able to run and use this tool. However, it supports only VirtualBox, which is not recommended by \emph{CAPEv2} because of its performance. The project is also no longer maintained, and some functions did not work (taking snapshots). The second option is to use a script recommended by sandbox contributors\footnote{https://github.com/doomedraven/Tools/blob/master/Virtualization/kvm-qemu.sh} and perform manual steps in the virtual machine configuration\footnote{https://www.doomedraven.com/2016/05/kvm.html\#modifying-kvm-qemu-kvm-settings-for-malware-analysis}. After several unsuccessful attempts with misconfigurations in the low-level virtual machine configuration, we were able to create four working images.

The sandbox requires disabling the firewall and running Python on the guest machine. We added the most popular applications like Google Chrome, Firefox, Adobe reader, Spotify\dots. We added one private key to C://Users/Administrator/.ssh and one password to the Google Chrome password database. We downloaded random images and documents from the internet.

\section{Network setup}
The network setup is a crucial point in the dynamic analysis. The guest machine has to reach the host machine to stay in touch with the result server. Secondly, there is an internet connection for the guest, which might be necessary for some malware types. As an example, we can see \emph{dropper}, which is responsible for downloading a payload.

We decided to collect the data under two different conditions - with internet connection (denoted by \emph{internet}) and without (denoted by \emph{none}). We engaged both because the \emph{internet} is much more difficult to set up and secure, and we wanted to start data collection as soon as possible. Both architectures are figured in the appendix \ref{app:network}.

\subsection{None}
\emph{None} setup is a straightforward option for an isolated network between host and guest. This approach requires a host-only interface created in \emph{KVM} virtualization tool. Host and guest are assigned with IP addresses from the same range. From a security point of view, we have to set up a firewall on the host machine. It should accept connections from the isolated network only on the result server's port.

\subsection{Internet}
We want to provide internet access to the guest machine during the analysis run. For this purpose, we prepared a VPN connection to the secured network through which the communication should be forwarded. That is considered a good practice to observe what the samples are doing and be able to stop it fastly. This network we call \emph{dirty lab}.

\emph{CAPEv2} supports VPN connection setup for each guest machine separately. We knew that it would be better to have a central gateway for all local traffic than connecting each particular guest machines to \emph{dirty lab}. However, we decided to use native sandbox functions at first. After experiencing some issues with \emph{CAPEv2} VPN configuration and an unsuccessful issue reporting, we decided to find another custom way. 

The main requirement is to centralize the traffic from the local network (malicious) to \emph{dirty lab}. The exit point we call \emph{router}. The surrounding university network has to be secured and isolated from malicious traffic. We also need to monitor host machines because of external intrusion and centralize logs from host machines.

In the following text we use \emph{l2} and \emph{l3} as a designation referring to ISO/OSI model of network communication \cite{Zimmermann1980}. By \emph{l2} we mean communication on the data link layer. Specifically, we mean ethernet/802.11 local networks where MAC (Media access address) is used for device identification. By \emph{l3} we mean IP communication on the network layer. IP addresses are used for device identification in \emph{l3}.

An expert recommended the designed network architecture after a consultation. Its basic idea is to avoid \emph{l3} communication on the local network such that IP address from one range is assigned only to the guest machine and then to the router device. This idea was supported mainly by the \emph{l2} VPN on the local network.

The role of the router is to receive the traffic going out of the local network and forward it to the \emph{dirty lab}. It can also monitor and capture the traffic. In \emph{dirty lab}, we were provided with the ipv4 interface only, so the \emph{router} performs network address translation (NAT). \emph{Router} machine is running Ubuntu operating system. It is configured to connect to \emph{dirty lab} using \emph{l3} VPN and to the guest machines using \emph{l2} VPN. In \emph{l2} VPN, \emph{router} is a server and in \emph{l3} VPN, it is a client (server is running in \emph{dirty lab}). \emph{Router} is a virtual machine for fast recovery capability. All logs from the server are sent to the central machine.

The host machine has to be configured as a client in communication with \emph{router} using \emph{l2} VPN to forward the guest's traffic. The idea of this setup is that all the traffic leaving this device is encapsulated in packets with university network IP addresses. This network is unknown to the guest device where malware is running. From the guest's point of view, the connection between the \emph{router} and \emph{guest} is on \emph{l2}. 

On the host, the interface for \emph{l2} VPN communication (TAP) is bridged with the original interface, which allows communication with the guest virtual machine (originally host-only). 

A connection to the internet from the guest machine goes through the host machine, the router, the dirty lab, and the internet. 

There has to be an additional setup on the host machine besides these listed in the case of \emph{none}. Each host machine in distributed cluster has to send all sandbox logs to the central Syslog server. All machines have to be set up with a monitoring tool to detect intrusion\footnote{example https://aide.github.io/}.

\section{Data collection pipeline}
During the distributed data collection, we used the following terminology. \emph{Master} is a machine responsible for sample distribution, and it has access to the NAS, where it stores analysis results. \emph{Worker} is another name for \emph{host} machine in the context of distribution.

This section describes the whole process of data collection from a malware sample over its dynamic analysis and ending with the behavioural features and signatures extracted from the \emph{JSON} report. All programs implemented to solve the mentioned problems are part of the attachment (\ref{app:attach}). Particular steps are automated, and their description follows.

\begin{enumerate}
    \itemsep0em 
    \item Download samples from the \emph{malwareBazaar}
    \item Filter PE files only
    \item Retrieve additional metadata
    \item Add hashes to database
    \item Distribute samples to workers
    \item Analyze sample and send result back
    \item Store result
    \item Extract JSON report
    \item Prune unnecessary parts
\end{enumerate}


\subsection{Abuse.ch malwareBazaar}
The place where we downloaded malware samples was abuse.ch\footnote{https://abuse.ch/} specifically its part called \emph{malwarebazaar}. The reason for its usage is its free access without any claims. \emph{MalwareBazaar} is a database of malicious (no benign files or adware) samples where everybody can share and download. In May of 2021, it contains over 325~000 samples. Malware samples are downloaded in compressed form - one archive for every day since the start of the project.

\subsection{File filtering}
Our project aims only at PE files described in \ref{chap:analysis}. After decompression of the original archives, we filter the files based on the file extension and file headers. We also filter the compressed files, decompress them, and again filter PE files only.

\subsection{Metadata}
The secondary intention was to obtain some basic metadata about each file to have basic information about each sample. We were able to gain academic access to the VirusTotal API\footnote{https://developers.virustotal.com/v3.0/reference}. We downloaded a metadata report for each of our samples. The report contains basic static information like hashes and fuzzy hashes, extracted strings, detection of various antiviruses, and even a summary of reports of used sandboxes.

\subsection{Distributed sandbox}
After dealing with issues in the host initialization part and even in the VPN setup part, we also encountered issues while setting up the distributed sandbox. \emph{CAPEv2} can orchestrate multiple host machines. It uses distributed mongo database\footnote{https://www.mongodb.com/} combined with a script that is run on the master machine to check the connection to registered worker devices. We spent with the configuration of distributed \emph{CAPEv2} large amount of time trying multiple different ways and following several pieces of advice but unsuccessfully. We decided to implement our lightweight solution for time reasons.

Hashes of our files are stored in a database (in our case \emph{JSON} file) with additional attributes. A script runs on the master machine that distributes samples among worker machines using their REST API. After the analysis is done, another script on the worker machine compresses the result and sends it back to the master. The last script manages the coming results and saves them to the NAS. Everything is recorded in the database.

\subsection{Result postprocessing}
We need only part of \emph{JSON} log, specifically behavioural features and signatures, for further modelling. Its extraction was done in two steps. Firstly, we decompressed the analysis result and extracted only \emph{JSON} log. Secondly, we extracted the mentioned parts and saved the shrank output. The whole report might have even gigabytes. However, the shrank variant has usually tens of megabytes. We could transfer the output to the metacentre where the model computation and explanation took place.

\section{Collected dataset}
When we start modelling experiments, we have a dataset consisting of $80000$ different samples in \emph{none} network setup. The \emph{internet} configuration took us several weeks to deal with, and the data collection started later and was slower. The \emph{internet} data will be investigated in future work as its collection continues. 

The complete dataset has approximately $2,5$~\emph{TB} in compressed form. Extracted features and signatures are much smaller (tens of gigabytes). 

Not all outputs of the sandbox described in \ref{chap:analysis} are presented in each analysis result. The configuration and other conditions determine it.

After examining the histogram of seen signatures, we chose a subset based on their frequency in the training set. We prefered signatures that are implemented in Python for convenient investigation of the original cause. In the \ref{app:signatures}, we can see our candidates, their frequencies in the dataset, and additional information, including even the groups described in chapter \ref{chap:classification}.

% The data in such domain are very biased. Sometimes we can see that malware can not be run under the conditions in a guest or is trying to use some \emph{.NET} library and if not found, it kills itself. The fact that we do not have an internet connection can make this bias stronger, so we have to count on it to decide what to use to train the model. We do not want to investigate network activities even if sometimes it contains something (malware attempts).



% We have some of the results which were mentioned in \ref{chap:infrastructure}. When we start the following reasoning, we have the proper amount of the data without internet access because the setup is straightforward and more accessible. 

% \section{Results of analysis}
% The outputs of the previous chapter are dynamic malware analysed samples. Even such experiments used machine learning algorithms on all the produced data, but this is very performance demanding because the output may have 15~\emph{MB} but even gigabytes. That is why we want to choose the only subset containing the most information about the run and behaviour. 

% The goal of this thesis is to use \emph{hmill} model (comprehensively described in \ref{chap:hmill}), which can work with graph-structured data and especially with JSON files. This fact we would like to use.

% Our approach is to create a simple binary classifier and then scale up to multiclass or even multilabel if possible regarding performance and time. This chapter describes our reasoning on choosing features and hidden states for our \emph{hmill} classifier.  Collected results are input, and the expected output is a set of features and states (maybe more than one possibility). This decision should be based on the data we have and the experience observed in prior work.

% Complete description of the output of the sandbox is in appendix \todo{reference}.
% Description what everything we have at the end for every single sample, size of dataset (GB and number of samples)...
% Try to compare to another public dataset with the attributes and mention that using this way is quite universal
% What we have X what the docs says (caused our misconfiguration...) - https://cuckoo.readthedocs.io/en/latest/usage/results/ (reference cuckoo book)

% \chapter{Data structure, features and states} \label{chap:data}

% \subsection{Sample distribution}
% Reference scripts and crons..., sample storage
% problems using provided distribution (concretize some, extract from notes, poor docs,..)
% using nas storage and so on...


% First phase of our work is about running chosen sandbox to capture sigificant number of analysis results for further use. This task could be divided into two subtask - building infrastructure and data collection. Due to the fact that we would like to collect more than $50 000$ samples and usual analysis take $5$ minutes we will need more than one active instance so the distribution is another challenge.


% Download samples from abuse
% Filter samples by file type (PE) - filtered directly pe files and then extracted zip files and filtered again
% Add filtered samples to database (store in json file, which I backup regularly, later could be improved)
% Retrieve additional metadata for files (VT)
% Distribute samples to slaves (each file should be analysed in each mode, for distribution REST API of cape sandbox is used)
% Collect and store results

% VT, Metadefender and abuse reports, mention academic access for VT so we end up with that only

% There is some useful info in this data. But nothing totally awesome. But good enough for pre-sorting.
% Interesting are:

% Abuse.CH:
% - First seen: Us the oldest date to estimate how old the sample is. We need that for time scales
% - ssdeep: Fuzzy hash, good to find similar samples
% - sha256: Most common hash to link them together
% - md5/.../other hashes: Sometimes used in articles. Keep them for reference

% Metadefender:
% Multi-scanner. Be aware to check which Av detected it. Only use the top 10 (check out av-comparatives or av-test)
% Creation date is important. If the sample is older the AVs had some time to add detection (older: Time between first seen and detection)


% Virustotal:
% Similar to metadefender. Plus: It has update time of engine.
% The other data in here:
% - Packers: Malware can be packed. A nice hint, if available
% - Import list: Used DLL functions. Some malware is loading external functionality during runtime....
% - Resources: Attached malware parts (encrypted => high entropy). Maybe also abusing Icons of well known tools (=> trojans)
% - Signature: Could be helpful to weed cleanstuff our. If it is properly signed by a authority we can trust....
% If we want to classify malware as the first AV vendor seeing it, we can not use the detections others have on it. Obviously.
% HTH
% Thorsten

% figure of the setup, description, argumentation
% similar projects (isolation of malware network having internet access)
% two branches - no internet, internet
% internet
%     describe in detail
%     firewall
%     security resoning
%     syslog...
% safe network made our work easier, mention couple of further ideas mentioned by Josef (thank to Josef at the beginning of thesis!)


% add literally every detail including some scripts to appendix, even configuration of network, vpn, reference manuals, security reasoning during setting up internet connection (see notes)...

% Previous connection
% - Follow up analysis chapter and summarize what we did
% Next connection
% - At the end should be described output of sandbox and at the beggining of next chapter should be stated what we have chosen and why



% Possible sources
% Chosen source
% - recommendation from team member
% - description of Abuse.ch
% - possible problems - bias?, Emotet results?, Some statistic from the report from VT which I downloaded (nice-to-have), maybe could be quite interesting to retrive list of file extentions - histogram?


% 


% - Realization of chapter 'data'
% - our infrastructure - distribution, other types of analyses, network infrastructure (marginally, I am not going to use those data, I think, but I performed it and I can write about the risks and so on...)
% - used tools
% - distrbution
% - pipeline
% - run
% - results
% - Conclusions, comming-outs, summary, discussion


% \todo{absorb chapter about data here, describe the output and marginally discussion on what should be feature vector and classes (but present it as ready decision - from the introduction)}

% \todo{this chapter could be absorbed into next chapter if it is too short...}
% \todo{do not forget to add goals to each chapter and go through the trash below each chapter!!}


% Lastly thrown out
% We would like to base our decision on the fact that in \ref{chap:analysis} we stated that crucial observations during the program run are \emph{processes, files, mutexes, registries, commands, api calls}.

% \todo{avoid background things, that should be moved to analysis part, here strictly what we solved and architecture and so on}

% NICE to have
% We can try to create some histogram using the data from virus total (families or something like that...)
% \todo{checkout some web interface example, if there is something else, and even configuration files}
% \todo{add sections of report from virus total and its features}
% https://developers.virustotal.com/v3.0/reference#files
% \todo{add some image}

%%----------------------------------------------------------------------------------------------

% This statement is truth even physically such big data amounts we have to solve on bigger cluster like cesnet.metacenter. \todo{mention data amount before and after processing pipeline}
% After we decided which data parts are useful we used linux tools and Julia language to \todo{reference Julia in appendix technology description and maybe even lazyjson library} extract what we need from original analysis results \todo{reference what we used in bash - if necessary (maybe I can add to attachment bash script with trasformation like that if needed), and reference pruner code in attachements, we should have appendix description of attachments}


% "behavior
%     "processes, "processtree  - list of processes related to malware run with details (api name, arguments...)
%     "summary
%         "files
%         "read_files
%         "write_files
%         "delete_files
%         "keys
%         "read_keys
%         "resolved_apis    
%     "enhanced - detail sequence of events (library loading, api calls with parameters...)


% \todo{more about them we say in chapter on modelling, or maybe here give example of code and signature detection and data part (and all parts) and to appendix we can add more, with reference to the repository}

% Describe parts of json report
% JSON report parts \todo{describe that json report is quite comprehensive and contain even earlier mentioned outputs} \todo{to appendix we should add some example of report.json}
% - size vary a lot, could be from 10MB to GBs
% lack of docs - https://github.com/cuckoosandbox/cuckoo/issues/2458

% "statistics - time statistics for particular part of malware analysis
% "info - sandbox details (machine, category, used modul...)
%     "version"
%     "started"
%     "ended"
%     "duration"
%     "id"
%     "category"
%     "custom"
%     "machine
%     "package"
%     "timeout"
% "CAPE - extraxted payload
% "behavior
%     "processes
%     "processtree
%     "summary
%     "enhanced
% "debug - debug log of sandbox
% "deduplicated_shots - screenshots
% "network
%     "domains
%     "tcp
%     "udp
%     "dns
%     "pcap_sha256"
%     "sorted_pcap_sha256"
% "static - static analysis of files
% "strings - extracted strings (static analysis technique)
% "suricata - network detection Suricata tool log
% "target - sample details
% "malfamily_tag"
% "signatures - list of signatures and their data
% "malscore"
% "ttps

% paragraph about each and resoning on if we can use it or not, and what kind of analysis can be based on particular part (I think we enumerated almost all parts of original log)


% Summary of the data set - How many samples, size...
% Also we can try to specify the requirements for our model (quite big amount of data - all the reports), we are not able to process whole outputs, reference hmill capacities on previous experiments (for example in Julia language)


% \todo{Final result of data collection - no internet samples (maybe mention at the end of previous chapter), I did collect some, but did not use them in further parts because priority was not to have everything}

% Their analysis is in future work same as data from static analysis (Virus total). These data are still part of our data set and was big part of our job so far. So the subject of our research is sandox output.

% Previous connection
% - Follow up statements about data in cybersecurity, follow previous chapter with choosing relevant parts of logs
% Way through this chapter, we do not use internet access data, because we did not have it at right time, no internet data are sufficient for our further experimentation
% Next connection
% - Chosen data will be used for further experiments


% reference some source that is talking about data in cyber security, resp. on theory part

% Data processing
% extracting desired parts
% Is there something else in processing?
% Pruning jsons in Julia...

% reference that we did use similar parts as predecessors (Pevny, Mandlik, Stiborek)

% Detail descriptions of candidates, for example describe signatures (just describe what they do and sort them to categories according to what they are doing) and give example and at once slightly say why the signatures could be interesting (based on behavior like this we can validate model, such that it is using the proper part of report, but we can see whatever else which looks like malicious)


% Next chapter - Due to the data quality we are not able to use some of them

% Get several reasonings for some parts of analysis, what we want and we do not, also we can compare it to Stiborek and other works (for example Mandlik had quite straight-forward dataset and we know that it worked), we can really investigate the whole output and present ideas. 


% At the end we can have more variants, but at the end we did examine two and that should not be a problem