\chapter{Infrastucture and data collection} \label{chap:infrastructure}
This chapter desribes realization of data collection process using \emph{CAPEv2} sandbox, problems we experienced and their solution. Theoretical background is in \ref{chap:analysis}. At the beginning we have only data source of malware samples mentioned in the thesis introduction \url{https://bazaar.abuse.ch/}. The output of this this task is dataset of dynamic malware analyses. It includes behavioral features and signatures, both input for our \emph{hmill} model.


\todo{avoid background things, that should be moved to analysis part, here strictly what we solved and architecture and so on}


https://github.com/koubadomik/cuckoo-distr-docs

First phase of our work is about running chosen sandbox to capture sigificant number of analysis results for further use. This task could be divided into two subtask - building infrastructure and data collection. Due to the fact that we would like to collect more than $50 000$ samples and usual analysis take $5$ minutes we will need more than one active instance so the distribution is another challenge.

We will describe the data collection process in detail. Downloaded malicious samples are our input and dynamic malware analysis is our output. Our pipeline could be seen on the figure \todo{add reference}


\todo{clarify it in thesis, reference section about abuse.ch!}

TODO: create image of pipeline 

\section{Samples}
Possible sources
Chosen source
- recommendation from team member
- description of Abuse.ch
- possible problems - bias?, Emotet results?, Some statistic from the report from VT which I downloaded (nice-to-have), maybe could be quite interesting to retrive list of file extentions - histogram?
Appendix: scraper script from abuse.ch
\section{Sample filtering}
Principle of detecting PE files - my way (reference file command,...)
Mention even that some files are zipped and we decompressed them and added results between our samples
Mention from some book how we can do this - I saw it in static analysis

\section{Static analysis}
VT, Metadefender and abuse reports, mention academic access for VT so we end up with that only

There is some useful info in this data. But nothing totally awesome. But good enough for pre-sorting.
Interesting are:

Abuse.CH:
- First seen: Us the oldest date to estimate how old the sample is. We need that for time scales
- ssdeep: Fuzzy hash, good to find similar samples
- sha256: Most common hash to link them together
- md5/.../other hashes: Sometimes used in articles. Keep them for reference

Metadefender:
Multi-scanner. Be aware to check which Av detected it. Only use the top 10 (check out av-comparatives or av-test)
Creation date is important. If the sample is older the AVs had some time to add detection (older: Time between first seen and detection)


Virustotal:
Similar to metadefender. Plus: It has update time of engine.
The other data in here:
- Packers: Malware can be packed. A nice hint, if available
- Import list: Used DLL functions. Some malware is loading external functionality during runtime....
- Resources: Attached malware parts (encrypted => high entropy). Maybe also abusing Icons of well known tools (=> trojans)
- Signature: Could be helpful to weed cleanstuff our. If it is properly signed by a authority we can trust....
If we want to classify malware as the first AV vendor seeing it, we can not use the detections others have on it. Obviously.
HTH
Thorsten

\section{Dynamic analysis}
\section{CapeV2 sandbox}
history (cuckoo)
description
    features...
configuration
sandbox architecture - dll monitor injection to malware process...
https://github.com/kevoreilly/CAPEv2

\subsection{Virtualization}
Downloaded images from organization sources (CTU in Prague)
Remote control, automatic setup (ansible, basic security,...), everything was intended to be scripted
VMS and their setup against detection
Mention two posibilities
    - kvm and doomdraven's script
    - vbox and vmcloak
        Experienced issued - old projects, not so effective like the kvm solution
their overall setup - to see as it is normal PC
    - Vmcloak
    ○ Virtualenvironment for lower version of python than pip installed and used thair steps
    ○ https://yrck.nl/posts/cuckoo-install.html
    - Sample files downloaded form the internet, names randomized (downloaded script, maybe I can try to write own, to have more likely names)
    - Software
    ○ Chrome
    ○ Firefox
    ○ (No Edge - not able to connect to internet error :D)
    ○ Adobe reader
    ○ Spotify
    ○ Gimp
    ○ VLC
    ○ WinRAR
    ○ Skype
    ○ Putty
    - private.ppk in Users/Administrator/.ssh generated by puttygen
    - One password in chrome database
\subsection{Network setup}
figure of the setup, description, argumentation
similar projects (isolation of malware network having internet access)
two branches - no internet, internet
internet
    describe in detail
    firewall
    security resoning
    syslog...
safe network made our work easier, mention couple of further ideas mentioned by Josef (thank to Josef at the beginning of thesis!)
\subsection{Sample distribution}
Reference scripts and crons..., sample storage
problems using provided distribution (concretize some, extract from notes, poor docs,..)
using nas storage and so on...

\section{Problems}
sandbox not running, firewall,...

\section{Results of analysis}
Complete description of the output of the sandbox is in appendix \todo{reference}.
Description what everything we have at the end for every single sample, size of dataset (GB and number of samples)...
Try to compare to another public dataset with the attributes and mention that using this way is quite universal
What we have X what the docs says (caused our misconfiguration...) - https://cuckoo.readthedocs.io/en/latest/usage/results/ (reference cuckoo book)

\section{Used technology stack for data collection}
Ubuntu 20.04
CapeV2
VirtualBox, KVM
Python 3
Ansible - mention that everything should be scripted from the beginning






here will be less references so add references to documentations and so...

add literally every detail including some scripts to appendix, even configuration of network, vpn, reference manuals, security reasoning during setting up internet connection (see notes)...
Go through the scripts here https://github.com/koubadomik/master-thesis if I did not miss something


Previous connection
- Follow up analysis chapter and summarize what we did
Next connection
- At the end should be described output of sandbox and at the beggining of next chapter should be stated what we have chosen and why



- Realization of chapter 'data'
- our infrastructure - distribution, other types of analyses, network infrastructure (marginally, I am not going to use those data, I think, but I performed it and I can write about the risks and so on...)
- used tools
- distrbution
- pipeline
- run
- results
- Conclusions, comming-outs, summary, discussion







\todo{absorb chapter about data here, describe the output and marginally discussion on what should be feature vector and classes (but present it as ready decision - from the introduction)}

% \todo{this chapter could be absorbed into next chapter if it is too short...}
% \todo{do not forget to add goals to each chapter and go through the trash below each chapter!!}
\chapter{Data structure, features and states} \label{chap:data}
The outputs of the previous chapter are dynamic malware analysed samples. Even such experiments used machine learning algorithms on all the produced data, but this is very performance demanding because the output may have 15~\emph{MB} but even gigabytes. That is why we want to choose the only subset containing the most information about the run and behaviour. 

The goal of this thesis is to use \emph{hmill} model (comprehensively described in \ref{chap:hmill}), which can work with graph-structured data and especially with JSON files. This fact we would like to use.

Our approach is to create a simple binary classifier and then scale up to multiclass or even multilabel if possible regarding performance and time. This chapter describes our reasoning on choosing features and hidden states for our \emph{hmill} classifier.  Collected results are input, and the expected output is a set of features and states (maybe more than one possibility). This decision should be based on the data we have and the experience observed in prior work.


\section{Dataset}
We have some of the results which were mentioned in \ref{chap:infrastructure}. When we start the following reasoning, we have the proper amount of the data without internet access because the setup is straightforward and more accessible. The setup with the internet connection and its collected data will be investigated in future work as its collection continues. 

The data in such domain are very biased (\ref{chap:class}). Sometimes we can see that malware can not be run under the conditions in a virtual machine or is trying to use some \emph{.NET} library and if not found, it kills itself. The fact that we do not have an internet connection can make this bias stronger, so we have to count on it to decide what to use to train the model. We do not want to investigate network activities even if sometimes it contains something (malware attempts).

The dataset consists of $80000$ different samples. Complete data have approximately $2,5$~\emph{TB} where the analysis result is compressed.

The output documentation is quite weak, but there is some (\url{https://cuckoo.readthedocs.io/en/latest/usage/results/}), and we also investigated outputs themselves. The output of general \emph{CAPEv2} sandbox analysis is described in table \ref{tab:sandbox-out}.

\begin{table}[h]
    \centering
    \caption{CAPEv2 Sandbox output (all possibilities)}
    \begin{tabular}{p{2cm}p{6cm}p{6cm}} 
        \toprule
        \textbf{Output} &
        \textbf{Meaning} &
        \textbf{In our output} \\
        \midrule
        pcap report & network traffic record (packet sequences) & Presented in output but not significant regarding the fact that during run was not internet access \\
        \midrule
        memory dump & dump of RAM (its analysis results could be also presented)& our version of sandbox do not support it \\
        \midrule
        bingraph & mechanism that discovers metamorphic malware \cite{Kwon2012}& presented\\
        \midrule
        behavioral log & raw logs of api calls and other (usually in bson) & presented \\
        \midrule
        dropped files & all dropped files are unchanged in separate directory & presented \\
        \midrule
        CAPE, procdump & other extracted payload in separate directory, extracted by various techniques \cite{Cape} & presented \\
        \midrule
        reports & sandbox allows us using several reporting and processing modules, their results are in separate directory  & several of them presented but the most comprehensive and important is \emph{report.json} \\
        \midrule
        screenshots & taken during analysis  & presented \\
        \bottomrule
    \end{tabular}
    \label{tab:sandbox-out}
\end{table}

Not all the outputs are presented in each sample. \emph{CAPEv2} presents results using web interface for example here \url{https://capesandbox.com/}~(after authentication).

There are various experiments involving machine learning approaches with analysis result input. Based on the fact that many of them use \emph{report.json} as primary input \cite{Darshan2016, Dinh2019a, Kim2020, Sethi2019}, we would like to investigate this report firstly. This report includes everything that the sandbox can provide. It also covers most of the storage on the disk, so included information is very comprehensive (see \ref{tab:report}). We are unsure which parts are suitable for our case, so the report is convenient. Compared to the mentioned authors, we have a model that accepts JSON documents.

% \todo{Summarise what we know from previous sections \todo{especially from analysis part, where we should summarise what usual program is doing in the computer and what we can observe (and what we can get from cuckoo monitor)} and go to the most concetrated information about run of program - we should end at the things which are in summary part of json log, but we can have more variants}

\section{Report}
\emph{JSON} notation is described in \ref{sec:json_notation}. The report usually has tens of megabytes but sometimes even gigabytes. It is quite easy to compress. We can have even ten times less storage because it contains many redundancies (sequences of API calls\dots). 

The documentation of each part of the report is quite poor, but many attributes are self-descriptive. Complete schema is in \ref{tab:report}. In attachments we can see example of real log (\ref{app:attach})

\begin{table}[h]
    \centering
    \caption{Parts of \emph{report.json}}
    \begin{tabular}{p{4cm}p{10cm}} 
        \toprule
        \textbf{Entry} &
        \textbf{Note} \\
        \midrule
        statistics & time statistics for particular part of malware analysis \\
        \midrule
        info & sandbox details (machine, category, used module, timeout\dots) \\
        \midrule
        debug & sandbox debug log \\
        \midrule
        target &  info about examined sample\\
        \midrule
        CAPE & extracted payload info \\
        \midrule
        behavior & processes, mutexes, commands and other attributes as enhanced log but even summary view \\
        \midrule
        deduplicated shots & screenshot summary \\
        \midrule
        network & network traffic report (domains, tcp, udp\dots) \\
        \midrule
        static analysis & results per file \\
        \midrule
        strings & extracted strings \\
        \midrule
        suricata &  output of suricata network detection tool (\url{https://suricata.readthedocs.io/en/latest/quickstart.html})\\
        \midrule
        malfamily tag &  malware family detection result\\
        \midrule
        malscore &  malware sample metric\\
        \midrule
        signatures &  list of signatures which were detected by sandbox \\
        \bottomrule
    \end{tabular}
    \label{tab:report}
\end{table}

\section{Features and states for classification}
Our goal is to train classifier so we need to find appropriate $\mathcal{X}, \mathcal{Y}$ (\emph{features} and \emph{states} as defined in \ref{chap:classification}). Following our statements, we find them in \emph{report.json}. More than one possibilities are stated, we will train models on some of the input data combination/s  based on further conditions.

\subsection{States}
We know that our samples are malware because of their source, as we mentioned in previous chapter \ref{chap:infrastructure}. So it does not make sense to try \emph{malware X cleanware} classification. The classification classes have to be related to the malware itself, its attributes and behavioural signs.

From the \emph{report.json} we extracted three candidates \emph{malware family}, \emph{signatures}, \emph{malscore}. All those attributes we can see as dependent variables where independent are some behavioural features or a combination of behavioural and static features. 

Assigning \emph{malware family} is a very complex task \cite{Gennari2011}, we often involve the behaviour of the sample and its similarity to other samples. The usual process involves decomposing this task to deterministic detection of specific behaviour and classifying according to their combination. The potential model could attempt to generalise the family assignment process (like in \cite{Rieck2008}) and point out features that could be used in a more complex way than usual deterministic ones. Straightforward thinking about this task is that it is more a clustering task - we need to find families and use them to categorise old, and new samples \cite{Pitolli2017}. Specific malware families are mentioned in chapter \ref{chap:analysis}

\emph{Malware score} has multiple definitions in many papers (\cite{Walker2019, Kumar2014}), and that is a crucial problem of these metrics. Of course, in the CAPEv2 sandbox, we can investigate its implementation, but the future classifier will be dependent on it. On the other hand, potential classifiers could assign scores based on more complex decisions. We could observe where it is against the original human-defined assignment. Nevertheless, this is quite risky because even the original \emph{malscore} assignment is a complex decision consisting of a series of steps. Examining it like a black box should not be the first thing which we want to use. We instead want to use its causes.

\emph{Signatures} most often tells us what sandbox observed, e.g. \ special kind of behaviour, static features\dots We can see them as local detectors which are checking particular feature in the malware analysis result. The potential model could generalise their assignment or even investigate other malicious signs that are often connected with some signatures - find new signatures or existing behaviour correlating with the original signature assignment.

\subsection{Features}
Following the assumption that our state candidates are related to the behaviour of malware sample, we choose some subset of the original \emph{report.json} which is related to that. We omit static features, state candidates and everything not related to behaviour. Two things remain, network and behaviour part. Our dataset does not have relevant network traffic due to the data collection details mentioned above. We take into consideration only the behaviour part of the original report. Its structure can be seen in \ref{tab:behavioral} and its example in attachment (\ref{app:attach}).

\begin{table}[h]
    \centering
    \caption{Parts of \emph{report.json} behavior}
    \begin{tabular}{p{2cm}p{12cm}} 
        \toprule
        \textbf{Entry} &
        \textbf{Meaning} \\
        \midrule
        processes & list of processes related to malware execution with details (api names, arguments\dots) \\
        \midrule
        process tree & structure of process execution\\
        \midrule
        summary & list of occured files, registry keys, mutexes, executed commands, api calls \\
        \midrule
        enhanced & comprehensive log of events during malware execution including parameters and timestamps and other\\
        \bottomrule
    \end{tabular}
    \label{tab:behavioral}
\end{table}

We can use it in one piece or just its parts. We have to be aware of a potential bias like timestamps which are in enhanced and processes parts. We would have to involve some preprocessing in case of using some of them as features.

Technologies used for data pruning and other parts of preprocessing are listed in \ref{app:technologies}. Code is in attachment (\ref{app:attach}).

The goal of this chapter was to describe potential states and features. We know that to train the model, we concentrate on specific parts of \emph{report.json}. We are moving to the model itself.



% Lastly thrown out
% We would like to base our decision on the fact that in \ref{chap:analysis} we stated that crucial observations during the program run are \emph{processes, files, mutexes, registries, commands, api calls}.

% NICE to have
% We can try to create some histogram using the data from virus total (families or something like that...)
% \todo{checkout some web interface example, if there is something else, and even configuration files}
% \todo{add sections of report from virus total and its features}
% https://developers.virustotal.com/v3.0/reference#files
% \todo{add some image}

%%----------------------------------------------------------------------------------------------

% This statement is truth even physically such big data amounts we have to solve on bigger cluster like cesnet.metacenter. \todo{mention data amount before and after processing pipeline}
% After we decided which data parts are useful we used linux tools and Julia language to \todo{reference Julia in appendix technology description and maybe even lazyjson library} extract what we need from original analysis results \todo{reference what we used in bash - if necessary (maybe I can add to attachment bash script with trasformation like that if needed), and reference pruner code in attachements, we should have appendix description of attachments}


% "behavior
%     "processes, "processtree  - list of processes related to malware run with details (api name, arguments...)
%     "summary
%         "files
%         "read_files
%         "write_files
%         "delete_files
%         "keys
%         "read_keys
%         "resolved_apis    
%     "enhanced - detail sequence of events (library loading, api calls with parameters...)


% \todo{more about them we say in chapter on modelling, or maybe here give example of code and signature detection and data part (and all parts) and to appendix we can add more, with reference to the repository}

% Describe parts of json report
% JSON report parts \todo{describe that json report is quite comprehensive and contain even earlier mentioned outputs} \todo{to appendix we should add some example of report.json}
% - size vary a lot, could be from 10MB to GBs
% lack of docs - https://github.com/cuckoosandbox/cuckoo/issues/2458

% "statistics - time statistics for particular part of malware analysis
% "info - sandbox details (machine, category, used modul...)
%     "version"
%     "started"
%     "ended"
%     "duration"
%     "id"
%     "category"
%     "custom"
%     "machine
%     "package"
%     "timeout"
% "CAPE - extraxted payload
% "behavior
%     "processes
%     "processtree
%     "summary
%     "enhanced
% "debug - debug log of sandbox
% "deduplicated_shots - screenshots
% "network
%     "domains
%     "tcp
%     "udp
%     "dns
%     "pcap_sha256"
%     "sorted_pcap_sha256"
% "static - static analysis of files
% "strings - extracted strings (static analysis technique)
% "suricata - network detection Suricata tool log
% "target - sample details
% "malfamily_tag"
% "signatures - list of signatures and their data
% "malscore"
% "ttps

% paragraph about each and resoning on if we can use it or not, and what kind of analysis can be based on particular part (I think we enumerated almost all parts of original log)


% Summary of the data set - How many samples, size...
% Also we can try to specify the requirements for our model (quite big amount of data - all the reports), we are not able to process whole outputs, reference hmill capacities on previous experiments (for example in Julia language)


% \todo{Final result of data collection - no internet samples (maybe mention at the end of previous chapter), I did collect some, but did not use them in further parts because priority was not to have everything}

% Their analysis is in future work same as data from static analysis (Virus total). These data are still part of our data set and was big part of our job so far. So the subject of our research is sandox output.

% Previous connection
% - Follow up statements about data in cybersecurity, follow previous chapter with choosing relevant parts of logs
% Way through this chapter, we do not use internet access data, because we did not have it at right time, no internet data are sufficient for our further experimentation
% Next connection
% - Chosen data will be used for further experiments


% reference some source that is talking about data in cyber security, resp. on theory part


% Data processing
% extracting desired parts
% Is there something else in processing?
% Pruning jsons in Julia...

% reference that we did use similar parts as predecessors (Pevny, Mandlik, Stiborek)

% Detail descriptions of candidates, for example describe signatures (just describe what they do and sort them to categories according to what they are doing) and give example and at once slightly say why the signatures could be interesting (based on behavior like this we can validate model, such that it is using the proper part of report, but we can see whatever else which looks like malicious)


% Next chapter - Due to the data quality we are not able to use some of them

% Get several reasonings for some parts of analysis, what we want and we do not, also we can compare it to Stiborek and other works (for example Mandlik had quite straight-forward dataset and we know that it worked), we can really investigate the whole output and present ideas. 


% At the end we can have more variants, but at the end we did examine two and that should not be a problem