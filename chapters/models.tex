\chapter{Modelling classifier} \label{chap:models}
In this chapter, we present trained \emph{hmill} models and their results. Our goal is to demonstrate that it is possible to train such a model and later explain it. We do not aim to strictly perform a long sequence of experiments and compare to prior. Model is just part of our work.

We do not model all variants of features and states mentioned in the previous chapter. We concentrate on at least one working model, and then we move on to model explaining experiments. From \emph{states} and \emph{features} in question we start with our top candidate - \emph{signatures} as states and \emph{behavior log} as features. 

\section{Behaviour as feature vector}
After further look at the \emph{behavior} part of \emph{report.json} we start with only \emph{summary} and \emph{enhanced} part. The reason is that other parts are quite comprehensive, and we would not be able to train the model with the hardware resources we have. Those two should be sufficient for a comprehensive look at malware behaviour. If the model were still too complex to train under our conditions, we would also omit the \emph{enhanced part}. We would mainly lose the information about the order of events because the summary part lists only artefacts (API calls, commands\dots).

\section{Signatures as states}
Our reasoning about states is based on the fact that the signatures are usually assigned based on some atomic fact, for example, the presence of some API call in the log. Let us call this fact \emph{subject of signature}. It can be one or more patterns seen in the behavioural log. The implementation of signature is deterministically detecting this part and add the signature to the log. We can consider \emph{copies self} signature example. If a specific dropped file that corresponds to the currently analysed one is dropped, the signature is active, and we would find it in \emph{report.json}.

If we choose features where the subjects are presented, we should be able to evaluate the model's explanation. By evaluation, we mean that we can examine the coincidence between the original subject and model explanation. Also, we want to see if the model can generalise the knowledge and maybe come up with some new explanation of particular signature - new better subject. It can also find parts that go with the particular signature's original subject - maybe another malicious behaviour.

More complex labels like malware family or malware score we might explore later. We assume that signatures themselves often support the assignment of those, and it is better to explore the original cause of the whole chain.

After examining the histogram of seen signature in our dataset, we choose a subset based on the frequency. We prefered signatures that are implemented in Python for convenient investigation of the original subject. In the \ref{app:signatures} we can see our candidates and their frequencies in the dataset.

The next task is to investigate the implementation of a particular signature and try to create groups for further conclusion-making. \emph{Python} implementation of each signature processes the output of sandbox and generates boolean values signing that the subject of a particular signature is presented in sample behavioural log. The signature subject can be API calls, file, registry key, process, command, and even more complex. If the signature implementation returns \emph{True}, we observe instance of this signature in the \emph{signatures} part of \emph{report.json}. An example of such a signature item could be seen in \ref{app:signatures}. The most important aside from \emph{name} and \emph{description} is field \emph{data} which sometimes contains signature subject.

We explored two types of grouping aspects. The first is the subject of a particular signature. The second is if we can see this subject in the behavioural log directly. For instance, if the signature implementation uses API calls, the API call list is directly in the log. If it uses the entropy of dropped files, it is not directly in the output. In \ref{app:signatures} we can see a simple example of implementation of \emph{antidebugsetunhandledexceptionfilter} signature, which is just checking the presence of a specific signature.
More about particular signatures and identified categories could be found in \ref{app:signatures}. 

We distinguish two groups based on the direct presence of the original subject in the feature vector. The reason why we categorise signatures in such a manner is that we would like to connect those categories with the model performance in further discussion. We expect that different categories might have different performance.

\section{Model}
The objective is to create a binary classifier that predicts the presence of a particular signature. Later, we can train even a multilabel classifier, assigning a set of signatures according to dynamic analysis. However, the binary classification, its performance and its explanations are the most important for us. 

% Multilabel classifier could be seen as multioutput binary model, so if we are able to scale up the hardware resources and its usage using available techniques (multithreading) multilabel model should be easily deriveable from original simle binary classfier.

Technology stack we used to train \emph{hmill} model is described in \ref{app:technologies}. Important is that we unsuccessfully experimented with multithreading in \emph{Julia} language for gradient computation. The resulting model was trained on one CPU, which a little bit shrank our possibilities.

The first experiments are without reported results because the model was too huge to converge to some significant performance. We were not able to train it on hardware and with thread limitations in a feasible time. This reality led us to skip one part of the feature vector - \emph{enhanced} part. So the resulting model is working with \emph{summary} part only. The tighter feature vector is an advantage for further training time. We still know that summary parts include compressed information aiming at (\emph{api calls, files, mutexes, registries, commands}).

The original code of the model is in thesis attachment \ref{app:attach}. It is a standard binary classifier. What is not so ordinary is that the model is a function of the input data (typical for \emph{hmill}). Another important fact is that the \emph{hmill} framework \emph{API} is general, so it is not accepting JSON documents directly. For this purpose, we use \emph{JsonGrinder} library, which can accept an array of JSON documents and produce schema. The schema is then used to create \emph{hmill} structures from each JSON document. Example of a schema, implied extractor and \emph{hmill} model for our data is in attachment (\ref{app:attach}). In our case, each signature has its own training set and even its schema implied by training set examples.

% \begin{algorithm}
%   \caption{Hmill model training}\label{algo:hmillbinary}
%   \begin{algorithmic}
%       \Procedure{HmillTrain}{$signature\_name, jsons, hyperparameters$}
%       \State $samples \gets load(jsons)$
%       \State $train,test \gets split(samples)$
%       \State $trnstates,tststates \gets extractstates(train, test)$
%       \State $schema \gets getshema(train)$
%       \State $extractor \gets suggestextractor(schema)$
%       \State $model \gets reflectinmodel(schema, extractor, class\_num, hyperparams)$
%       \State $optimize(model, train, trnstates)$
%       \State $metrics \gets evaluate(model, tststates)$
%       \EndProcedure
%   \end{algorithmic}
% \end{algorithm}

\subsection{Model hyperparameters and dataset attributes}
In our experiment, we do not involve techniques like early stopping or hyperparameter tuning. Our goal is to demonstrate the classifier performance. We do not aspire to concentrate on this part only. We build on previous experiments on \cite{Mandlik2020} where the author published even the hyperparameter set in \emph{device id} example. This example was also from the cybersecurity field—those parameters we used as the initial setup.

We were not successful immediately, and it took several attempts of our hand-picked adjustments. After each adjustment, we tried to train the model and observed convergence of loss function and balanced accuracy on training and testing data. Since we did not tune hyperparameters or anything else, we did not use a validation set. 
% Due to the excellent ration between testing and training data, signature distribution and convincing performance we did not involve cross validation.

In the table \ref{tab:hyperparams} we can observe hyperparameters of the model and other training-independent facts which we used.


\begin{table}[h]
  \centering
  \caption{Hyperparameters of \emph{hmill} model}
  \begin{tabular}{p{6cm}p{8cm}} 
      \toprule
      \textbf{Parameter} &
      \textbf{Value} \\
      \midrule
      samples & $80000$ (1:1 testing and training) \\
      \midrule
      minibatch size & $1000$ (random subsampled from training set in each iteration)\\
      \midrule
      hidden units (neurons)& $20$\\
      \midrule
      iterations & $120$\\
      \midrule
      optimizer & \emph{ADAM} \cite{Kingma2014}\\
      \midrule
      loss function & \emph{logit cross entrophy} (\ref{chap:classification})\\
      \bottomrule
  \end{tabular}
  \label{tab:hyperparams}
\end{table}


Difference from \cite{Mandlik2020} is \emph{minibatch size} and \emph{number of iterations} which worked for us better in this changed form. Data used in their case were of a much smaller scale (\emph{device id}) which could be why.

We set up the number of \emph{iterations}  so that we observe reasonable convergence. We checked overfitting by monitoring the difference between accuracy on training and testing data. They do not differ significantly.
% We did not challenge this parameter, and we expect further convergence.

\section{Results}
% Chosen metrics are based on metrics which are reported by similar papers \todo{reference Pevny, Madlik and Stiborek}. 
We choose the metrics following the chapter \ref{chap:classification} where we mentioned classifiers in the cybersecurity field and their pitfalls. The calculation was supported by \emph{EvalMetrics.jl} library (\ref{app:technologies}). Results of experiments could be seen in \ref{tab:models_res}, the table is divided into two groups based on categories presented above. There are also some other metrics and visualisations in \ref{app:models}. Resulting models are part of attachment \ref{app:attach}.

Metrics which we are reporting in \ref{tab:models_res} are robust in the case of an imbalanced dataset which is also our case. In the table, we also include a percentage of positive examples to see this disbalance and assess the \emph{FNR} and \emph{FPR}.

\begin{table}[h]
  \centering
  \caption{Models performance (values rounded for 3 decimal spaces, P denotes positive examples ratio in our dataset)}
  % \begin{tabular}{p{5cm}p{3cm}p{3cm}p{3cm}} 
  \begin{tabular}{lllll}
      \toprule
      \textbf{Signature} &
      \textbf{Bal. acc.} &
      \textbf{FNR} &
      \textbf{FPR} &
      \textbf{P [\%]}
      \\
      \midrule
      antidebug setunhandledexceptionfilter & $0.9801$ & $0.0289$ & $0.0109$ & $45$ \\
      \midrule
      copiesself & $0.924$ & $0.125$ & $0.0279$ & $18$ \\
      \midrule
      deletesself & $0.997$ & $0.005$ & $0.002$ & $27$ \\
      \midrule
      enumeratesrunningprocesses & $0.972$ & $0.050$ & $0.007$ & $16$ \\
      \midrule
      stealthtimeout & $0.701$ & $0.064$ & $0.331$ & $21$ \\
      \midrule
      useswindowsutilities & $0.958$ & $0.078$ & $0.006$ & $18$ \\
      \midrule
      removeszoneidads & $0.999$ & $0.000$ & $0.000$ & $28$ \\
      \midrule[0.3pt]
      \midrule[0.3pt]
      antisandboxsleep & $0.969$ & $0.037$ & $0.026$ & $39$ \\
      \midrule
      dropper & $0.911$ & $0.147$ & $0.032$ & $15$ \\
      \midrule
      invalidauthenticodesignature & $0.607$ & $0.668$ & $0.113$ & $36$ \\
      \midrule
      packerentropy & $0.605$ & $0.748$ & $0.043$ & $22$ \\
      \midrule
      stealthnetwork & $0.942$ & $0.008$ & $0.109$ & $66$ \\
      \bottomrule
  \end{tabular}
  \label{tab:models_res}
\end{table}

\section{Discussion}
Original signature implementation is deterministically examining the sandbox output and marking some behaviour as we stated above. The most significant difference between the signatures is if their subject (\emph{api call, file \dots}) is directly among our features or it is not. 

Firstly, let us describe our expectations. Based on this categorisation, we assume that the performance of signatures in the first category is better than the one in the second. We could not be sure that the model would have something according to what it will generalise, but we know that it should consider the subject of the original signature in the first case.

In the case of the first category of signatures, we observe quite consistent performance above $95$~\%. Signature \emph{copiesself} has $92$~\%, which is still sufficient for explaining. This deviation could be caused by the fact that the original signature is examining dropped files and checking if the current file is among them. Nevertheless, the filename varies, so the entropy might be very high and might cause big \emph{FNR}. Signature \emph{deletesself} might have similar issues, but it is determining according to API calls and not dropped files which could cause that its performance is better. The only significant outlier in this group is \emph{stealthtimeout} signature which examines a sequence of API calls that could be quite complicated. After going through some files where this signature was presented, we could not identify the particular API calls in the original log, so that the assignment might be more tricky. The main cause of low accuracy is, in this case, \emph{FPR}.

Despite some anomalies that we inspect in the next chapter, this signature group's performance is as we expected. 

% Even the \emph{stealthtimeout} we will explain on positive examples with high confidence to see what is in the game there.

The second group behaves more unexpectedly because the performance in 3 of 5 cases is above $90$~\%. Signature \emph{antisandboxsleep} uses API calls in a more complex way, so the original classifier may involve these. \emph{Dropper} has significantly larger \emph{FNR}. However, its performance is still good for explanations. \emph{Stealthnetwork} should look at network activity which is not among the features. The excellent performance of some signatures in this group arouses our interest in the explaining part. In cases of \emph{invalidauthenticodesignature} and \emph{packerentropy} explanation, we should not take it too seriously due to the performance. Overall the first group has statistically better performance as we expected, but this comparison is not so relevant.

We also performed several experiments with a more general multilabel classifier, but we did not observe convergence under our hardware resource conditions.

This chapter concludes that we can train \emph{hmill} classifier to classify the presence of a particular signature based on the summary part of the behavioural log from \emph{CAPEv2} sandbox. This classifier has a significant performance of more than $90\%$ as balanced accuracy for most signatures, which is sufficient for further explaining experiments. Such models should have strong confidence for us to choose some samples for explainer (\ref{chap:explain}). 

% We are able to classify only some of signatures with such performace which we mentioned in discussion.

In the explanation chapter, we should examine different categories of signatures based on their performance. The models with our intuition should investigate such that they are using what the original signature did. The unexpectedly bad ones we should not try to explain because of their low confidence. The unexpectedly good ones we need to explain to see the subject used by the model.


% We could also investigate if the instance of particular subject for instance specific api call not only their list is presented because the signature detect specific list of api calls. But we categorize signature based on the fact if the whole category of subject is presented in output.

% Because the algorithm has to converge to something else which is corresponding to positive training examples and this do not have to be straight-forward. But on the other hand we assume that classifier could generalize in such way because that could lead us to some new knowledge in explanation part.

% \begin{enumerate}
%   \item Directly in \emph{report.json} in parts \emph{summary, enhanced}
%   \begin{itemize}
%     \item API calls - \emph{injectionrwx, antidebugsetunhandledexceptionfilter, removeszoneidads, deletesself, stealthtimeout, enumeratesrunningprocesses, injectionrunpe}
%     \item Files - \emph{copiesself}
%     \item Commands - \emph{useswindowsutilities}
%   \end{itemize}
%   \item Not directly in \emph{report.json} in parts \emph{summary, enhanced} (some clues might be presented, but the direct information is not there)
%   \begin{itemize}
%     \item Processes - \emph{dropper}
%     \item Sleep - \emph{ antisandboxsleep}
%     \item Entrophy - \emph{packerentropy}
%     \item Network - \emph{stealthnetwork}
%     \item Authsign - \emph{invalidauthenticodesignature}
%   \end{itemize}
% \end{enumerate}
%NICE TO HAVE
%we can try to play more with some average bal. acc. or FNR and FPR overall dataset and over each group...

% in conclusion clarify that we did not used only summary part of json
% \todo{clarify this in section introductions and so, not to confuse somebody with those ongoing changes}.
% \todo{we may: Connect to the chapter about classification generally - hmill model, what specifically we use}
% \todo{If not enough data and discussion, we could try to discuss the data part of signature and original sample and how it corresponds to the performance of our models}
% \todo{we can compare to others but not necessary}
% \todo{report table of metrics including the balance of dataset (pie graph?) and at least one graph}
% Convergence went on we just did not have enough time and performance to run it again and again (maybe I can plot loss function plot to prove that )
%%-----------------------------------------------

% % Compare signatures
% Each group or each signature in particular. Our expectation compared to real results.

% - technically signatures are deterministic (heuristic) view on the same thing, but in case of machine learning we are traing to target the result statistically (point of discussion)

% Describe general intention and direction of reasoning - what we want to find in models, success criteria
% We expect that net will be working on samples where the subject of signature is directly in the report, hopefully. Than that in such situation we can find something else. Finally, in case that the signature subject is not easily observable from report, we can find something else or just say ok, we are not able.


% But our goal was to train the classifier and then explain it so we are able to examine trained classifiers and that was succesful.

% Mention multilabel just marginally.
% mention even multilabel as addition (in principle it should be possible but for us it was not priority)
% - mention experiments (several attempts we have) and accuracy per item but only as something as addition - maybe more iteration, because multilabel is just multioutput single label, predicting at once should be possible, for it was not priority
% check overfitting - Pevny said that balanced accuracy on training set
% Minibatches are generated randomly from training set in each iteration \todo{reference it in the classification theory part if it is there}.

% In figure \todo{add histogram visualization with percentage, not exact numbers} There is a histogram where we can observe frequency of each signature in our dataset.
% - Used technologies, several experiments, pseudocode (algorithm), hyper parameters, report evaluation metrics,types of classifiers used (motivation for that) COMPARISON WITH PRIOR ALTERNATIVES (I think it is not so necessary our goal is more explaining of the model?, Mention metacenter (even in thanks part)
%   - Modelling - single label classifier
%     - GOAL: Using HMill, create models. Report results.
%   - Modelling - multi label classifier
%     - GOAL: Using HMill, create models. Report results.
%   - Performance, early stopping, hyper parameters...
%   - Detectection of some signatures, some not, why - no data; why...
%     - technically signatures are deterministic (heuristic) view on the same thing, but in case of machine learning we are traing to target the result statistically
%     - we did not care so much about tuning hyper parameters and comparing
%        - We used parameters similar to similar experiments with same framework, 1000/150 look to converge (hopefully)
%        - we experimented with different kind of signatures (statistically even functionally) - describe the kinds (report all results, compare using f-score or FNR, FPR for example)
%        - than we experimented with multilabel case - trying to predict more than one signatures at one time
%   - (Technical background, used metascenter archicture...)
%   - reference Mandlik's and Pevny's works, hopefully similar
%     - Mandlik is reporting
%       - Single label - from each signature kind atleast one (but possibly more) - For different kinds of signatures - create it as a part of training itself
%         - Params: empirically and based on previous experiments, no hyper parameters tuning (some of signatures have significant performance with chosen so we did not spend time with that, not goal of this thesis)
%             - samples 80_000, test: 0.5, train: 0.5
%             - minibatch - 1000
%             - neurons - 20
%             - iterations (random subssampling) - 120 (should be sufficient), mention it by each signature particularly because some of jobs did not catch it on time so the number of iterations is smaller
%             - optimizer ADAM
%         - before training:
%             - balance of training set - how many positive and negative - DONE
%                 - I can use pie graph
%         - At the end of training
%             - metrics: accuracy, balanced accuracy - DONE
%             - curves: prcurve, roccurve (normal and with log on x-axis) - DONE (scores and targets)
%                 - roc curve with log scale on x axis (FPR is more inportant for us)
%             - comparative: 
%                 - f-score, auroc, auprc - DONE
%                     - I can argument as comparison with Mandlik, if it is going to work
%                 - tpr, fpr, acc - DONE
%                     - Jan Stiborek comparison
%                     - I can use use column graphs
%         - multilabel - nice-to-have


% - I tried to choose enhanced and summary part but it was too much and enhanced part contains bias like timestamp and so on, so I decided to choose only summary part and later only segments from it, using whole summary lead us to really slow convergence...I think (try to find between models...)

% reference json grinder and its usage to process data

% - with enhanced part and without it
%   - Report some results per signature (some reference in appendices) report even balancing of data set for particular signature
%   - comment each
%   - compare to similar experiments (there are experiments with also data from sandbox)



% To appendix add everything, for example even extracted schema...

% Describe data loading and modelling, split conditions, random minibatch... - reference code in attachment, we did follow parameters in similar models (Mandlik, identification.jl) and it was starting point, describe parameters and conditions, we can also dump tree of extractor and corresponding model


% If we started with signatures we can report some statistics (basic principle should be presented in previous chapter)

% Basic statistics for various parts and variants
%   - signature histogram (balanced) - histogram.csv
%   - ...
%   - a lot of Emotet (maybe should be on another place) (malware family problem I think, mention among potential biases), where is this attribute in the original report?
%   - - Balanced dataset - in term of accuracy metric performance
%   - Bias - - Bias in practical data like this - security data, what are the influences (as an example could be ip addresses...)

% Using:



% \todo{In this section we will formulate why we started with signatures (and even ended), also why we started with summary and enhanced and than skipped only to summary part}

% Previous connection
% - Based on previously chosen data and described theory of hmill we have chosen technology and tried to train the model in different setups with different data
% Way through this chapter

% Next connection
% - Resulting models will be objects of explanation experiments



% DIRECTLY in output
% 	- API calls - are in in summary part
% 		○ "injection_rwx" 28251
% 		○ "antidebug_setunhandledexceptionfilter" 18223
% 		○ "removes_zoneid_ads" 11070
% 		○ "deletes_self" 10805
% 		○ "stealth_timeout" 8253
% 		○ "enumerates_running_processes" 6324
% 		○ "injection_runpe"  5542
%   - Files - are in summary part
% 		○ "copies_self" 7137
% 	- Commands - are in summary part
% 		○ "uses_windows_utilities" 6987

% NOT DIRECTLY in output
% 	- Processes - not directly in summary part
% 		○ "dropper"  6045
% 	- Time - not directly in summary part
% 		○ "antisandbox_sleep" 15810
% 	- AuthSign - not directly in summary part
% 		○ "invalid_authenticode_signature" 14346
% 	- Entrophy - not directly in summary part
% 		○ "packer_entropy" 8815
% 	- Combinations - 
%     "stealth_network" 26430
