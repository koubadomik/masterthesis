\chapter{Modelling classifier}
Following previous chapter we want to present our way of modelling and results we have. The main goal of this part is to train the models based on prior experiments, not strictly perform long sequence of experiments and compare to prior. On the other hand we would like to present similar metrics comparing to our collages. We did not model all variants mentioned in previous chapter, we concentrated on at least one working model. 

- Follow the chapter about classification generally - hmill model, what specifically we use

- used technologies
  - \todo{reference JsonGrinder, Mill, EvalMetrics...}
  - reference, describe functioning

Describe data loading and modelling, split conditions, random minibatch... - reference code in attachment, we did follow parameters in similar models (Mandlik, identification.jl) and it was starting point, describe parameters and conditions, we can also dump tree of extractor and corresponding model
  - mention even multilabel as addition (in principle it should be possible but for us it was not priority)

Describe general intention and direction of reasoning - what we want to find in models, success criteria
We expect that net will be working on samples where the subject of signature is directly in the report, hopefully. Than that in such situation we can find something else. Finally, in case that the signature subject is not easily observable from report, we can find something else or just say ok, we are not able.

- with enhanced part and without it
  - Report some results per signature (some reference in appendices) report even balancing of data set for particular signature
  - comment each
  - compare to similar experiments (there are experiments with also data from sandbox)



Previous connection
- Based on previously chosen data and described theory of hmill we have chosen technology and tried to train the model in different setups with different data
Way through this chapter

Next connection
- Resulting models will be objects of explanation experiments


- I tried to choose enhanced and summary part but it was too much and enhanced part contains bias like timestamp and so on, so I decided to choose only summary part and later only segments from it, using whole summary lead us to really slow convergence...I think (try to find between models...)

reference json grinder and its usage to process data

- Used technologies, several experiments, pseudocode (algorithm), hyper parameters, report evaluation metrics,types of classifiers used (motivation for that) COMPARISON WITH PRIOR ALTERNATIVES (I think it is not so necessary our goal is more explaining of the model?, Mention metacenter (even in thanks part)
  - Modelling - single label classifier
    - GOAL: Using HMill, create models. Report results.
  - Modelling - multi label classifier
    - GOAL: Using HMill, create models. Report results.
  - Performance, early stopping, hyper parameters...
  - Detectection of some signatures, some not, why - no data; why...
    - technically signatures are deterministic (heuristic) view on the same thing, but in case of machine learning we are traing to target the result statistically
    - we did not care so much about tuning hyper parameters and comparing
       - We used parameters similar to similar experiments with same framework, 1000/150 look to converge (hopefully)
       - we experimented with different kind of signatures (statistically even functionally) - describe the kinds (report all results, compare using f-score or FNR, FPR for example)
       - than we experimented with multilabel case - trying to predict more than one signatures at one time
  - (Technical background, used metascenter archicture...)
  - reference Mandlik's and Pevny's works, hopefully similar
    - Mandlik is reporting
      - Single label - from each signature kind atleast one (but possibly more) - For different kinds of signatures - create it as a part of training itself
        - Params: empirically and based on previous experiments, no hyper parameters tuning (some of signatures have significant performance with chosen so we did not spend time with that, not goal of this thesis)
            - samples 80_000, test: 0.5, train: 0.5
            - minibatch - 1000
            - neurons - 20
            - iterations (random subssampling) - 120 (should be sufficient), mention it by each signature particularly because some of jobs did not catch it on time so the number of iterations is smaller
            - optimizer ADAM
        - before training:
            - balance of training set - how many positive and negative - DONE
                - I can use pie graph
        - At the end of training
            - metrics: accuracy, balanced accuracy - DONE
            - curves: prcurve, roccurve (normal and with log on x-axis) - DONE (scores and targets)
                - roc curve with log scale on x axis (FPR is more inportant for us)
            - comparative: 
                - f-score, auroc, auprc - DONE
                    - I can argument as comparison with Mandlik, if it is going to work
                - tpr, fpr, acc - DONE
                    - Jan Stiborek comparison
                    - I can use use column graphs
        - multilabel - nice-to-have
            - mention experiments (several attempts we have) and accuracy per item but only as something as addition - maybe more iteration, because multilabel is just multioutput single label, predicting at once should be possible, for it was not priority



Convergence went on we just did not have enough time and performance to run it again and again (maybe I can plot loss function plot to prove that )
        
      