

% We could also investigate if the instance of particular subject for instance specific api call not only their list is presented because the signature detect specific list of api calls. But we categorize signature based on the fact if the whole category of subject is presented in output.

% Because the algorithm has to converge to something else which is corresponding to positive training examples and this do not have to be straight-forward. But on the other hand we assume that classifier could generalize in such way because that could lead us to some new knowledge in explanation part.

% \begin{enumerate}
%   \item Directly in \emph{report.json} in parts \emph{summary, enhanced}
%   \begin{itemize}
%     \item API calls - \emph{injectionrwx, antidebugsetunhandledexceptionfilter, removeszoneidads, deletesself, stealthtimeout, enumeratesrunningprocesses, injectionrunpe}
%     \item Files - \emph{copiesself}
%     \item Commands - \emph{useswindowsutilities}
%   \end{itemize}
%   \item Not directly in \emph{report.json} in parts \emph{summary, enhanced} (some clues might be presented, but the direct information is not there)
%   \begin{itemize}
%     \item Processes - \emph{dropper}
%     \item Sleep - \emph{ antisandboxsleep}
%     \item Entrophy - \emph{packerentropy}
%     \item Network - \emph{stealthnetwork}
%     \item Authsign - \emph{invalidauthenticodesignature}
%   \end{itemize}
% \end{enumerate}
%NICE TO HAVE
%we can try to play more with some average bal. acc. or FNR and FPR overall dataset and over each group...

% in conclusion clarify that we did not used only summary part of json
% \todo{clarify this in section introductions and so, not to confuse somebody with those ongoing changes}.
% \todo{we may: Connect to the chapter about classification generally - hmill model, what specifically we use}
% \todo{If not enough data and discussion, we could try to discuss the data part of signature and original sample and how it corresponds to the performance of our models}
% \todo{we can compare to others but not necessary}
% \todo{report table of metrics including the balance of dataset (pie graph?) and at least one graph}
% Convergence went on we just did not have enough time and performance to run it again and again (maybe I can plot loss function plot to prove that )
%%-----------------------------------------------

% % Compare signatures
% Each group or each signature in particular. Our expectation compared to real results.

% - technically signatures are deterministic (heuristic) view on the same thing, but in case of machine learning we are traing to target the result statistically (point of discussion)

% Describe general intention and direction of reasoning - what we want to find in models, success criteria
% We expect that net will be working on samples where the subject of signature is directly in the report, hopefully. Than that in such situation we can find something else. Finally, in case that the signature subject is not easily observable from report, we can find something else or just say ok, we are not able.


% But our goal was to train the classifier and then explain it so we are able to examine trained classifiers and that was succesful.

% Mention multilabel just marginally.
% mention even multilabel as addition (in principle it should be possible but for us it was not priority)
% - mention experiments (several attempts we have) and accuracy per item but only as something as addition - maybe more iteration, because multilabel is just multioutput single label, predicting at once should be possible, for it was not priority
% check overfitting - Pevny said that balanced accuracy on training set
% Minibatches are generated randomly from training set in each iteration \todo{reference it in the classification theory part if it is there}.

% In figure \todo{add histogram visualization with percentage, not exact numbers} There is a histogram where we can observe frequency of each signature in our dataset.
% - Used technologies, several experiments, pseudocode (algorithm), hyper parameters, report evaluation metrics,types of classifiers used (motivation for that) COMPARISON WITH PRIOR ALTERNATIVES (I think it is not so necessary our goal is more explaining of the model?, Mention metacenter (even in thanks part)
%   - Modelling - single label classifier
%     - GOAL: Using HMill, create models. Report results.
%   - Modelling - multi label classifier
%     - GOAL: Using HMill, create models. Report results.
%   - Performance, early stopping, hyper parameters...
%   - Detectection of some signatures, some not, why - no data; why...
%     - technically signatures are deterministic (heuristic) view on the same thing, but in case of machine learning we are traing to target the result statistically
%     - we did not care so much about tuning hyper parameters and comparing
%        - We used parameters similar to similar experiments with same framework, 1000/150 look to converge (hopefully)
%        - we experimented with different kind of signatures (statistically even functionally) - describe the kinds (report all results, compare using f-score or FNR, FPR for example)
%        - than we experimented with multilabel case - trying to predict more than one signatures at one time
%   - (Technical background, used metascenter archicture...)
%   - reference Mandlik's and Pevny's works, hopefully similar
%     - Mandlik is reporting
%       - Single label - from each signature kind atleast one (but possibly more) - For different kinds of signatures - create it as a part of training itself
%         - Params: empirically and based on previous experiments, no hyper parameters tuning (some of signatures have significant performance with chosen so we did not spend time with that, not goal of this thesis)
%             - samples 80_000, test: 0.5, train: 0.5
%             - minibatch - 1000
%             - neurons - 20
%             - iterations (random subssampling) - 120 (should be sufficient), mention it by each signature particularly because some of jobs did not catch it on time so the number of iterations is smaller
%             - optimizer ADAM
%         - before training:
%             - balance of training set - how many positive and negative - DONE
%                 - I can use pie graph
%         - At the end of training
%             - metrics: accuracy, balanced accuracy - DONE
%             - curves: prcurve, roccurve (normal and with log on x-axis) - DONE (scores and targets)
%                 - roc curve with log scale on x axis (FPR is more inportant for us)
%             - comparative: 
%                 - f-score, auroc, auprc - DONE
%                     - I can argument as comparison with Mandlik, if it is going to work
%                 - tpr, fpr, acc - DONE
%                     - Jan Stiborek comparison
%                     - I can use use column graphs
%         - multilabel - nice-to-have


% - I tried to choose enhanced and summary part but it was too much and enhanced part contains bias like timestamp and so on, so I decided to choose only summary part and later only segments from it, using whole summary lead us to really slow convergence...I think (try to find between models...)

% reference json grinder and its usage to process data

% - with enhanced part and without it
%   - Report some results per signature (some reference in appendices) report even balancing of data set for particular signature
%   - comment each
%   - compare to similar experiments (there are experiments with also data from sandbox)



% To appendix add everything, for example even extracted schema...

% Describe data loading and modelling, split conditions, random minibatch... - reference code in attachment, we did follow parameters in similar models (Mandlik, identification.jl) and it was starting point, describe parameters and conditions, we can also dump tree of extractor and corresponding model


% If we started with signatures we can report some statistics (basic principle should be presented in previous chapter)

% Basic statistics for various parts and variants
%   - signature histogram (balanced) - histogram.csv
%   - ...
%   - a lot of Emotet (maybe should be on another place) (malware family problem I think, mention among potential biases), where is this attribute in the original report?
%   - - Balanced dataset - in term of accuracy metric performance
%   - Bias - - Bias in practical data like this - security data, what are the influences (as an example could be ip addresses...)

% Using:



% \todo{In this section we will formulate why we started with signatures (and even ended), also why we started with summary and enhanced and than skipped only to summary part}

% Previous connection
% - Based on previously chosen data and described theory of hmill we have chosen technology and tried to train the model in different setups with different data
% Way through this chapter

% Next connection
% - Resulting models will be objects of explanation experiments



% DIRECTLY in output
% 	- API calls - are in in summary part
% 		○ "injection_rwx" 28251
% 		○ "antidebug_setunhandledexceptionfilter" 18223
% 		○ "removes_zoneid_ads" 11070
% 		○ "deletes_self" 10805
% 		○ "stealth_timeout" 8253
% 		○ "enumerates_running_processes" 6324
% 		○ "injection_runpe"  5542
%   - Files - are in summary part
% 		○ "copies_self" 7137
% 	- Commands - are in summary part
% 		○ "uses_windows_utilities" 6987

% NOT DIRECTLY in output
% 	- Processes - not directly in summary part
% 		○ "dropper"  6045
% 	- Time - not directly in summary part
% 		○ "antisandbox_sleep" 15810
% 	- AuthSign - not directly in summary part
% 		○ "invalid_authenticode_signature" 14346
% 	- Entrophy - not directly in summary part
% 		○ "packer_entropy" 8815
% 	- Combinations - 
%     "stealth_network" 26430
