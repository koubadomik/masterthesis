\chapter{Modelling classifier} \label{chap:models}
In this chapter we want to present our \emph{hmill} models and their results. Our goal is to train the models using hyperparameters based on prior experiments with \emph{hmill} framework. We are not going to perform hyper parameter tuning and other robust techniques, our goal is to demonstrate that it is possible to train such model and later explain it. We do not aim to strictly perform long sequence of experiments and compare to prior. On the other hand we would like to present similar metrics of evaluation. 

We do not model all variants of features and states mentioned in previous chapter, we concentrate on at least one working model and then we move on to model explaining experiments.

In previous chapter we formulated several possibilities for model output and input. We use \emph{hmill} framework for data modelling (\ref{chap:hmill}). From \emph{states} and \emph{features} in question we started with our top candidate - \emph{signatures} as states and \emph{behavior log} as features. 

Our reasoning about states is based on the fact that the signatures are usually assigned based on some atomic fact, for example presence of some api call in log. If we choose states which are part of behavioral log where this information is presented, we should be able to evaluate model's prediction or draw some conclusions based on it. 

Also we want to see if the model is able to generalize the knowledge and maybe come up with some new explanation of particular signature or just things which go with the original subject of particular signature. By subject we mean the subset of the original log based on which presence the sandbox assigned current signature.

More complex labels like malware family or malware score are something what we can explore later. We assume that the assignement of those is often supported by signatures themselves and it should be better to explore the original cause of the whole chain.

\section{Behavior as feature vector}
After further look at the \emph{behavior} part of \emph{report.json} we start with only \emph{summary} and \emph{enhanced} part. The reason is that other parts are quite comprehensive and we will not be able to train the model with the hardware resources we have. \todo{some better arguments}. If the model would be still to complex to train under our conditions we omit also the \emph{enhanced part}. At such case we would lost mainly the information about the order of events because the summary part only lists occurences of artifacts (api calls, commands\dots).

\section{Signatures as states}
Brief description of signatures is in previous chapter (\ref{chap:data}). Signatures are assigned to each sample based on its behavior. In \ref{app:histogram} we can see histogram of the most frequent signatures in our dataset.

From the original histogram we choose specific subset based on the frequency (not too much and too little), presence of implementation in python repository (not all of them we were able to find) \todo{some better arguments,...}. In the \ref{app:frequecies} we can see our candidates and their frequencies in dataset.

Next task is to investigate what the implementation of particular signature is about and try to create groups for further conclusion making. \emph{Python} implementation of each signture is processing the output of sandbox and generates boolean values signing that the subject of particular signature is presented in sample behavioral log. The subject of signature can be api call, dropped file, process, command and even something more complex. If the signature implementation returns \emph{True}, we observe instance of this signature in the \emph{signatures} part of \emph{report.json}. An example of such signature item coud be seen in \ref{lst:signature}. The most important aside from \emph{name} and \emph{description} is field \emph{data} which contains detected signature subject.

\begin{lstlisting}[language=json, caption={Example of signature item in \emph{report.json}},captionpos=b, label={lst:signature}]
  {
      "name": "antisandbox_sleep",
      "description": 
        "A process attempted to delay the analysis task.",
      "severity": 2,
      "weight": 1,
      "confidence": 100,
      "references": [],
      "data": [
        {
          "Process": 
              "explorer.exe tried to sleep 300.0 seconds"
        }
      ],
      "new_data": [],
      "alert": false,
      "families": []
    },
  
\end{lstlisting}



We explored two types of grouping aspects. First is the subject of particular signature and then second is if we can see this subject in the behavioral log directly. For instance if the signature implementation is using api calls so the api call list is directly in the log. If it uses entrophy of dropped files, it is not directly in the output. In listing \ref{lst:signatureimpl} we can see simple example of \emph{antidebugsetunhandledexceptionfilter} signature which is just checking presence of specific signature.

\begin{lstlisting}[language=mypython, caption={Example of signature implementation},captionpos=b, label={lst:signatureimpl}]
  
  class antidebug_setunhandledexceptionfilter(Signature):
  name = "antidebug_setunhandledexceptionfilter"
  description = "SetUnhandledExceptionFilter detected (possible anti-debug)"
  severity = 1
  categories = ["anti-debug"]
  authors = ["redsand"]
  minimum = "1.3"
  evented = True

  def __init__(self, *args, **kwargs):
      Signature.__init__(self, *args, **kwargs)

  filter_apinames = set(["SetUnhandledExceptionFilter"])

  def on_call(self, call, process):
      if call["api"] == "SetUnhandledExceptionFilter":
         return True
  
\end{lstlisting}


Based on our reasoning we identified following categories:

\begin{enumerate}
  \item Directly in \emph{report.json} in parts \emph{summary, enhanced}
  \begin{itemize}
    \item API calls - \emph{injectionrwx, antidebugsetunhandledexceptionfilter, removeszoneidads, deletesself, stealthtimeout, enumeratesrunningprocesses, injectionrunpe}
    \item Files - \emph{copiesself}
    \item Commands - \emph{useswindowsutilities}
  \end{itemize}
  \item Not directly in \emph{report.json} in parts \emph{summary, enhanced} (some clues might be presented, but the direct information is not there)
  \begin{itemize}
    \item Processes - \emph{dropper}
    \item Sleep - \emph{ antisandboxsleep}
    \item Entrophy - \emph{packerentropy}
    \item Combinations - \emph{stealthnetwork}
    \item Authsign - \emph{invalidauthenticodesignature}
  \end{itemize}
\end{enumerate}

More about particular signatures could be found in \ref{app:signatures}.

The reason why we categorize signatures in such manner is that we would like to connect those categories with the model performace in the further discussion. We expect that different categories might have different performance.

\section{Model}
We create binary classifier which predicts the presence of particular signature. Later we can train even multilabel classifier which would assign set of signatures according to dynamic analysis but the most important for us is the binary classification, its performance and its explanations. Multilabel classifier could be seen as multioutput binary model, so if we are able to scale up the hardware resources and its usage using available techniques (multithreading) multilabel model should be easily deriveable from original simle binary classfier.

Technology stack we used to train \emph{hmill} model is described in \ref{app:technologies}. Important is that we unsuccesfuly experimented with multithreading in \emph{Julia} language for gradient computation. The resulting model was trained on one CPU which a little bit shrinked our possibilities.

First experiments are without reported results because model was to huge to converge to some significant performance. We were not able to train it on hardware and with thread limitationsin in feasible time. This reality led us to skip one part of feature vector - \emph{enhanced} part. So the resulting model is working with \emph{summary}p art of original report only. Looking back on this part of experimenting we know that the problem might have been even in hyper parameter choice. But tighter feature vector was advantage for further training time and we still know that summary parts include compressed information which we are aiming at (\emph{api calls, files, mutexes, registries, commands}).

Original code of the model is in thesis attachment \ref{app:attach}. The pseudocode of the whole process can be seen in \ref{algo:hmillbinary}. It is standard binary classifier. What is not so ordinary is that the model is function of the input data (typical for \emph{hmill}). Another important fact is that the \emph{hmill} framework \emph{API} is more general so its is not accepting json documents directly. For this purpose we use \emph{JsonGrinder} library which is able to accept array of json documents and produce schema (more in \ref{app:technologies}). Schema is then used to create \emph{hmill} structures from each json document. Schema extraxted from our data is in \ref{app:schema}.

\todo{add pseudocode of analyser}

\begin{algorithm}
  \caption{Dependency Graph Assembly}\label{algo:hmillbinary}
  \begin{algorithmic}[1]
      \Procedure{MyProcedure}{}
      \State $\textit{stringlen} \gets \text{length of }\textit{string}$
      \State $i \gets \textit{patlen}$
      \If {$i > \textit{stringlen}$} \Return false
      \EndIf
      \State $j \gets \textit{patlen}$
      \If {$\textit{string}(i) = \textit{path}(j)$}
      \State $j \gets j-1$.
      \State $i \gets i-1$.
      \State \textbf{goto} \emph{loop}.
      \State \textbf{close};
      \EndIf
      \State $i \gets i+\max(\textit{delta}_1(\textit{string}(i)),\textit{delta}_2(j))$.
      \State \textbf{goto} \emph{top}.
      \EndProcedure
  \end{algorithmic}
\end{algorithm}

\subsection{Model hyperparameters and dataset attributes}
In our experiment we do not involve techniques like early stopping or hyperparameter tuning. Our goal is to demonstrate the classifier performance we do not aspire to spend a lot of time in this part. We build on previous experiments namely on \cite{Mandlik2020} where author published even the set of hyperparameter used in \emph{device id} example. This example was also from cybersecurity field. Those parameters we used as initial.

We were not succesful immediately, it took several attempts of our hand-picked adjustments. After each adjustment we tried to train the model and observed convergence of loss function and balanced accuracy on training and testing data. Due to the fact that we did not tune hyperparameters or anything else, we did not use validation set. Due to the good ration for testing and training data and signature distribution and due to the convincing performance we did not involve cross validation.

In the table \ref{tab:hyperparams} we can observe hyperparameters of the model and other training-independent facts.


\begin{table}[h]
  \centering
  \caption{Parts of \emph{report.json}}
  \begin{tabular}{p{6cm}p{8cm}} 
      \toprule
      \textbf{Parameter} &
      \textbf{Value} \\
      \midrule
      samples & $80000$ (1:1 testing and training) \\
      \midrule
      minibatch size & $1000$ (random subsampled from training set in each iteration)\\
      \midrule
      hidden units (neurons)& $20$\\
      \midrule
      iterations & $120$\\
      \midrule
      optimizer & \emph{ADAM} \cite{Kingma2014}\\
      \bottomrule
  \end{tabular}
  \label{tab:hyperparams}
\end{table}


Difference from \cite{Mandlik2020} is \emph{minibatch size} and \emph{number of iterations} which worked for us in this changed form. Data used in their case were of much smaller scale (\emph{device id}) which could be the reason.

The number of \emph{iterations} we setup so that we observe reasonable convergence. We did not challenged this parameter, we expect further convergence. We checked overfitting by monitoring difference between accuracy on training and testing data. They does not differ significantly.

\section{Results}
Chosen metrics are based on metrics which are reported by similar papers \todo{reference Pevny, Madlik and Stiborek}. We also follow the chapter \ref{chap:classification} where we mentioned classifiers in cyber security field and their pitfalls. The calculation was supported by \emph{EvalMetrics.jl} library (\ref{app:technologies}).

\begin{table}[h]
  \centering
  \caption{Models performance (values rounded for 3 decimal spaces)}
  % \begin{tabular}{p{5cm}p{3cm}p{3cm}p{3cm}} 
  \begin{tabular}{llll}
      \toprule
      \textbf{Signature} &
      \textbf{Bal. acc.} &
      \textbf{FNR} &
      \textbf{FPR}
      \\
      \midrule
      antidebug setunhandledexceptionfilter & $0.9801$ & $0.0289$ & $0.0109$ \\
      \midrule
      copiesself & $0.924$ & $0.125$ & $0.0279$ \\
      \midrule
      deletesself & $0.997$ & $0.005$ & $0.002$ \\
      \midrule
      enumeratesrunningprocesses & $0.972$ & $0.050$ & $0.007$ \\
      \midrule
      stealthtimeout & $0.701$ & $0.064$ & $0.331$ \\
      \midrule
      useswindowsutilities & $0.958$ & $0.078$ & $0.006$ \\
      \midrule
      removeszoneidads & $0.999$ & $0.000$ & $0.000$ \\
      \midrule
      antisandboxsleep & $0.969$ & $0.037$ & $0.026$ \\
      \midrule
      dropper & $0.911$ & $0.147$ & $0.032$ \\
      \midrule
      invalidauthenticodesignature & $0.607$ & $0.668$ & $0.113$ \\
      \midrule
      packerentropy & $0.605$ & $0.748$ & $0.043$ \\
      \midrule
      stealthnetwork & $0.942$ & $0.008$ & $0.109$ \\
      \bottomrule
  \end{tabular}
  \label{tab:models_res}
\end{table}

Results of experiments could be seen in \ref{tab:models_res}, there are also some visualizations in \ref{app:models}.

\section{Discussion}
% Compare signatures

% - technically signatures are deterministic (heuristic) view on the same thing, but in case of machine learning we are traing to target the result statistically (point of discussion)

% Describe general intention and direction of reasoning - what we want to find in models, success criteria
% We expect that net will be working on samples where the subject of signature is directly in the report, hopefully. Than that in such situation we can find something else. Finally, in case that the signature subject is not easily observable from report, we can find something else or just say ok, we are not able.
Each group or each signature in particular. Our expectation compared to real results.

We performed also several experiments with more general multilabel classifier but we did not observe convergence under our resource conditions.


Conclusion of this chapter is that we are able to train \emph{hmill} classifier to classify presence of particular signature based on summary part of behavioral log from \emph{CAPEv2} sandbox. This classifier has significant performance more than $90\%$ as balanced accuracy for majority of signatures and this is sufficient for further explaining experiments. Such models should have strong confidence for us to be able to choose some samples for explainer (\ref{chap:explain}). Obviously we are able to classify only some of signatures with such performace which we mentioned in discussion.










%NICE TO HAVE
% in conclusion clarify that we did not used only summary part of json
% \todo{clarify this in section introductions and so, not to confuse somebody with those ongoing changes}.
% \todo{we may: Connect to the chapter about classification generally - hmill model, what specifically we use}
% \todo{If not enough data and discussion, we could try to discuss the data part of signature and original sample and how it corresponds to the performance of our models}
% \todo{we can compare to others but not necessary}
% \todo{report table of metrics including the balance of dataset (pie graph?) and at least one graph}
% Convergence went on we just did not have enough time and performance to run it again and again (maybe I can plot loss function plot to prove that )
%%-----------------------------------------------

% But our goal was to train the classifier and then explain it so we are able to examine trained classifiers and that was succesful.

% Mention multilabel just marginally.
% mention even multilabel as addition (in principle it should be possible but for us it was not priority)
% - mention experiments (several attempts we have) and accuracy per item but only as something as addition - maybe more iteration, because multilabel is just multioutput single label, predicting at once should be possible, for it was not priority
% check overfitting - Pevny said that balanced accuracy on training set
% Minibatches are generated randomly from training set in each iteration \todo{reference it in the classification theory part if it is there}.

% In figure \todo{add histogram visualization with percentage, not exact numbers} There is a histogram where we can observe frequency of each signature in our dataset.
% - Used technologies, several experiments, pseudocode (algorithm), hyper parameters, report evaluation metrics,types of classifiers used (motivation for that) COMPARISON WITH PRIOR ALTERNATIVES (I think it is not so necessary our goal is more explaining of the model?, Mention metacenter (even in thanks part)
%   - Modelling - single label classifier
%     - GOAL: Using HMill, create models. Report results.
%   - Modelling - multi label classifier
%     - GOAL: Using HMill, create models. Report results.
%   - Performance, early stopping, hyper parameters...
%   - Detectection of some signatures, some not, why - no data; why...
%     - technically signatures are deterministic (heuristic) view on the same thing, but in case of machine learning we are traing to target the result statistically
%     - we did not care so much about tuning hyper parameters and comparing
%        - We used parameters similar to similar experiments with same framework, 1000/150 look to converge (hopefully)
%        - we experimented with different kind of signatures (statistically even functionally) - describe the kinds (report all results, compare using f-score or FNR, FPR for example)
%        - than we experimented with multilabel case - trying to predict more than one signatures at one time
%   - (Technical background, used metascenter archicture...)
%   - reference Mandlik's and Pevny's works, hopefully similar
%     - Mandlik is reporting
%       - Single label - from each signature kind atleast one (but possibly more) - For different kinds of signatures - create it as a part of training itself
%         - Params: empirically and based on previous experiments, no hyper parameters tuning (some of signatures have significant performance with chosen so we did not spend time with that, not goal of this thesis)
%             - samples 80_000, test: 0.5, train: 0.5
%             - minibatch - 1000
%             - neurons - 20
%             - iterations (random subssampling) - 120 (should be sufficient), mention it by each signature particularly because some of jobs did not catch it on time so the number of iterations is smaller
%             - optimizer ADAM
%         - before training:
%             - balance of training set - how many positive and negative - DONE
%                 - I can use pie graph
%         - At the end of training
%             - metrics: accuracy, balanced accuracy - DONE
%             - curves: prcurve, roccurve (normal and with log on x-axis) - DONE (scores and targets)
%                 - roc curve with log scale on x axis (FPR is more inportant for us)
%             - comparative: 
%                 - f-score, auroc, auprc - DONE
%                     - I can argument as comparison with Mandlik, if it is going to work
%                 - tpr, fpr, acc - DONE
%                     - Jan Stiborek comparison
%                     - I can use use column graphs
%         - multilabel - nice-to-have


% - I tried to choose enhanced and summary part but it was too much and enhanced part contains bias like timestamp and so on, so I decided to choose only summary part and later only segments from it, using whole summary lead us to really slow convergence...I think (try to find between models...)

% reference json grinder and its usage to process data

% - with enhanced part and without it
%   - Report some results per signature (some reference in appendices) report even balancing of data set for particular signature
%   - comment each
%   - compare to similar experiments (there are experiments with also data from sandbox)



% To appendix add everything, for example even extracted schema...

% Describe data loading and modelling, split conditions, random minibatch... - reference code in attachment, we did follow parameters in similar models (Mandlik, identification.jl) and it was starting point, describe parameters and conditions, we can also dump tree of extractor and corresponding model


% If we started with signatures we can report some statistics (basic principle should be presented in previous chapter)

% Basic statistics for various parts and variants
%   - signature histogram (balanced) - histogram.csv
%   - ...
%   - a lot of Emotet (maybe should be on another place) (malware family problem I think, mention among potential biases), where is this attribute in the original report?
%   - - Balanced dataset - in term of accuracy metric performance
%   - Bias - - Bias in practical data like this - security data, what are the influences (as an example could be ip addresses...)

% Using:



% \todo{In this section we will formulate why we started with signatures (and even ended), also why we started with summary and enhanced and than skipped only to summary part}

% Previous connection
% - Based on previously chosen data and described theory of hmill we have chosen technology and tried to train the model in different setups with different data
% Way through this chapter

% Next connection
% - Resulting models will be objects of explanation experiments



% DIRECTLY in output
% 	- API calls - are in in summary part
% 		○ "injection_rwx" 28251
% 		○ "antidebug_setunhandledexceptionfilter" 18223
% 		○ "removes_zoneid_ads" 11070
% 		○ "deletes_self" 10805
% 		○ "stealth_timeout" 8253
% 		○ "enumerates_running_processes" 6324
% 		○ "injection_runpe"  5542
%   - Files - are in summary part
% 		○ "copies_self" 7137
% 	- Commands - are in summary part
% 		○ "uses_windows_utilities" 6987

% NOT DIRECTLY in output
% 	- Processes - not directly in summary part
% 		○ "dropper"  6045
% 	- Time - not directly in summary part
% 		○ "antisandbox_sleep" 15810
% 	- AuthSign - not directly in summary part
% 		○ "invalid_authenticode_signature" 14346
% 	- Entrophy - not directly in summary part
% 		○ "packer_entropy" 8815
% 	- Combinations - 
%     "stealth_network" 26430
