\chapter{Model explaining} \label{chap:expth}
In the following chapter, we describe the model explainability definition and techniques used in practice. Finally, we give details on \emph{HMill} explainer, which we use in our experiments. At the very end, we describe our use case of signature's binary classifier explanation.

In the case of complex models, we often want to explain their predictions to be sure about their reliability. The model explainability is crucial in critical systems, e.g., health, transport. If we want to use our models and want people to believe them, we should explain their predictions and demonstrate that they are not based on random correlations in the training data. For example, in \cite{Pevny2020}, authors identified that their model classifies mainly according to the timestamp field in the original sample, which was different for malware and cleanware samples. That was an obvious mistake because this detail is not the difference between malware and cleanware but between analysis conditions.

In May 2018, General Data Protection Regulation (GDPR) became law. It has innovative clauses on automated decision-making and, to some extent, even the right of its explanation. All individuals might enforce to obtain \say{meaningful explanations of the logic involved} when automated decision making takes place \cite{Guidotti2018}. That is a significant scientific challenge in the field where we face such a great boom regarding the statistical model's performance and a disproportionately weak understanding of its behaviour. As an example, we can see the neural net generalization, which is still very challenging for us \cite{Zhang2016}.

Techniques of interpretation and explaining are growing in popularity as a tool for further statistical model analysis. It might lead us to better model understanding or shed some more light on the examined domain (extract new knowledge) \cite{Montavon2018}.

\section{Definition}
Based on \cite{Montavon2018} we define two essential terms.
\begin{definition}
An \emph{iterpretation} is the mapping $\mathcal{Y}\rightarrow\mathcal{D}$, where $\mathcal{Y}$ is a \emph{state space} (defined in \ref{chap:classification}, e.g., real-valued vectors or sequences,  $\mathcal{D}$ denotes a domain which is human-readable and understandable (image, heatmap, sequence of words, etc.).
\end{definition}
% Furthermore, we assume the domain to be the same as the feature space $\mathcal{X}$. It means that if we classify images, the interpretation would be an image.


\begin{definition}
An \emph{explanation} $e \in \mathcal{X}$ is a subset $e$ of sample $x$ that contributed for the predicted state $h(x)=\hat{y} \in \mathcal{Y}$, or contributed significantly more than other members of $x$.
\end{definition}

We often express the explanation as the original feature vector with a relevance score vector, e.g., real-valued vector of the same dimension as the input, where positive items indicate relevant features and zeros indicate irrelevant ones \cite{Montavon2018}.

This definition of the \emph{interpretation} is quite vague because human readability and understandability is not something to measure or observe precisely. On the other hand, the explanation is a little bit more specific. For example, we can see the task where we aim to select part of the original feature vector responsible for most probability accumulated in the output, e.g., softmax output. This output might be interpretable by human, or we have to find another mapping to an interpretable domain, e.g., \ $\mathbb{R}^{n}\rightarrow\mathcal{X}$. For instance, if we get an explanation of network classifying images, we might get a real-valued matrix. However, we translate it back to the image with highlighted pixels to be better understood. \cite{Lapuschkin2015, Simonyan2014, Landecker2013}. If our subject would be natural language processing, the explanation might be a highlighted text \cite{Arras2017, Li2016}.

A different perspective of understandability is looking at the available time that the user (human) is available or allowed to spend on the explanation understanding \cite{Guidotti2018}. Then we can check the complexity or quality of the explanation by measuring the time to understand the explanation. However, we have to control the person's background knowledge performing the interpretation \cite{Guidotti2018}.

Authors of \cite{Montavon2018} address that the primary bias in \emph{interpretability} is that the majority of the model evaluation metrics works with the model as with a black box. Sufficient information for the evaluation is just a set of predicted labels (or class probabilities) and ground-truth labels. Often, we do not observe model parameters, hyperparameters, model hypothesis space, etc. 

Another problem is that some situations are not easily transferable to a numerical form, e.g., real-valued vectors. Examples we can find in ethics or legality. Interpretability of such concepts is ambiguous even in the world without machine learning.

\section{Categorization}
Based on \cite{Guidotti2018}, and \cite{Lipton2016}, there are several aspects according to which we can categorize the model explaining.

If we know the model we are explaining and investigate its functioning, parameters, and other details during the learning process, we speak about the white box explaining --- often called \emph{transparency}. In this case, the goal of explanation might be to answer the question \emph{How does the model work?}. On the other hand, if we examine the model's output without considering what model is used and how it works, we speak about black box explaining --- often called \emph{post-hoc} explanation. In this case, the question is \emph{What else can the model tell us?}.

Based on the scope of interpretation, we may distinguish two categories. \emph{Global interpretability} means that we can interpret all predictions. We know interpretation $f$: $\forall y \in \mathcal{Y}$, $\exists i \in \mathcal{D}$ $f: y\mapsto i$. In other words, the interpretation is mapping (or relation), and if the model is globally interpretable, this mapping is \emph{serial} (also called \emph{left-total}). On the other hand, \emph{local interpretability} means that we can interpret only some predictions, so the relation is not \emph{serial}.

\section{Explanability desiderata}
In \cite{Lipton2016}, authors present a comprehensive insight into interpretability research and interpretable model properties. We list some of them below.

\subsection{Trust}
Trust is the first term which is very complicated even due to its own interpretation. We might build trust in the model's performance, so the better the model is, the more trustworthy. However, the accuracy of the model is not the only metric we should consider. The situation is more complex in the sense that we need to examine the whole context. By context, we mean the accuracy on specific examples, e.g., unseen examples or examples where people can classify with high precision. We should also evaluate the accuracy repeatedly in time.

\subsection{Causality X Correlation}
Correlation of two random variables $X,Y$ is defined in \eqref{eq:corr}, where $cov(X,Y)$ denotes covariance and $\sigma$ standard deviation. On the other hand, causality is defined as the relation between $X$ (a cause), which contributes to the production of $Y$ (an effect), $X, Y$ might be events, processes, states, objects or generally random variables.

\begin{equation} \label{eq:corr}
    \rho_{X,Y}=\frac{cov(X,Y)}{\sigma_X\sigma_Y}
\end{equation}

Every statistician was instructed about situations where the data show us the results we want to see. That might be demonstrated in examples that should be so absurd that nobody can take it seriously. For example, the divorce rate in Maine correlates with the per capita consumption of margarine between 2000 and 2009 \footnote{http://www.tylervigen.com/spurious-correlations}. Those correlations are often called \emph{spurious}. However, we have to remember that the underlying process generating the data is assumed to be random no matter how complex it can be. We must not forget that the researcher is making assumptions and choosing what data are modelled. The statistical model itself should not serve as an argument for the cause and effect relation between modelled variables. If we want to conclude causality, we should involve other experiments and research in the particular domain to uncover the generating process itself. On the other hand, the correlation is easily measurable, and a conclusion about it is based only on its calculation. More on this topic can be found in \cite{Kenny1979}.

Even if we observe the correlation and know that the relationship is not random, it is difficult to conclude the cause and effect. One of the possible reasons might be a confounding variable. It is a variable influencing both features and states of our model. If such a variable exists, we might falsely conclude causality, although the confounder causes the correlation. More on this topic can be found in \cite{Skelly2012}.

\subsection{Transferability}
In a usual setup, we split our data randomly and create a training and testing set. Then we estimate the generalization error by observing the difference between training and testing error. However, regarding the model deployment, we should observe its behaviour in practice. It might face different situations or, even worse, its deployment might influence the domain itself. The difference between training and real data might be caused by the solid assumptions we make but cannot meet. This trend refers to the robust statistics field where we face the problem of assumption violation \cite{Erceg-Hurn2008}.

\subsection{Informativness}
This point is about the model's ability to extend human intuition and knowledge by pointing out the most important parts of comprehensive inputs. It can also provide a stronger overview of the space we are examining, e.g., provide some similarity measure on our examples, which might be essential for gaining labelled data using unsupervised or semisupervised learning.

\subsection{Fair and ethical decision making}
If we want algorithms making autonomous decisions under our control and being of our interest, we need to interpret its decisions. That is a very significant issue because we need to deal with the fact that artificial intelligence is much more capable of making fast and precise decisions than humans. It is not clear if we do not degrade artificial intelligence capabilities by trying to understand it. However, it is necessary because we have already used it against each other, e.g., \cite{Boldyreva2018}. By adopting GDPR, we face this challenge even for legal reasons. The field of ethics in AI \cite{Siau2020} is an inexhaustible well of challenges beginning with autonomous driving and ending with absolute manipulation of a mass of people.

\section{Interpretable model}
As we mentioned, we can distinguish two basic types of explanation according to our goal --- \emph{transparency} and \emph{post-hoc}.

\subsection{Transparency}
Authors in \cite{Lipton2016} refer to several attributes which can be treated during a particular model's \emph{transparency} research --- \emph{simulatability, decomposability} and \emph{algorithmic transparency}.

By \emph{simulatability} is meant that the model prediction can be simulated by a human in a reasonable amount of time given the model parameters and input example. This capability is closely connected with the model complexity. That might seem like it is about the type of model such that we can say that, e.g., decision trees are better interpretable than neural nets. The truth is that simpler models like linear regression or decision trees tend to be better interpretable, but it is because they are usually involved in straightforward use cases. On the other hand, simulatability is strictly determined by a limited amount of human cognition. That leads us to conclude that a very complex decision tree is not more transparent than a lightweight neural net.

\emph{Decomposability} stands for the ability that each input, parameter and calculation admits an intuitive explanation. The input interpretability throws out the game majority of models where dimensionality reduction and other feature engineering techniques are involved. The interpretation of parameters and calculations might be a human-readable description of decision tree nodes, and the opposite might be a large number of weights and biases in a neural net.

The last notion of \emph{algorithmic transparency} is about observing the learning algorithm and its mathematical background. The algorithm has to be fully explorable using mathematical tools. For example, in the linear model, the shape of the error surface can be understood, and we can prove that the training process will converge to a unique solution. On the other hand, heuristic algorithms used in deep models like \emph{stochastic gradient descent} can not be fully observed, and we cannot be sure about its adaptation in a new use case.

Examples of a model with a significant level of transparency are linear/logistic regression, decision trees, KNN, Rule-based learners, and generative additive models. More on this topic can be found in \cite{Arrieta2019}.

\subsection{Post-hoc}
Post-hoc interpretability has a different goal than the previous approach. It can extract more information from the model and help us gain new overall knowledge or understand what is in the input that causes such a prediction. This technique can be used to interpret opaque (not transparent) models without examining their complex logic.

There are several techniques for post-hoc interpretation. Their list follows.

\begin{itemize}
    \itemsep0em 
    \item \emph{Text explanation} --- We still assume that we cannot assign textual interpretation by hand because that might mean that our model is transparent. However, we might automate the inference of a textual form. We can train another model which maps the prediction of the original model to its textual explanation. An example of such an approach is in \cite{Krening2017}, where authors trained a reinforcement model to perform a particular task and a second model to explain its decisions.
    \item \emph{Visualization} --- Images and other visual outputs are considered very straightforward for human understanding. We often involve dimensionality reduction and other techniques to display the situation in two or three dimensions, such as \cite{Pearson1901}. An example of a visualization of neural net explanation using heatmap can be found in \cite{Zeiler2013}.
    \item \emph{Local explanations} --- This approach explains specific parts of the training set, e.g.,  specific samples. It might find the parts of a sample that contribute to the prediction the most. An example might be the saliency map used for neural nets \cite{Simonyan2014}. It is important to emphasize that the explanation has to be treated in a specific context, i.e., the saliency map might change drastically even if the example was changed only slightly.
    \item \emph{Explanations by example} --- Usually, if the teacher explains the theory with a running example, especially in mathematics, there is a much greater chance that many students would understand. The principle is the same in the model explanation by example. The model can provide such an explanation along with predictions, e.g., prediction and a set of examples which are similar  \cite{Caruana1999}. We might involve a clustering algorithm.
    \item \emph{Explanations by simplification} --- This approach aims at model simplification while maintaining its performance.
    \item \emph{Feature relevance explanation} --- We aim at scoring the input variables, which are later compared based on their scores, and we can conclude which variables are the most important for a particular prediction.
\end{itemize}

This kind of explaining is used with SVM, where we can see model simplification or local explanations. Other significant models are neural nets and their variations, where we see feature relevance and visualization techniques. Examples can be found in \cite{Arrieta2019}.

Another example of a general model explaining is in \cite{Alvarez-Melis2018}, where authors focus on self-explaining models. Other sources might be \cite{Strumbelj2013,Robnik-Sikonja2008,Montavon2018}.

\section{Explaining HMill models}
Our model can model tree-structured data --- \emph{JSON} documents, as stated in the previous chapter. This section describes an explanation method for \emph{HMill} models proposed and demonstrated in \cite{Pevny2020}. Introduced explainer attempts to explain structured \emph{HMill} model. It uses a post-hoc approach with \emph{feature relevance explanation}. So far, it is the only known approach of \emph{HMill} model explanation.

The goal of \emph{HMill} explainer is to find a minimal subset of the input sample (\emph{JSON} subtree), which is classified to the same class as the original sample. We can identify what parts of the original \emph{JSON} are the most relevant. It might also improve our understanding of the domain-specific knowledge, as the authors state.

In the following text, we assume an \emph{HMill} binary classification model being a black box decomposed only to two function $h$ and $f$. The first function is $h(d)=v$ where $d$ is \emph{JSON} document or its subtree and the output $v\in\mathbb{R}^{m}$ represents the embedded sample (all abstract model nodes are evaluated but the root). The embedded sample is then classified by $f(h(d))=c$, where $f$ denotes final abstract model node evaluation function and $c$ confidence that $d$ belongs to the positive class.

\subsection{Explainer steps}
The basic idea builds on top of the subtree selection problem solution. Our problem is a specific case of subtree selection. It can be formulated such that for a given tree $T$, an expensive evaluation function $r(t)=q$ ($q\in\mathbb{R}$) and a threshold $\tau\in\mathbb{R}$, we want to find subtree $t$ with minimal number of nodes such that $r(t)\geq\tau$. 

In the case of \emph{HMill} explainer, the input tree is a \emph{JSON} document, and the evaluation function is $f$, which outputs the confidence that the document belongs to a positive class. The authors introduced several subset selection methods, which is a less general case of subtree problem. It is not possible to adopt subset selection methods in subtree problems directly. Firstly, we have to decide if we need to maintain the tree structure of the result or not. The authors mention two solutions to deal with this problem: first, ignoring the structure and the second, exploiting it.

\subsection{Subset selection}
\emph{Greedy addition} starts with an empty subset and in each step adds a new item until the threshold is achieved. Each new item is chosen such that the gain in the evaluation function is maximal across all elements which are not in the subset.

\emph{Heuristic addition} sorts the elements in the set by heuristic ranking (see below) and adds elements to the subset starting from the largest ranking until the threshold is achieved.

\emph{Random removal} might follow any of the previous methods. It starts with the initial subset, which has already achieved the threshold. It permutes all items in the subset and removes items from the beginning until the evaluation drops below the threshold. If it drops, the lastly removed item is added back, and the algorithm continues with a new permutation.
% \emph{Fine tuning}
\subsection{Minimal subtree adaptation}
\emph{Flat search} performs subset selection on isolated nodes of the tree. The root is added to the explanation by default. After subset selection is made, all nodes which are not reachable from the root are removed from the explanation. That can be done because they do not impact classification based on the definition of the \emph{HMill} model tree structure.

\emph{Level-by-level search} performs subset selection on each level of the tree. It takes into account only nodes whose parents are in the explanation so far.

\subsection{Subtree ranking}
\subsubsection{Model gradient ranking}
This approach is based on the absolute value of gradients for the parts of the input. That is adopted even in the case of a saliency map of image processing neural net. The crucial idea is to compute the gradient vector (\eqref{eq:grad}) of the model with respect to the embedding of a particular subtree $c$ of a particular sample $d$. Alternatively, we want to examine how much a slight change in the embedding of a specific subtree influences the model's prediction. Computation of the gradient ranking of a particular subtree is the absolute value of the sum of items in the resulting gradient vector. Note that $h(d)$ is originally a function of $h(c)$, which is not obvious from \eqref{eq:grad}. However, if it was not, the derivative would be zero.

\begin{equation} \label{eq:grad}
    \frac{\partial f(h(d))}{\partial h(c)} \in \mathbb{R}^m
\end{equation}

% \begin{equation} \label{eq:grad2}
%     r(c)=|\sum_{i=1}^{m}gr_i|
% \end{equation}

\subsubsection{GNN explainer mask ranking}
This method is originally designed to explain \emph{GNN} models \cite{Ying2019}. The main idea is to use this explainer for the graph created by \emph{JSON} subtrees and edges between them. Explainer uses a mask $m=\mathbb{R}^{|\mathcal{E}|}$, where $|\mathcal{E}|$ denotes all edges between the subtrees and their predecessors in a \emph{JSON} document and $m_i\in[0,1]$. This mask represents how much information is passed along each edge during the update step of GNNs. Suppose the explainer is asked to explain a classification decision on a particular subtree. In that case, the mask is optimized using stochastic gradient descent to maximize the probability of correct classification on that subtree. After the optimization, the explainer suggests $k$ edges with the largest values as an explanation. The value of $k$ should be properly tuned because the explanation does not have to be classified to the same class as the original sample if $k$ is fixed in advance. The values of the mask might be used as ranking in \emph{HMill} explainer.

\subsubsection{Banzhaf values}
Game theoretical approaches in feature extraction which uses \emph{Shapley values} and \emph{Banhaf values} can be found in \cite{Afghah2018}. Both come from the cooperative game theory. They are related to the metric of how much a certain player contributes to various coalitions on average. In feature extraction, the Banzhaf value might describe interdependency among the extracted features and their relevancy to the target class. On the other hand, \emph{shapley value} might be used to show the contribution of a particular feature in improving the classification accuracy when all possible coalitions of features are considered. In the case of \emph{HMill} explainer, the authors used Bazhaf values. 

A sampling algorithm is used to approximate Banzhaf values \cite{Bachrach2010}. There are two values stored for each subtree in the \emph{JSON} document -- the classifier's average confidence in coalitions which includes the subtree and the average confidence of coalitions that do not. Coalitions are generated randomly. After running more iterations, the approximation of Banzhaf values for each subtree is the difference between the two values stored in it. Banzhaf values approximation is used as the subtree ranking in \emph{HMill} explainer. If we do not fix the seed, the explanation is stochastic and might be unstable regarding the number of iterations and the output.

\subsection{Results}
Authors in \cite{Pevny2020} present qualitative and quantitative analysis for different \emph{HMill} explainer setups. The best result regarding the computational time and size of explanation was reported for the Banzhaf values-based heuristic addition approach combined with Level-by-level search. We will use this setup.

\section{Other methods for structured data}
We mentioned three alternative techniques for \emph{JSON} modelling in the previous chapters --- rules, flattening, and GNNs. Flattening is a technique of feature engineering more than modelling. It might be challenging to explain its predictions, as a general explanation algorithm stops at the flattened features and cannot explain the flattening functions. That refers to the weak decomposability of such methods mentioned earlier.

Rules are usually transparent, so their interpretability is straightforward. Finally, examples of graph neural net explaining can be found in \cite{Ying2019, Huang2020} 

% The first part of the thesis addressed the essential theoretical background. It covers the basics of malware analysis, machine learning, and especially the \emph{HMill} framework and the \emph{HMill} Explainer. In the following part, we describe our setup, results, and discussion.

\section{CAPEv2 explaining}
In the chapter \ref{chap:classification}, we introduced two factors according to which we categorize signatures.

We expect that explanation of the model, the cause of which is in the report, should be a set containing this cause. As an example, we expect that if the original signature's cause is a specific API call, it should be presented in the explanation of the binary classifier for this signature. As authors of \emph{HMill} explainer mentioned, we hope that the explanation contains even something new. In other words, we expect that we could observe explanations that contain entries that are not the original cause. However, they might reasonably substitute it --- they are connected to the same effects. It is also possible to identify new signatures because all samples are malicious and the model might generalize based on different similarities in the training set.

We want to explain at least one hundred samples for each classifier to observe repeatedly seen parts. We choose positive samples because we want to explain the positive class predictions.

\subsection{Additions}
Since \emph{HMill} explainer usually process sample by sample, one hundred explanations mean one hundred \emph{JSON} files. This quantity is still hard to interpret, and that is why we involved three additional ideas.

We merged all explanations for one signature into a one \emph{JSON} file and for each entry computed a number of occurrences across explanations. We assume that the most general formulation of an explanation should be seen repeatedly. 

We also counted the frequency of each particular key (name of the field in JSON file, e.g., \ \emph{read files, resolved apis}) in explanations such that for each signature, we see how often the explainer detects a particular key. We can compare the original signature's cause with the most seen key.

The last idea is that we compute the frequencies of entries seen across different signatures. We assume that in such a way, we could partially identify the bias that is caused by the entries that we see in multiple explanations across signatures. In such a case, it should be considered too general.