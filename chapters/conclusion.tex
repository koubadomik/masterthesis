\chapter{Conclusions} \label{chap:concl}
The main objective of this thesis was to design a pipeline that has a malware dataset as the input and a machine learning model and its explanation as the output. The whole process was motivated by high accuracy model interpretability to achieve greater compliance of machine learning and cybersecurity. Theory background and methods are summarized in the first part of the thesis. The setup, experienced problems, results, and their discussion are in the second part.

We set up eight physical machines with the \emph{CAPEv2} sandbox in two different setups --- with internet and without internet connection. Using the open source sandbox and our programs, we collected dynamic malware analyses for 80,000 malware samples retrieved from MalwareBazaar\footnote{https://bazaar.abuse.ch/}. We reported the problems experienced during the data collection process and the description of the whole setup, including our code.

We used \emph{JSON} reports of the sandbox as an input for \emph{Hierarchical multiple instance learning} framework \cite{Mandlik2020}, the choice of this technology was justified by its ability to model \emph{JSON} documents and better scalability in comparison with other methods. The classification model features are behavioural parts, and predicted classes are malware signatures, both included in the original \emph{JSON} report. 
To evaluate our models better, we investigated the original signature's implementation and found out their true cause. We created a binary classifier for each of the chosen signatures (overall 12). We observed how each model performs in the context of the true signature's cause. Nine classifiers had a balanced accuracy of more than 90\%. We reported and discussed individual results.

Finally, we experimented with the model explaining. Even though there might be hundreds of entries from the original behavioural report used as a feature set, the explainer only provides 3--5 entries as an explanation for each of the nine explained models. It is evident from our observations that some models were intensely associated with the original signature's cause. It is worth noting that there were cases where the model used different behavioural features with high accuracy. We reported and discussed all results.

Despite the significant amount of work we faced during the sandboxing, we managed to meet the goals of this thesis. We wanted our experiments to be repeatable, and therefore the source code and other technicalities are in the attachment of the thesis. In addition, we mentioned specific issues faced during the work, along with ideas for future work.

\section*{Future work}
The sandbox \emph{CAPEv2} does not include native support for extensive data collection since it is designed as a tool for malware analysis more than for machine learning experiments. Because our solution consists of many manual steps, it is worth adding some out-of-box solution for the clustered sandbox run to collect larger datasets. This solution might be based on the existing parts of the sandbox, which did not work for us. It can also be built on top of our lightweight tools. The main objective is to make it more user friendly for everyday use in machine learning.

\emph{HMill} models showed good accuracy, and the framework should be part of other experiments with complex data like ours. There should also be larger datasets of dynamic analysis reports with different signatures and malware samples, such as the one with the \emph{internet} data which we did not use eventually. Multilabel classification might be involved in the signature prediction as well. 

The data quality in the cybersecurity domain should not be overlooked, e.g., an additional effort in redundancy/noise reduction in reports. That is also related to precisely controlled conditions during the malware analysis.

The framework creates the model directly from the \emph{JSON} data which is very convenient for the model explanation. We can retrieve the explanation directly as a human-readable \emph{JSON} document. This capability should be examined in different situations, emphasizing practical applications, e.g., new signatures extraction, zero-days, and interpretability for cybersecurity professionals. Addressed challenges like causality detection or spurious correlation should also be taken into account.