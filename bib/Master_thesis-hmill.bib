Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@misc{Edwards2017,
archivePrefix = {arXiv},
arxivId = {stat.ML/1606.02185},
author = {Edwards, Harrison and Storkey, Amos},
eprint = {1606.02185},
primaryClass = {stat.ML},
title = {{Towards a Neural Statistician}},
year = {2017}
}
@inproceedings{Gartner2002,
author = {G{\"{a}}rtner, Thomas and Flach, Peter and Kowalczyk, Adam and Smola, Alex},
pages = {179--186},
title = {{Multi-Instance Kernels.}},
year = {2002}
}
@article{Dietterich1997,
address = {GBR},
author = {Dietterich, Thomas G and Lathrop, Richard H and Lozano-P{\'{e}}rez, Tom{\'{a}}s},
doi = {10.1016/S0004-3702(96)00034-3},
issn = {0004-3702},
journal = {Artif. Intell.},
keywords = {drug design,machine learning,structure-activity relationships},
number = {1–2},
pages = {31--71},
publisher = {Elsevier Science Publishers Ltd.},
title = {{Solving the Multiple Instance Problem with Axis-Parallel Rectangles}},
url = {https://doi.org/10.1016/S0004-3702(96)00034-3},
volume = {89},
year = {1997}
}
@inproceedings{Nowak2006,
abstract = {Bag-of-features representations have recently become popular for content based image classification owing to their simplicity and good performance. They evolved from texton methods in texture analysis. The basic idea is to treat images as loose collections of independent patches, sampling a representative set of patches from the image, evaluating a visual descriptor vector for each patch independently, and using the resulting distribution of samples in descriptor space as a characterization of the image. The four main implementation choices are thus how to sample patches, how to describe them, how to characterize the resulting distributions and how to classify images based on the result. We concentrate on the first issue, showing experimentally that for a representative selection of commonly used test databases and for moderate to large numbers of samples, random sampling gives equal or better classifiers than the sophisticated multiscale interest operators that are in common use. Although interest operators work well for small numbers of samples, the single most important factor governing performance is the number of patches sampled from the test image and ultimately interest operators can not provide enough patches to compete. We also study the influence of other factors including codebook size and creation method, histogram normalization method and minimum scale for feature extraction.},
address = {Berlin, Heidelberg},
author = {Nowak, Eric and Jurie, Fr{\'{e}}d{\'{e}}ric and Triggs, Bill},
booktitle = {Computer Vision -- ECCV 2006},
editor = {Leonardis, Ale{\v{s}} and Bischof, Horst and Pinz, Axel},
isbn = {978-3-540-33839-0},
keywords = {{\&} A. Pinz (Eds.),{\&} Triggs,B. (2006). Sampling Strategies for Bag-of-Features,Computer Vision -- ECCV 2006 (pp. 490–503). Spring,E.,F.,H. Bischof,Jurie,Nowak},
pages = {490--503},
publisher = {Springer Berlin Heidelberg},
title = {{Sampling Strategies for Bag-of-Features Image Classification}},
year = {2006}
}
@inproceedings{Bunescu2007,
abstract = {We present a new approach to multiple instance learning (MIL) that is particularly effective when the positive bags are sparse (i.e. contain few positive instances). Unlike other SVM-based MIL methods, our approach more directly enforces the desired constraint that at least one of the instances in a positive bag is positive. Using both artificial and real-world data, we experimentally demonstrate that our approach achieves greater accuracy than state-of-the-art MIL methods when positive bags are sparse, and performs competitively when they are not. In particular, our approach is the best performing method for image region classification.},
address = {New York, NY, USA},
author = {Bunescu, Razvan C and Mooney, Raymond J},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
doi = {10.1145/1273496.1273510},
isbn = {9781595937933},
pages = {105--112},
publisher = {Association for Computing Machinery},
series = {ICML '07},
title = {{Multiple Instance Learning for Sparse Positive Bags}},
url = {https://doi.org/10.1145/1273496.1273510},
year = {2007}
}
@article{Stiborek2018,
abstract = {This work addresses classification of unknown binaries executed in sandbox by modeling their interaction with system resources (files, mutexes, registry keys and communication with servers over the network) and error messages provided by the operating system, using vocabulary-based method from the multiple instance learning paradigm. It introduces similarities suitable for individual resource types that combined with an approximative clustering method efficiently group the system resources and define features directly from data. This approach effectively removes randomization often employed by malware authors and projects samples into low-dimensional feature space suitable for common classifiers. An extensive comparison to the state of the art on a large corpus of binaries demonstrates that the proposed solution achieves superior results using only a fraction of training samples. Moreover, it makes use of a source of information different than most of the prior art, which increases the diversity of tools detecting the malware, hence making detection evasion more difficult.},
author = {Stiborek, Jan and Pevn{\'{y}}, Tom{\'{a}}s̆ and Reh{\'{a}}k, Martin},
doi = {https://doi.org/10.1016/j.eswa.2017.10.036},
issn = {0957-4174},
journal = {Expert Systems with Applications},
keywords = {Classification,Dynamic analysis,Malware,Multiple instance learning,Random forest,Sandboxing},
pages = {346--357},
title = {{Multiple instance learning for malware classification}},
url = {https://www.sciencedirect.com/science/article/pii/S0957417417307170},
volume = {93},
year = {2018}
}
@misc{Amores2013,
abstract = {Multiple Instance Learning (MIL) has become an important topic in the pattern recognition community, and many solutions to this problem have been proposed until now. Despite this fact, there is a lack of comparative studies that shed light into the characteristics and behavior of the different methods. In this work we provide such an analysis focused on the classification task (i.e., leaving out other learning tasks such as regression). In order to perform our study, we implemented fourteen methods grouped into three different families. We analyze the performance of the approaches across a variety of well-known databases, and we also study their behavior in synthetic scenarios in order to highlight their characteristics. As a result of this analysis, we conclude that methods that extract global bag-level information show a clearly superior performance in general. In this sense, the analysis permits us to understand why some types of methods are more successful than others, and it permits us to establish guidelines in the design of new MIL methods. {\textcopyright} 2013 Elsevier B.V.},
author = {Amores, Jaume},
booktitle = {Artificial Intelligence},
doi = {10.1016/j.artint.2013.06.003},
issn = {00043702},
keywords = {Bag-of-Words,Codebook,Multi-instance learning},
title = {{Multiple instance classification: Review, taxonomy and comparative study}},
year = {2013}
}
@inproceedings{Keeler1991,
author = {Keeler, James and Rumelhart, David and Leow, Wee},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Lippmann, R P and Moody, J and Touretzky, D},
publisher = {Morgan-Kaufmann},
title = {{Integrated Segmentation and Recognition of Hand-Printed Numerals}},
url = {https://proceedings.neurips.cc/paper/1990/file/e46de7e1bcaaced9a54f1e9d0d2f800d-Paper.pdf},
volume = {3},
year = {1991}
}
@inproceedings{Pevny2017,
abstract = {Many objects in the real world are difficult to describe by means of a single numerical vector of a fixed length, whereas describing them by means of a set of vectors is more natural. Therefore, Multiple instance learning (MIL) techniques have been constantly gaining in importance throughout the last years. MIL formalism assumes that each object (sample) is represented by a set (bag) of feature vectors (instances) of fixed length, where knowledge about objects (e.g., class label) is available on bag level but not necessarily on instance level. Many standard tools including supervised classifiers have been already adapted to MIL setting since the problem got formalized in the late nineties. In this work we propose a neural network (NN) based formalism that intuitively bridges the gap between MIL problem definition and the vast existing knowledge-base of standard models and classifiers. We show that the proposed NN formalism is effectively optimizable by a back-propagation algorithm and can reveal unknown patterns inside bags. Comparison to 14 types of classifiers from the prior art on a set of 20 publicly available benchmark datasets confirms the advantages and accuracy of the proposed solution.},
address = {Cham},
author = {Pevn{\'{y}}, Tom{\'{a}}{\v{s}} and Somol, Petr},
booktitle = {Advances in Neural Networks - ISNN 2017},
editor = {Cong, Fengyu and Leung, Andrew and Wei, Qinglai},
isbn = {978-3-319-59072-1},
pages = {135--142},
publisher = {Springer International Publishing},
title = {{Using Neural Network Formalism to Solve Multiple-Instance Problems}},
year = {2017}
}
@inproceedings{Andrews2003,
author = {Andrews, Stuart and Tsochantaridis, Ioannis and Hofmann, Thomas},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Becker, S and Thrun, S and Obermayer, K},
publisher = {MIT Press},
title = {{Support Vector Machines for Multiple-Instance Learning}},
url = {https://proceedings.neurips.cc/paper/2002/file/3e6260b81898beacda3d16db379ed329-Paper.pdf},
volume = {15},
year = {2003}
}
@misc{PevnyKovarik2019a,
archivePrefix = {arXiv},
arxivId = {cs.LG/1906.00764},
author = {Pevny, Tomas and Kovarik, Vojtech},
eprint = {1906.00764},
primaryClass = {cs.LG},
title = {{Approximation capability of neural networks on spaces of probability measures and tree-structured domains}},
year = {2019}
}
@inproceedings{Pevny2016a,
abstract = {Modelling network traffic is gaining importance to counter modern security threats of ever increasing sophistication. It is though surprisingly difficult and costly to construct reliable classifiers on top of telemetry data due to the variety and complexity of signals that no human can manage to interpret in full. Obtaining training data with sufficiently large and variable body of labels can thus be seen as a prohibitive problem. The goal of this work is to detect infected computers by observing their HTTP(S) traffic collected from network sensors, which are typically proxy servers or network firewalls, while relying on only minimal human input in the model training phase. We propose a discriminative model that makes decisions based on a computer's all traffic observed during a predefined time window (5 minutes in our case). The model is trained on traffic samples collected over equally-sized time windows for a large number of computers, where the only labels needed are (human) verdicts about the computer as a whole (presumed infected vs. presumed clean). As part of training, the model itself learns discriminative patterns in traffic targeted to individual servers and constructs the final high-level classifier on top of them. We show the classifier to perform with very high precision, and demonstrate that the learned traffic patterns can be interpreted as Indicators of Compromise. We implement the discriminative model as a neural network with special structure reflecting two stacked multi instance problems. The main advantages of the proposed configuration include not only improved accuracy and ability to learn from gross labels, but also automatic learning of server types (together with their detectors) that are typically visited by infected computers.},
address = {New York, NY, USA},
author = {Pevny, Tomas and Somol, Petr},
booktitle = {Proceedings of the 2016 ACM Workshop on Artificial Intelligence and Security},
doi = {10.1145/2996758.2996761},
isbn = {9781450345736},
keywords = {big data,learning indicators of compromise,malware detection,neural network,user modeling},
pages = {83--91},
publisher = {Association for Computing Machinery},
series = {AISec '16},
title = {{Discriminative Models for Multi-Instance Problems with Tree Structure}},
url = {https://doi.org/10.1145/2996758.2996761},
year = {2016}
}
@techreport{Frank2003,
author = {Frank, Eibe and Xu, Xin},
title = {{Applying propositional learning algorithms to multi-instance data}},
year = {2003}
}
@techreport{Xu2003,
author = {Xu, Xin},
title = {{Statistical learning in multiple instance problems}},
year = {2003}
}
@mastersthesis{Mandlik2020,
author = {Mandlik, Simon and Pevny, Tomas},
publisher = {Czech Technical University in Prague},
title = {{Mapping the Internet — Modelling Entity Interactions in Complex Heterogeneous Networks}},
year = {2020}
}
@inproceedings{Wang2000,
author = {Wang, Jun and Zucker, Jean-daniel},
booktitle = {Proc. 17th International Con. on Machine Learning},
pages = {1119--1126},
title = {{Solving the multiple-instance problem: A lazy learning approach}},
year = {2000}
}
@phdthesis{Dong2006,
abstract = {Motivated by various challenging real-world applications, such as drug activity prediction and image retrieval, multi-instance (MI) learning has attracted considerable interest in recent years.  Compared with standard supervised learning, the MI learning task is more difficult as the label information of each training example is incomplete. Many MI algorithms have been proposed. Some of them are specifically designed for MI problems whereas others have been upgraded or adapted from standard single-instance learning algorithms.  Most algorithms have been evaluated on only one or two benchmark datasets, and there is a lack of systematic comparisons of MI learning algorithms.  This thesis presents a comprehensive study of MI learning algorithms that aims to compare their performance and find a suitable way to properly address different MI problems. First, it briefly reviews the history of research on MI learning. Then it discusses five general classes of MI approaches that cover a total of 16 MI algorithms. After that, it presents empirical results for these algorithms that were obtained from 15 datasets which involve five different real-world application domains.  Finally, some conclusions are drawn from these results: (1) applying suitable standard single-instance learners to MI problems can often generate the best result on the datasets that were tested, (2) algorithms exploiting the standard asymmetric MI assumption do not show significant advantages over approaches using the so-called collective assumption, and (3) different MI approaches are suitable for different application domains, and no MI algorithm works best on all MI problems.},
address = {Hamilton, New Zealand},
author = {Dong, Lin},
keywords = {machine learning,multi-instance learning},
language = {en},
publisher = {University of Waikato},
title = {{A Comparison of Multi-instance Learning Algorithms}},
url = {https://hdl.handle.net/10289/2453},
volume = {Master of Science (MSc)},
year = {2006}
}
@misc{Zaheer2018,
archivePrefix = {arXiv},
arxivId = {cs.LG/1703.06114},
author = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Ruslan and Smola, Alexander},
eprint = {1703.06114},
primaryClass = {cs.LG},
title = {{Deep Sets}},
year = {2018}
}
@inproceedings{Maron1998,
author = {Maron, Oded and Lozano-P{\'{e}}rez, Tom{\'{a}}s},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Jordan, M and Kearns, M and Solla, S},
publisher = {MIT Press},
title = {{A Framework for Multiple-Instance Learning}},
url = {https://proceedings.neurips.cc/paper/1997/file/82965d4ed8150294d4330ace00821d77-Paper.pdf},
volume = {10},
year = {1998}
}
