Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Mittelstadt2019,
author = {Mittelstadt, Brent and Russell, Chris and Wachter, Sandra},
doi = {10.1145/3287560.3287574},
isbn = {9781450361255},
journal = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
publisher = {ACM},
title = {{Explaining Explanations in AI}},
url = {http://dx.doi.org/10.1145/3287560.3287574},
year = {2019}
}
@article{Ribeiro2016,
archivePrefix = {arXiv},
arxivId = {1602.04938},
author = {Ribeiro, Marco T{\'{u}}lio and Singh, Sameer and Guestrin, Carlos},
eprint = {1602.04938},
journal = {CoRR},
title = {{"Why Should {\{}I{\}} Trust You?": Explaining the Predictions of Any Classifier}},
url = {http://arxiv.org/abs/1602.04938},
volume = {abs/1602.0},
year = {2016}
}
@article{Pevny2020,
author = {Pevn{\'{y}}, Tom{\'{a}}{\v{s}}},
title = {{Explaining Classifiers Trained on Raw Hierarchical Multiple-Instance DataNo Title}},
year = {2020}
}
@article{Montavon2018,
abstract = {This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. As a tutorial paper, the set of methods covered here is not exhaustive, but sufficiently representative to discuss a number of questions in interpretability, technical challenges, and possible applications. The second part of the tutorial focuses on the recently proposed layer-wise relevance propagation (LRP) technique, for which we provide theory, recommendations, and tricks, to make most efficient use of it on real data.},
author = {Montavon, Gr{\'{e}}goire and Samek, Wojciech and M{\"{u}}ller, Klaus-Robert},
doi = {https://doi.org/10.1016/j.dsp.2017.10.011},
issn = {1051-2004},
journal = {Digital Signal Processing},
keywords = {Activation maximization,Deep neural networks,Layer-wise relevance propagation,Sensitivity analysis,Taylor decomposition},
pages = {1--15},
title = {{Methods for interpreting and understanding deep neural networks}},
url = {https://www.sciencedirect.com/science/article/pii/S1051200417302385},
volume = {73},
year = {2018}
}
@article{Lipton2016,
archivePrefix = {arXiv},
arxivId = {1606.03490},
author = {Lipton, Zachary Chase},
eprint = {1606.03490},
journal = {CoRR},
title = {{The Mythos of Model Interpretability}},
url = {http://arxiv.org/abs/1606.03490},
volume = {abs/1606.0},
year = {2016}
}
@article{Guidotti2018,
archivePrefix = {arXiv},
arxivId = {1802.01933},
author = {Guidotti, Riccardo and Monreale, Anna and Turini, Franco and Pedreschi, Dino and Giannotti, Fosca},
eprint = {1802.01933},
journal = {CoRR},
title = {{A Survey Of Methods For Explaining Black Box Models}},
url = {http://arxiv.org/abs/1802.01933},
volume = {abs/1802.0},
year = {2018}
}
@article{Zhang2016,
archivePrefix = {arXiv},
arxivId = {1611.03530},
author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
eprint = {1611.03530},
journal = {CoRR},
title = {{Understanding deep learning requires rethinking generalization}},
url = {http://arxiv.org/abs/1611.03530},
volume = {abs/1611.0},
year = {2016}
}
